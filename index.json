[{"authors":null,"categories":null,"content":"My name is Wenlin Chen. I am a PhD student in Machine Learning at University of Cambridge (Machine Learning Group) and Max Planck Institute for Intelligent Systems (Empirical Inference Department). My supervisors are Professor José Miguel Hernández-Lobato and Professor Bernhard Schölkopf. My advisor is Dr Hong Ge.\nI am keen on basic research in machine learning and its scientific applications (e.g., drug discovery). My research interest lies at the intersection of probabilistic methods, deep learning, and causal inference. I aim to develop data-efficient machine learning methods that enable active data collection, robust inference and prediction, efficient data compression, and realistic generation of novel synthetic data.\n","date":1603670400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603670400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My name is Wenlin Chen. I am a PhD student in Machine Learning at University of Cambridge (Machine Learning Group) and Max Planck Institute for Intelligent Systems (Empirical Inference Department). My supervisors are Professor José Miguel Hernández-Lobato and Professor Bernhard Schölkopf.","tags":null,"title":"Wenlin Chen","type":"authors"},{"authors":["Wenlin Chen","Samuel Horváth","Peter Richtárik"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"9b46c19f513bcfc9944742d03bf99dce","permalink":"https://wenlin-chen.github.io/publication/chen2020optimal/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/publication/chen2020optimal/","section":"publication","summary":"It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with \"important\" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.","tags":[],"title":"Optimal Client Sampling for Federated Learning","type":"publication"},{"authors":["Andrew Webb","Charles Reynolds","Wenlin Chen","Henry Reeve","Dan Iliescu","Mikel Luján","Gavin Brown"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"73540bab56f2b0d108a710976f7b0dcb","permalink":"https://wenlin-chen.github.io/publication/webb2020ensemble/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/publication/webb2020ensemble/","section":"publication","summary":"End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue—are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where overparameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.","tags":[],"title":"To Ensemble or Not Ensemble: When Does End-to-End Training Fail?","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://wenlin-chen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]