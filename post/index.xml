<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes | Wenlin Chen</title>
    <link>https://wenlin-chen.github.io/post/</link>
      <atom:link href="https://wenlin-chen.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Notes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2024 Wenlin Chen</copyright><lastBuildDate>Sat, 01 Jun 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Notes</title>
      <link>https://wenlin-chen.github.io/post/</link>
    </image>
    
    <item>
      <title>Diffusion Sampling and Generative Modeling</title>
      <link>https://wenlin-chen.github.io/post/score/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/score/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h3 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data distribution
$$p(x)=\frac{\exp(-E(x))}{Z}$$
&lt;ul&gt;
&lt;li&gt;Intractable normalizing constant
$$Z=\int \exp(-E(x)) dx$$&lt;/li&gt;
&lt;li&gt;Tractable score function
$$\nabla_x \log p(x)=-\nabla_x E(x)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generative Modeling
&lt;ul&gt;
&lt;li&gt;Samples $x_1,\cdots,x_N\sim p(x)$ are available&lt;/li&gt;
&lt;li&gt;The energy function $E$ is unknown and needs to be learned from data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sampling
&lt;ul&gt;
&lt;li&gt;The energy function $E$ is given&lt;/li&gt;
&lt;li&gt;Samples from $p(x)$ are not available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diffusion&#34;&gt;Diffusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian convolution kernel
$$p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$$&lt;/li&gt;
&lt;li&gt;Intractable noisy marginal
$$p(y)=\int p(y|x)p(x)dx$$&lt;/li&gt;
&lt;li&gt;Intractable denoising posterior
$$p(x|y)=\frac{p(y|x)p(x)}{p(y)}$$&lt;/li&gt;
&lt;li&gt;Intractable noisy score
$$\nabla_y\log p(y)=\nabla_y\log \int p(y|x)p(x)dx$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;score-identities&#34;&gt;Score Identities&lt;/h1&gt;
&lt;h3 id=&#34;denoising-score-identity&#34;&gt;Denoising Score Identity&lt;/h3&gt;
&lt;p&gt;$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\nabla_y\log p(y|x)]=\int\nabla_y\log p(y|x) p(x|y)dx$$
Proof: By definition, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \frac{\int\nabla_y p(y|x)p(x)dx}{p(y)} \\
&amp;amp;= \int\nabla_y\log(y|x)\frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;tweedie-score-identity&#34;&gt;Tweedie Score Identity&lt;/h3&gt;
&lt;p&gt;$$\nabla_y\log p(y)=\frac{\alpha\mathbb{E}_{p(x|y)}[ x ]-y}{\sigma^2}=\int\left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx$$
Proof: By denoising score identity, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= \int\nabla_y\left(-\frac{\lVert y-\alpha x\rVert^2}{2\sigma^2}\right) p(x|y)dx \\
&amp;amp;= \int \left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;target-score-identity&#34;&gt;Target Score Identity&lt;/h3&gt;
&lt;p&gt;$$\nabla_y\log p(y)=\alpha^{-1}\mathbb{E}_{p(x|y)}[\nabla_x\log p(x)]=\alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx$$
Proof: By denoising score identity and using the following three identities
$$\nabla_y \log p(y|x)=-\alpha^{-1}\nabla_x \log p(y|x)$$
$$\nabla_x \log p(y|x)=\nabla_x\log p(x|y)-\nabla_x \log p(x)$$
$$\int \nabla_x\log p(x|y)p(x|y)dx=\int \nabla_xp(x|y)dx=\nabla_x\int p(x|y)dx=0$$
we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= -\alpha^{-1}\int\nabla_x\log p(y|x) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int(\nabla_x \log p(x)-\nabla_x\log p(x|y)) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int\nabla_x \log p(x) p(x|y)dx \\
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;mixed-score-identity&#34;&gt;Mixed Score Identity&lt;/h3&gt;
&lt;p&gt;$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\alpha(x+\nabla_x\log p(x))-y]=\int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx$$&lt;/p&gt;
&lt;p&gt;Proof: Using a variance-preserving scheme $\sigma^2=1-\alpha^2$ and combining target score identity and denoising score identity with coefficients $\alpha^2$ and $1-\alpha^2$, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int \left((\alpha\nabla_x \log p(x) + (1-\alpha^2)\nabla_y\log p(y|x)\right)p(x|y)dx \\
&amp;amp;= \int \left(\alpha\nabla_x \log p(x) + (1-\alpha^2)\frac{\alpha x - y}{\sigma^2}\right)p(x|y)dx \\
&amp;amp;= \int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx
\end{aligned}
$$&lt;/p&gt;
&lt;h1 id=&#34;diffusion-sampling&#34;&gt;Diffusion Sampling&lt;/h1&gt;
&lt;h3 id=&#34;monte-carlo-estimator&#34;&gt;Monte Carlo Estimator&lt;/h3&gt;
&lt;p&gt;Estimate the noisy score with Tweedie score identity using Monte Carlo
$$\nabla_y \log p(y)\approx \frac{\frac{\alpha}{K}\sum_{k=1}^K x_k-y}{\sigma^2},\quad x_k\sim p(x|y)$$
Initialize the sampler for $p(x|y)$ with its mean $\mathbb{E}_{p(x|y)}[ x ]$ estimated by importance sampling
$$\mathbb{E}_{p(x|y)}[ x ]\approx\frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right.$$
Proof: Using the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\mathbb{E}_{p(x|y)}[ x ]
&amp;amp;= \int x p(x|y)dx \\
&amp;amp;= \int x \frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \frac{\int x p(y|x)p(x)dx}{\int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int x \exp(-E(x))q(x|y)dx}{\int \exp(-E(x))q(x|y)dx} \\
&amp;amp;\approx \frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y)
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;importance-sampling-estimator&#34;&gt;Importance Sampling Estimator&lt;/h3&gt;
&lt;p&gt;Estimate the noisy score with the target score identity using importance sampling
$$\nabla_y \log p(y)\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right.$$
Proof: Using target score identity and the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha p(y)} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha \int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) \exp(-E(x)) q(x|y) dx}{\alpha \int \exp(-E(x)) q(x|y)dx} \\
&amp;amp;\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y)
\end{aligned}
$$&lt;/p&gt;
&lt;h1 id=&#34;generative-modeling&#34;&gt;Generative Modeling&lt;/h1&gt;
&lt;h3 id=&#34;denoising-score-matching&#34;&gt;Denoising Score Matching&lt;/h3&gt;
&lt;p&gt;Learn a score network $s_{\theta}(x)$ by minimizing
$$L(\theta)=\mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2]=\mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C$$
Proof: Using the denoising score identity, we have
$$
\begin{aligned}
\mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2]
&amp;amp;= \int \lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2 p(y) dy \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \int \left&amp;lt;s_{\theta}(y),\nabla_y \log p(y)\right&amp;gt;p(y)dy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(x|y)p(y)dxdy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(y|x)p(x)dxdy + C&amp;rsquo; \\
&amp;amp;= \iint \left( \lVert s_{\theta}(y) \rVert^2 + \lVert \nabla_y\log p(y|x) \rVert^2 - 2 \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;\right)p(y|x)p(x)dxdy + C \\
&amp;amp;= \iint \lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2 p(y|x)p(x)dxdy + C \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;nonparametric-estimator&#34;&gt;Nonparametric Estimator&lt;/h3&gt;
&lt;p&gt;$$\nabla_y \log p(y)=\frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x)$$
Proof: By definition of noisy score, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \nabla_y\log \int p(y|x)p(x)dx \\
&amp;amp;\approx \nabla_y\log\sum_{k=1}^K p(y|x_k),\quad x_k\sim p(x) \\
&amp;amp;= \frac{\sum_{k=1}^K \nabla_y p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x) \\
&amp;amp;=\frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x) \\
\end{aligned}
$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
