<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wenlin Chen</title>
    <link>https://wenlin-chen.github.io/</link>
      <atom:link href="https://wenlin-chen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Wenlin Chen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2025 Wenlin Chen</copyright><lastBuildDate>Mon, 05 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Wenlin Chen</title>
      <link>https://wenlin-chen.github.io/</link>
    </image>
    
    <item>
      <title>Training Neural Samplers with Reverse Diffusive KL Divergence</title>
      <link>https://wenlin-chen.github.io/publication/he2025training/</link>
      <pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/he2025training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Training One-Step Diffusion Models Without Distillation</title>
      <link>https://wenlin-chen.github.io/publication/zhang2025towards/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/zhang2025towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Your Image is Secretly the Last Frame of a Pseudo Video</title>
      <link>https://wenlin-chen.github.io/publication/chen2025your/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2025your/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks</title>
      <link>https://wenlin-chen.github.io/publication/chen2024neural/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024neural/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging Task Structures for Improved Identifiability in Neural Network Representations</title>
      <link>https://wenlin-chen.github.io/publication/chen2024leveraging/</link>
      <pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024leveraging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling Variability in Human Annotator Simulation</title>
      <link>https://wenlin-chen.github.io/publication/wu2024modelling/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/wu2024modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Diffusive Gibbs Sampling</title>
      <link>https://wenlin-chen.github.io/publication/chen2024diffusive/</link>
      <pubDate>Sun, 21 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024diffusive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Diffusion and Latent Variable Models</title>
      <link>https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/</guid>
      <description>&lt;h3 id=&#34;1-latent-variable-models&#34;&gt;1. Latent Variable Models&lt;/h3&gt;
&lt;h4 id=&#34;11-introduction&#34;&gt;1.1 Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Observed data: $x\in\mathcal{X}\subseteq\mathbb{R}^n$.&lt;/li&gt;
&lt;li&gt;Latent variable: $z\in\mathbb{R}^d$ ($d\leq n$).&lt;/li&gt;
&lt;li&gt;The joint distribution is factorized as the product of likelihood and prior:
$$p_{\theta}(x,z)=p_{\theta}(x|z)p(z).$$
&lt;ul&gt;
&lt;li&gt;The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network:
$$p_{\theta}(x|z)=\mathcal{N}(x|\mu_{\theta}(z),\sigma^2I).$$&lt;/li&gt;
&lt;li&gt;The prior is often chosen to be a simple distribution like standard Gaussian:
$$p(z)=\mathcal{N}(z|0,I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The marginal likelihood (or model evidence) is usually used for model selection:
$$p_{\theta}(x)=\int p_{\theta}(x|z)p(z) dz.$$
&lt;ul&gt;
&lt;li&gt;Maximum marginal likelihood learning $\max_{\theta}\log p_{\theta}(x)$ in intractable due to the intractable integral.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The posterior is given by Bayes&amp;rsquo; rule:
$$p_{\theta}(z|x)=\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(x)}=\frac{p_{\theta}(x|z)p(z)}{\int p_{\theta}(x|z)p(z) dz}.$$
&lt;ul&gt;
&lt;li&gt;The posterior is also intractable due to the intractability of the marginal likelihood.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-variational-autoencoders-vaes&#34;&gt;1.2 Variational Autoencoders (VAEs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;VAEs introduce an amortized mean-field Gaussian variational posterior (or encoder/inference model) with mean and diagnoal variance parameterized by neural networks:
$$q_{\phi}(z|x)=\mathcal{N}(z|\tilde{\mu}_{\phi}(x),\text{diag}(\tilde{\sigma}_{\phi}(x)^2))\approx p_{\theta}(z|x).$$&lt;/li&gt;
&lt;li&gt;The parameters of both the generation and inference models are jointly learned by maximizing a tractable evidence lower bound (ELBO) of the log marginal likelihood:
$$\log p_{\theta}(x)\geq\mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\right]=\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]-\text{KL}(q_{\phi}(z|x)||p(z))=\mathcal{F}(\theta,\phi).$$
&lt;ul&gt;
&lt;li&gt;The first term $\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]$ controls the reconstruction error.&lt;/li&gt;
&lt;li&gt;The second term $\text{KL}(q_{\phi}(z|x)||p(z))$ regularizes the approximate posterior to be close to the prior.&lt;/li&gt;
&lt;li&gt;In order to backpropagate through samples $z\sim q_{\phi}(z|x)$, we employ the reparameterization trick:
$$z=\tilde{\mu}_{\phi}(x)+\tilde{\sigma}_{\phi}(x)\odot \epsilon,\quad \epsilon\sim N(0,I).$$&lt;/li&gt;
&lt;li&gt;It is quite hard to train both generation and inference models well in practice due to difficulties in balancing the two terms above during optimization (e.g., the variational posterior can easily collapse to the prior due to variational overpruning).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The gap between the ELBO and log marginal likelihood is given by:
$$\log p_{\theta}(x)-\mathcal{F}(\theta,\phi)=\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\geq 0.$$
&lt;ul&gt;
&lt;li&gt;Maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posteriors.&lt;/li&gt;
&lt;li&gt;The ELBO attains equality when the approximate posterior is exactly the same as the true posterior:
$$\log p_{\theta}(x)=\mathcal{F}(\theta,\phi)\iff\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))=0\iff q_{\phi}(z|x)=p_{\theta}(z|x).$$&lt;/li&gt;
&lt;li&gt;However, the mean-field variational poterior $q_{\phi}(z|x)$ is uni-modal and the reverse KL divergence is known to be mode-seeking instead of mass convering. This means that $q_{\phi}(z|x)$ can only fit to one of the modes in the true posterior $p_{\theta}(z|x)$, which limits the expressivity of VAEs and makes it hard for the aggregated poseterior $q_{\phi}(z)=\int q_{\phi}(z|x)p_{\text{data}}(x)dx$ to be close to the prior $p(z)=\mathcal{N}(0,I)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-diffusion-and-hierarchical-latent-variable-models&#34;&gt;2. Diffusion and Hierarchical Latent Variable Models&lt;/h3&gt;
&lt;h4 id=&#34;21-introduction&#34;&gt;2.1 Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We slightly change the notation following the convention of hierarchical latent variable models.
&lt;ul&gt;
&lt;li&gt;Observed data: $x_0=x\in\mathcal{X}\subseteq\mathbb{R}^n$.&lt;/li&gt;
&lt;li&gt;We employ a sequence of latent variables $x_1,\cdots,x_T\in\mathbb{R}^d$ ($d\leq n$) to capture data representation at different levels.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The results in a new marginal likelihood:
$$p_{\theta}(x_0)=\int p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)dx_{1:T}.$$&lt;/li&gt;
&lt;li&gt;The prior over the last latent variable is a Standard Gaussian:
$$p(x_T)=\mathcal{N}(x_T|0,I).$$&lt;/li&gt;
&lt;li&gt;The likelihoods (or decoders/generation models) are Gaussians with mean parameterized by a time-dependent neural network:
$$p_{\theta}(x_{t-1}|x_t)=\mathcal{N}(x_{t-1}|\mu_{\theta}(x_t,t),\sigma_t^2 I).$$&lt;/li&gt;
&lt;li&gt;The posterior is intractable as before:
$$p_{\theta}(x_{1:T}|x_0)=\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{p_{\theta}(x_0)}.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-hierarchical-variational-autoencoders-hvaes&#34;&gt;2.2 Hierarchical Variational Autoencoders (HVAEs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HVAEs approximate the intractable posterior with an amortized Gaussian variational posterior (or encoder/inference model):
$$q_{\phi}(x_{1:T}|x_0)\approx p_{\theta}(x_{1:T}|x_0).$$&lt;/li&gt;
&lt;li&gt;There are different design choices for the factorization of the inference model.
&lt;ul&gt;
&lt;li&gt;Bottom-up factorization:
$$q_{\phi}(x_{1:T}|x_0)=\prod_{t=1}^T q_{\phi}(x_t|x_{t-1}).$$&lt;/li&gt;
&lt;li&gt;Top-down factorization:
$$q_{\phi}(x_{1:T}|x_0)=q_{\phi}(x_T|x_0)\prod_{t=2}^T q_{\phi}(x_{t-1}|x_t,x_0).$$&lt;/li&gt;
&lt;li&gt;The mean and diagnoal variance of the factors $q_{\phi}(x_t|x_{t-1})$ and $q_{\phi}(x_{t-1}|x_t,x_0)$ are parameterized by neural networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The tractable ELBO can be derived for HVAEs:
$$\log p_{\theta}(x_0)\geq\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{q_{\phi}(x_{1:T}|x_0)}\right]=\mathcal{F}(\theta,\phi).$$
&lt;ul&gt;
&lt;li&gt;For the bottom-up factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=1}^{T-1}\text{KL}(q_{\phi}(x_t|x_{t-1})||p_{\theta}(x_t|x_{t+1}))-\text{KL}(q_{\phi}(x_T|x_{T-1})||p(x_T))\right].$$&lt;/li&gt;
&lt;li&gt;For the top-down factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q_{\phi}(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q_{\phi}(x_T|x_0)||p(x_T))\right].$$&lt;/li&gt;
&lt;li&gt;The parameters of both generation and inference models are learned jointly by maximizing the ELBO.&lt;/li&gt;
&lt;li&gt;To reduce the model size, parameter sharing between the generation and inference models is possible.&lt;/li&gt;
&lt;li&gt;HVAEs are very flexible and expressive, since only the last latent variable $x_T$ is constrained to be standard Gaussian and the model will need to figure out the intermediate latent variables $x_{1:T-1}$.&lt;/li&gt;
&lt;li&gt;This causes even more difficulties during optimization than VAEs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;23-denoising-diffusion-probabilistic-models-ddpms&#34;&gt;2.3 Denoising Diffusion Probabilistic Models (DDPMs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DDPMs are a class of state-of-the-art deep generative models.&lt;/li&gt;
&lt;li&gt;DDPMs define a fixed bottom-up inference model (or diffusion process):
$$q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}).$$
&lt;ul&gt;
&lt;li&gt;No dimensionality reduction is performed $(d=n)$.&lt;/li&gt;
&lt;li&gt;The factors are predefined Gaussian convolution kernels:
$$q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I).$$
$$\implies q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I),\quad \bar{\alpha}_t=\prod_{s=1}^t \alpha_s.$$&lt;/li&gt;
&lt;li&gt;The top-down form of the inference model is analytically tractable:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1}|\tilde{\mu}(x_t,x_0),\tilde{\sigma}_t^2),$$
where
$$\tilde{\mu}(x_t,x_0)=\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t,$$
$$\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t}.$$
&lt;ul&gt;
&lt;li&gt;Sketch of proof: By Bayes&amp;rsquo; rule and the Markovian property, we have
$$q(x_{t-1}|x_t,x_0)= \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\propto q(x_t|x_{t-1})q(x_{t-1}|x_0).$$
Since everything is Gaussian with analytical expression, $\tilde{\mu}(x_t,x_0)$ and $\tilde{\sigma}_t^2$ can be figured out by moment matching.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The tractable ELBO can be derived for DDPMs:
$$
\begin{aligned}
\mathcal{F}(\theta)
&amp;amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q(x_T|x_0)||p(x_T))\right] \\
&amp;amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T \frac{\lVert\tilde{\mu}(x_t,x_0)-\mu_{\theta}(x_t,t)\rVert^2}{2\sigma_t^2}\right]+const.
\end{aligned}
$$
&lt;ul&gt;
&lt;li&gt;In constrast to HVAEs with flexible intermediate latent variables, the fixed inference model in DDPMs provides extra supervision signals for the intermediate latent variables.&lt;/li&gt;
&lt;li&gt;The objective for each intermediate latent variable looks like a regression objective and is much easier to optimize.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The ELBO can be further simplified using variance reduction techniques with the reparameterization trick:
&lt;ol&gt;
&lt;li&gt;Reparameterize $x_0$ according to the predefined inference model $q(x_t|x_0)$:
$$x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\quad\implies\quad x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}},\quad\epsilon\sim\mathcal{N}(0,I).$$&lt;/li&gt;
&lt;li&gt;Plug in the reparameterized $x_0$ to $\tilde{\mu}(x_t,x_0)$:
$$\tilde{\mu}(x_t,x_0)=\tilde{\mu}\left(x_t,\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right).$$&lt;/li&gt;
&lt;li&gt;Reparameterize $\mu_{\theta}(x_t,t)$ in the same form as $\tilde{\mu}(x_t,x_0)$:
$$\mu_{\theta}(x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right).$$&lt;/li&gt;
&lt;li&gt;The regression objectives can then be simplified to noise matching with a time-dependent noise prediction network $\epsilon_{\theta}(x_t,t)$:
$$\sum_{t=1}^T\lambda(t)\mathbb{E}_{\mathcal{N}(\epsilon|0,I)}[\lVert\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The analytical solution for the weighting function is given by
$$\lambda(t)=\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}.$$&lt;/li&gt;
&lt;li&gt;However, setting $\lambda(t)=1$ makes training much more stable in practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right)+\sigma_t z_t,\quad z_t\sim\mathcal{N}(0,I).$$
&lt;ul&gt;
&lt;li&gt;The variance of $p_{\theta}(x_{t-1}|x_t)$ can be set to
$$\sigma_t^2=1-\alpha_t\quad\text{or}\quad \sigma_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t},$$
which work equally well in practice for large $T$.&lt;/li&gt;
&lt;li&gt;It can be helpful to have a better approximation for the true covariance $\Sigma_{\theta}(x_t, t)$ of $p_{\theta}(x_{t-1}|x_t)$ especially when the number of denoising steps is small. Note that the true covariance (and all higher-order moments in principle) can be derived analytically from the score function:
$$\Sigma_{\theta}(x_t, t)=\frac{1-\alpha_t}{\alpha_t}\left(I+(1-\alpha_t)\nabla_{x_t}s_{\theta}(x_t,t)\right).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Connection to score matching and score-based diffuson models:
&lt;ul&gt;
&lt;li&gt;Denoising score matching with a time-dependent score network $s_{\theta}(x_t, t)$:
$$
\begin{aligned}
\mathbb{E}_{q(x_t|x_0)}\left[\lVert\nabla_{x_t}\log q(x_t|x_0)-s_{\theta}(x_t, t)\rVert^2\right]
&amp;amp;=\mathbb{E}_{q(x_t|x_0)}\left[\left\lVert\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{1-\bar{\alpha}_t}+s_{\theta}(x_t, t)\right\rVert^2\right] \\
&amp;amp;= \mathbb{E}_{\mathcal{N}(\epsilon|0,I)}\left[\left\lVert\frac{\epsilon}{\sqrt{1-\bar{\alpha}_t}}+s_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, t)\right\rVert^2\right].
\end{aligned}
$$
Therefore, the score network $s_{\theta}(x_t,t)$ and the noise prediction network $\epsilon_{\theta}(x_t,t)$ have the following connection:
$$s_{\theta}(x_t,t)=-\frac{\epsilon_{\theta}(x_t,t)}{\sqrt{1-\bar{\alpha}_t}}.$$
Furthermore, this gives rise to the Tweedie&amp;rsquo;s formula:
$$\mu_{\theta}(x_t,t)=\frac{x_t+(1-\alpha_t)s_{\theta}(x_t,t)}{\sqrt{\alpha_t}}.$$&lt;/li&gt;
&lt;li&gt;The sampling procedure of DDPM corresponds to the predictor step (numerical SDE solution with discretization) in the predictor-corrector sampling precedure of score-based diffusion models. One could also in principle introduce the corrector step in DDPM sampling by leveraging the relationship between $s_{\theta}(x_t, t)$ and $\epsilon_{\theta}(x_t, t)$ above.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;24-denoising-diffusion-implicit-models-ddims&#34;&gt;2.4 Denoising Diffusion Implicit Models (DDIMs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DDIMs define a fixed top-down inference model:
$$q(x_{1:T}|x_0)=q(x_T|x_0)\prod_{t=2}^T q(x_{t-1}|x_t,x_0).$$
&lt;ul&gt;
&lt;li&gt;No dimensionality reduction is performed $(d=n)$.&lt;/li&gt;
&lt;li&gt;The factors are predefined Gaussians to ensure that $q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0+(1-\bar{\alpha}_t)I)$ as in DDPMs:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}\left(x_{t-1}\left|\sqrt{\bar{\alpha}_{t-1}}x_0+\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}}\sqrt{1-\bar{\alpha}_{t-1}-s_t^2},s_t^2\right)\right..$$
&lt;ul&gt;
&lt;li&gt;Note that the forward diffusion process induced by a DDIM is not necessarily Markovian:
$$q(x_t|x_{t-1},x_0)=\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)},$$
since $x_t$ could depend on both $x_{t-1}$ and $x_0$ for some choices of the variance $s_t^2$.&lt;/li&gt;
&lt;li&gt;The variance $s_t^2$ controls how stochastic the forward diffusion process is.
&lt;ul&gt;
&lt;li&gt;$x_{t-1}$ becomes a deterministic function of $x_t$ and $x_0$ for $s_t=0$.&lt;/li&gt;
&lt;li&gt;If we set the variance $s_t^2$ as in DDPM:
$$s_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\bar{\alpha}_{t}/\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}},$$
then DDIM recovers DDPM which has a markovian forward diffusion process:
$$q(x_t|x_{t-1},x_0)=q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_0+(1-\alpha_t)I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The generation models are defined as
$$\begin{aligned}
p_{\theta}(x_{t-1}|x_t)
&amp;amp;=\int q(x_{t-1}|x_t,x_0)p_{\theta}(x_0|x_t)dx_0 \\
&amp;amp;\approx q(x_{t-1}|x_t,\tilde{x}_0(x_t)),\quad \tilde{x}_0(x_t)\sim p_{\theta}(x_0|x_t).
\end{aligned}$$
where we reparameterize $\tilde{x}_0(x_t)$ according to the forward diffusion process:
$$
\tilde{x}_0(x_t)=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)}{\sqrt{\bar{\alpha}_t}}.
$$&lt;/li&gt;
&lt;li&gt;The training objective for DDIM has the identical form (up to an additive constant) to that for DDPM with a different weighting function $\lambda(t)$ for each choice of the variance $s_t^2$ in $q(x_{t-1}|x_t,x_0)$.&lt;/li&gt;
&lt;li&gt;Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\sqrt{\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_t}}\left(x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)\right)+\epsilon_{\theta}(x_t, t)\sqrt{1-\bar{\alpha}_{t-1}-s_t^2}+s_t e,\quad e\sim\mathcal{N}(0,I).$$
&lt;ul&gt;
&lt;li&gt;DDPM is a special case with $s_t^2=\tilde{\sigma}_t^2$. This shows that at each denoising step, DDPM first predicts $\tilde{x}_0(x_t)$ from $x_t$ and then add noise to the predicted $\tilde{x}_0(x_t)$ to get $x_{t-1}$, where the added noise is a combination of the predicted noise $\epsilon_{\theta}(x_t, t)$ and random noise $e$.&lt;/li&gt;
&lt;li&gt;$s_t=0$ results in a deterministic generation process (DDIM). For DDIM, the noise added to get $x_{t-1}$ from $\tilde{x}_0(x_t)$ is just the predicted noise $\epsilon_{\theta}(x_t, t)$ and there is no random noise $e$.&lt;/li&gt;
&lt;li&gt;In practice, we can introduce a hyperparameter $\eta\geq0$ to control the randomness, i.e., $s_t^2=\eta\cdot\tilde{\sigma}_t^2$.&lt;/li&gt;
&lt;li&gt;Accelarated sampling can be achieved by considering a subset of time steps $\{\tau_1,\cdots,\tau_T\}\subseteq\{1,\cdots,T\}$ at generation time:
$$x_{\tau_{i-1}}=\sqrt{\frac{\bar{\alpha}_{\tau_{i-1}}}{\bar{\alpha}_{\tau_{i}}}}\left(x_{\tau_{i}}-\sqrt{1-\bar{\alpha}_{\tau_{i}}}\epsilon_{\theta}(x_{\tau_{i}}, \tau_i)\right)+\epsilon_{\theta}(x_{\tau_{i}}, \tau_i)\sqrt{1-\bar{\alpha}_{\tau_{i-1}}-s_{\tau_{i}}^2}+s_{\tau_{i}} e,\quad e\sim\mathcal{N}(0,I).$$
In practice, using DDIM (i.e., $s_{\tau_{i}}=0$) results in better sampling quantiy when the number of denoising steps is small. DDPM tends to outperform DDIM when the number of denoising steps is large.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Score and Flow Matching</title>
      <link>https://wenlin-chen.github.io/post/score_flow_matching/</link>
      <pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/score_flow_matching/</guid>
      <description>&lt;h3 id=&#34;1-generative-modeling&#34;&gt;1. Generative Modeling&lt;/h3&gt;
&lt;h4 id=&#34;11-problem-setting&#34;&gt;1.1 Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data distribution $p_{\text{data}}(x)$ is unknown.&lt;/li&gt;
&lt;li&gt;Samples from $p_{\text{data}}(x)$ are available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-goals&#34;&gt;1.2 Goals&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the density of the data distribution $p_{\text{data}}(x)$.&lt;/li&gt;
&lt;li&gt;Generate new samples from $p_{\text{data}}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-score-matching-for-diffusion-models&#34;&gt;2. Score Matching for Diffusion Models&lt;/h3&gt;
&lt;h4 id=&#34;21-notation&#34;&gt;2.1 Notation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denote data by $x_0=x$ and noise by $x_T$.&lt;/li&gt;
&lt;li&gt;Wiener process SDE ($dt&amp;gt;0$):
$$dw_t=z\sqrt{dt},\quad z\sim\mathcal{N}(0,I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-diffusion-process&#34;&gt;2.2 Diffusion Process&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Forward diffusion SDE ($dt&amp;gt;0$):
$$dx_t=f_t(x_t)dt+g_tdw_t.$$
&lt;ul&gt;
&lt;li&gt;Forward diffusion SDE corrupts data to noise.&lt;/li&gt;
&lt;li&gt;$f_t(x_t)$ is a vector-valued drift coefficient.&lt;/li&gt;
&lt;li&gt;$g_t$ is a scalar-valued diffusion coefficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reverse diffusion SDE ($dt&amp;lt;0$):
$$dx_t=\left(f_t(x_t)-g_t^2\nabla_{x_t} \log p_t(x_t)\right)dt + g_tdw_t.$$
&lt;ul&gt;
&lt;li&gt;Reverse diffusion SDE recovers data from noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probability flow ODE ($dt&amp;lt;0$):
$$dx_t=\left(f_t(x_t)-\frac{1}{2}g_t^2\nabla_{x_t} \log p_t(x_t)\right)dt.$$
&lt;ul&gt;
&lt;li&gt;Probability flow ODE and reverse diffusion SDE have the same marginal $p_t(x_t)$ at every time $t$.&lt;/li&gt;
&lt;li&gt;Probability flow ODE allows for likelihood evaluation for test or generated samples as it converts a diffusion model to a continuous normalizing flow (see the flow section below for details about likelihood evaluation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;23-denoising-score-matching&#34;&gt;2.3 Denoising Score Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;To generate data using the reverse diffusion SDE or probability flow PDE starting from a tractable noise distribution $\pi(x_T)$, we need to estimate the score $\nabla_{x_t} \log p_t(x_t)$ at every time $t$.&lt;/li&gt;
&lt;li&gt;Learn a time-dependent score network $s_{\theta}(x_t, t)$ by minimizing the score matching objetive:
$$\mathcal{L}_{SM}(\theta) = \mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,T)}[\lambda(t)\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_t(x_t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The score matching objective is intractable since we do not know the true score $\nabla_{x_t} \log p_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We instead minimize the tractable denoising score matching objective:
$$\mathcal{L}_{DSM}(\theta)=\mathbb{E}_{p_{t|0}(x_t|x_0)p_{\text{data}}(x_0)\mathcal{U}(t|0,T)}[\lambda(t)\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_{t|0}(x_t|x_0)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;It can be shown that $\nabla_{\theta}\mathcal{L}_{SM}(\theta)=\nabla_{\theta}\mathcal{L}_{DSM}(\theta).$&lt;/li&gt;
&lt;li&gt;Proof: See my notes on &lt;a href=&#34;https://wenlin-chen.github.io/post/score_identities&#34;&gt;Score Identities for Sampling and Generative Modeling&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Weighting function $\lambda(t)$:
&lt;ul&gt;
&lt;li&gt;Magnitude weighting balances the magnitude of the score matching loss across time $t$:
$$\lambda(t)\propto1/\lVert \nabla_{x_t} \log p_{t|0}(x_t|x_0)\rVert^2.$$&lt;/li&gt;
&lt;li&gt;Likelihood weighting $\lambda(t)=g_t^2$ leads to a nice connection between KL divergence and Fisher divergence:
$$\text{KL}(p_{\text{data}}(x_0)||p_{\theta}(x_0))\leq\frac{T}{2}\mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,T)}[g_t^2\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_t(x_t)\rVert^2]+\text{KL}(p_T(x_T)||\pi(x_T)).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Predictor-corrector samplers for generation: correct the discretization errors in the numerical SDE solution with MCMC.
&lt;ul&gt;
&lt;li&gt;Initialize $x\sim \pi(x)$ and $t\gets T$.&lt;/li&gt;
&lt;li&gt;Repeat the following two steps until $t=0$:
&lt;ul&gt;
&lt;li&gt;Predictor (numerical SDE solver with small discretization step $\Delta t&amp;lt;0$):
$$\Delta x\gets\left(f_t(x)-g_t^2 s_{\theta}(x, t)\right)\Delta t + g_t\sqrt{|\Delta t|}z,\quad z\sim N(0,I).$$
$$x\gets x+\Delta x.$$
$$t\gets t+\Delta t.$$&lt;/li&gt;
&lt;li&gt;Corrector (a few steps of Langevin dynamics with step-size $\eta$):
$$x\gets x+\eta s_{\theta}(x,t)+\sqrt{2\eta}z,\quad z\sim N(0,I).$$
The step-size $\eta$ is usually parameterized in the following form with a signal-to-noise ratio (SNR) hyperparameter:
$$\eta=2\times\text{SNR}^2\times\frac{\lVert z\rVert^2}{\lVert s_{\theta}(x,t)\rVert^2}.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why do we need multiple noise levels?
&lt;ul&gt;
&lt;li&gt;Large noise corrupts the data distribution $p_{\text{data}}(x_0)$ too much and will produce samples from the wrong distribution.&lt;/li&gt;
&lt;li&gt;When the noise level is small, $p_{\text{data}}(x_0)$ can have many low density regions in real-world applications. Since $\mathcal{L}_{DSM}(\theta)$ is weighted by $p_{\text{data}}(x_0)$, the score estimates in the low density regions can be very poor. Since (random) initial samples are highly likely located in one of the low density regions, it is difficult for Langevin dynamics to move them to the high density regions due to poor score estimates at the beginning.&lt;/li&gt;
&lt;li&gt;Score functions are blind for densities with disconnected support. To see this, consider a mixture of two distributions
$$p_{\text{data}}(x_0)=w r_1(x_0) + (1-w) r_2(x_0)$$
with disjoint supports $\mathcal{X}_1$ and $\mathcal{X}_2$ for the two mixture components. The score function is given by
$$\nabla_x \log p_{\text{data}}(x_0)=
\begin{cases}\nabla_x \log r_1(x_0), &amp;amp;x\in\mathcal{X}_1\\
\nabla_x \log r_2(x_0), &amp;amp;x\in\mathcal{X}_2\end{cases},$$
which contains no information of the mixture weight $w$. In theory, one regularity condition of score matching is $p_{\text{data}}(x_0)&amp;gt;0$ everywhere. In practice, there will also be an issue when the mixture components are weakly connected. Hence, score matching with small noise is unable capture the correct weighting between different modes and will lead to biased samples.&lt;/li&gt;
&lt;li&gt;Therefore, we need to start with a large noise level to make the modes connected, which enables accurate estimation of the weight for each mode. The noise level is gradually reduced to zero so that the samples are refined to be distributed as $p_{\text{data}}(x_0)$ with correct weighting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-flow-matching-for-continuous-normalizing-flows&#34;&gt;3. Flow Matching for Continuous Normalizing Flows&lt;/h3&gt;
&lt;h4 id=&#34;31-notation&#34;&gt;3.1 Notation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denote data by $x_T=x$ and noise by $x_0$
&lt;ul&gt;
&lt;li&gt;This follows the convention in flow matching.&lt;/li&gt;
&lt;li&gt;Note: this is the opposite to the definition in diffusion models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-discrete-normalizing-flow&#34;&gt;3.2 Discrete Normalizing Flow&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Discrete normalzing flow with an invertible transformation function $\phi$ (and $T=1$):
$$x_1=\phi(x_0),\quad x_0\sim\pi(x_0),$$
$$\log p(x_1)=\log \pi(\phi^{-1}(x_1)) +\log\left|\det\frac{\partial \phi^{-1}(x_1)}{\partial x_1}\right|.$$&lt;/li&gt;
&lt;li&gt;Compose $T$ discrete normalizing flows:
$$x_{t+1}=\phi_{t}(x_t),$$
$$\phi=\phi_{T-1}\circ\cdots\circ\phi_0,$$
$$\log p(x_T)=\log \pi(\phi^{-1}(x_T)) +\sum_{t=1}^T\log\left|\det\frac{\partial \phi_{t-1}^{-1}(x_t)}{\partial x_{t}}\right|.$$&lt;/li&gt;
&lt;li&gt;Discrete residual flow transformation:
$$\phi_t(x_t)=x_{t+1}=x_t+\delta u_t(x_t).$$
&lt;ul&gt;
&lt;li&gt;$u_t$ needs to be $1/\delta$-Lipschitz to guarantee the invertibility of $\phi_t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn a parametric flow transformation function $\phi_{\theta}$ by MLE:
$$\max_{\theta}~\mathbb{E}_{p_{\text{data}}(x_T)}[\log p_{\theta}(x_T)].$$
&lt;ul&gt;
&lt;li&gt;We need to enforce invertibility in the architecture of $\phi_{\theta}$.&lt;/li&gt;
&lt;li&gt;We need to compute and backpropagate through the inverse and Jacobian for $\phi_{\theta}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;33-continuous-normalizing-flow&#34;&gt;3.3 Continuous Normalizing Flow&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Continuous residual flow transformation:
$$x_{t+\delta}=x_t+\delta u_t(x_t).$$
$$u_t(x_t)=\lim_{\delta\to0}\frac{x_{t+\delta}-x_t}{\delta}=\frac{dx_t}{dt}.$$&lt;/li&gt;
&lt;li&gt;Continuous normalizing flow with a transformation function $\phi_t$ induced by the vector field $u_t$:
$$\frac{dx_t}{dt}=u_t(x_t)\quad\implies\quad x_t=x_0+\int_0^t u_s(x_s)ds,\quad x_0\sim\pi(x_0),$$
&lt;ul&gt;
&lt;li&gt;Set $T=1$ following convention.&lt;/li&gt;
&lt;li&gt;The flow transformation is defined as $x_t=\phi_t(x_0)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The probability path $p_t$ induced by the vector field $u_t$ follows
$$\frac{d}{dt}\log p_t(x_t)=-\nabla_{x_t}\cdot u_t(x_t)\quad\implies\quad\log p_t(x_t)=\log\pi(x_0)-\int_{0}^t\nabla_{x_s}\cdot u_s(x_s)ds.$$
&lt;ul&gt;
&lt;li&gt;Proof: By the transport equation, we have
$$
\begin{aligned}
\frac{\partial}{\partial t} p_t(x_t)
&amp;amp;= -\nabla_{x_t}\cdot(u_t(x_t)p_t(x_t)) \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t)-\left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt;.
\end{aligned}$$
The total derivative is then given by
$$
\begin{aligned}
\frac{d}{d t} p_t(x_t)
&amp;amp;= \frac{\partial}{\partial t} p_t(x_t) + \left&amp;lt;\nabla_{x_t}p_t(x_t),\frac{dx_t}{d t}\right&amp;gt; \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t)-\left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt; + \left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt; \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t).
\end{aligned}
$$
Therefore, we have
$$\frac{d}{dt}\log p_t(x_t)=\frac{1}{p_t(x_t)}\frac{d}{d t} p_t(x_t)=-\nabla_{x_t}\cdot u_t(x_t).$$&lt;/li&gt;
&lt;li&gt;The transport equation for diffusion models with $u_t(x_t)=f_t(x_t)-\frac{1}{2}g_t^2\nabla_{x_t} \log p_t(x_t)$ is the Fokker-Planck equation:
$$\frac{\partial}{\partial t} p_t(x_t)+\nabla_{x_t}\cdot(f_t(x_t)p_t(x_t))-\frac{1}{2}g_t^2\nabla_{x_t}^2p_t(x_t)=0.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In practice, both $x_t$ and $\log p_t(x_t)$ can be solved jointly using a numerical ODE integrator:
$$
\frac{d}{dt}
\begin{pmatrix}
x_t \\
\log p_t(x_t)
\end{pmatrix}=
\begin{pmatrix}
u_t(x_t) \\
-\nabla_{x_t}\cdot u_t(x_t)
\end{pmatrix}.
$$&lt;/li&gt;
&lt;li&gt;Note that there are many different vector fields $u_t$ that can induce probability paths between $p_0$ and $p_1$.&lt;/li&gt;
&lt;li&gt;Learn a parameteric time-dependent vector field $u_{\theta}(x_t, t)$ by MLE:
$$\max_{\theta}~\mathbb{E}_{p_{\text{data}}(x_1)}[\log p_{\theta}(x_1)].$$
&lt;ul&gt;
&lt;li&gt;We do not need to choose the number of flow transformations as in composed discrete normalizing flows.&lt;/li&gt;
&lt;li&gt;$u_t$ only needs to be $L$-Lipschitz with any value $L$ to guarantee the invertibility of $\phi_t$.&lt;/li&gt;
&lt;li&gt;Numerical simulation of the ODE with backpropagation makes training very slow and expensive.&lt;/li&gt;
&lt;li&gt;Divergence estimator scales poorly with dimensionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;34-conditional-flow-matching&#34;&gt;3.4 Conditional Flow Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We want a simulation-free objective like score matching.&lt;/li&gt;
&lt;li&gt;Learn a time-dependent vector field $u_{\theta}(x_t, t)$ by minimizing the flow matching objetive:
$$\mathcal{L}_{FM}(\theta)=\mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(x_t, t)-u_t(x_t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The flow matching objective is intractable since we do not know the true vector field $u_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To circumvent the intractability, we consider a specific probability path $p_t(x_t)$ defined by a conditional probability path $p_{t|1}(x_t|x_1)$:
$$p_t(x_t)=\int p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1,$$
with boundary conditions
$$p_{0|1}(x)=\pi(x)\implies p_0(x_0)=\pi(x_0),$$
$$p_{1|1}(x)=\delta(x-x_1)\implies p_1(x_1)=p_{\text{data}}(x_1).$$&lt;/li&gt;
&lt;li&gt;The marginal vector field $u_t$ can be obtained from the corresponding conditional vector field $u_{t|1}(x_t|x_1)$ and conditional probability path $p_{t|1}(x_t|x_1)$ through the following identity:
$$u_t(x_t)=\mathbb{E}_{p_{1|t}(x_1|x_t)}[u_{t|1}(x_t|x_1)]=\int u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)}dx_1.$$
&lt;ul&gt;
&lt;li&gt;Proof: We verify that this is consistent with the transport equation for the marginals.
$$
\begin{aligned}
\frac{\partial}{\partial t}p_t(x_t)
&amp;amp;= \frac{\partial}{\partial t} \int p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= \int \frac{\partial}{\partial t} p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= - \int \nabla_{x_t}\cdot (u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)) p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= - \int \nabla_{x_t}\cdot (u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)) dx_1 \\
&amp;amp;= - \nabla_{x_t}\cdot  \int  u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)p_{\text{data}}(x_1) dx_1  \\
&amp;amp;= - \nabla_{x_t}\cdot \left( \int  u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)} dx_1 p_t(x_t) \right) \\
&amp;amp;= - \nabla_{x_t}\cdot (u_t(x_t) p_t(x_t)). \\
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We now introduce a tractable conditional flow matching objective:
$$\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(x_t,t)-u_{t|1}(x_t|x_1) \rVert^2].$$
&lt;ul&gt;
&lt;li&gt;It can be shown that $\nabla_{\theta}\mathcal{L}_{FM}(\theta)=\nabla_{\theta}\mathcal{L}_{CFM}(\theta)$.&lt;/li&gt;
&lt;li&gt;Proof: We follow a similar idea to the proof of denoising score matching but with the following equality.
$$
\begin{aligned}
\mathbb{E}_{p_t(x_t)}[\left&amp;lt;u_{\theta}(x_t,t),u_t(x_t)\right&amp;gt;]
&amp;amp;= \int \left&amp;lt;u_{\theta}(x_t,t),u_t(x_t)\right&amp;gt; p_t(x_t) dx_t \\
&amp;amp;= \int \left&amp;lt;u_{\theta}(x_t,t),\int u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)}dx_1\right&amp;gt; p_t(x_t) dx_t \\
&amp;amp;= \iint \left&amp;lt;u_{\theta}(x_t,t),u_{t|1}(x_t|x_1)\right&amp;gt; p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 dx_t \\
&amp;amp;= \mathbb{E}_{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}[\left&amp;lt;u_{\theta}(x_t,t),u_{t|1}(x_t|x_1)\right&amp;gt;].
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In practice, we may want our conditional vector field $u_{t|1}$ to generate a conditional Gaussian probability path:
$$p_{t|1}(x_t|x_1)=\mathcal{N}(x_t|\mu_t(x_1),\sigma_t(x_1)^2I),$$
with boundary conditions
$$\mu_0(x_1)=0,~\sigma_0(x_1)=1\quad\implies\quad p_{0|1}(x|x_1)=\pi(x)=\mathcal{N}(x|0,I),$$
$$\mu_1(x_1)=x_1,\quad\sigma_1(x_1)=0\quad\implies\quad p_{1|1}(x|x_1)=\delta(x-x_1).$$
&lt;ul&gt;
&lt;li&gt;One correpsonding conditional vector field is given by
$$u_{t|1}(x_t|x_1)=\frac{x-\mu_t(x_1)}{\sigma_t(x_1)}\frac{d\sigma_t(x_1)}{d t}+\frac{d \mu_t(x_1)}{d t},$$
with a conditional flow transformation
$$x_t(x_1)=\phi_{t|1}(x_0|x_1)=\mu_t(x_1)+\sigma_t(x_1)x_0.$$&lt;/li&gt;
&lt;li&gt;Proof: We verify that the following two quantities are identical.
$$\frac{d}{dt}x_t(x_1)=\frac{d}{dt}\mu_t(x_1)+x_0\frac{d}{dt}\sigma_t(x_1),$$
and
$$
\begin{aligned}
u_{t|1}(x_t(x_1)|x_1)
&amp;amp;= \frac{\mu_t(x_1)+\sigma_t(x_1)x_0-\mu_t(x_1)}{\sigma_t(x_1)}\frac{d\sigma_t(x_1)}{d t}+\frac{d \mu_t(x_1)}{d t} \\
&amp;amp;=x_0\frac{d}{dt}\sigma_t(x_1)+\frac{d}{dt}\mu_t(x_1).
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;Reparameterize the conditional flow matching objective:
$$\implies \mathcal{L}_{CFM}(\theta)=\mathbb{E}_{\pi(x_0)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}\left[\left\lVert u_{\theta}(\mu_t(x_1)+\sigma_t(x_1)x_0,t)-\left(\frac{d}{dt}\mu_t(x_1)+x_0\frac{d}{dt}\sigma_t(x_1) \right)\right\rVert^2\right].$$&lt;/li&gt;
&lt;li&gt;Example with optimal transport conditional vector field (linear interpolation):
$$\mu_t(x_1)=tx_1,\quad \sigma_t(x_1)=(1-t)+t\sigma_{min}=1-(1-\sigma_{min})t.$$
$$\implies x_t(x_1)=tx_1+(1-(1-\sigma_{min})t)x_0.$$
$$\implies u_{t|1}(x_t(x_1)|x_1)=x_1-(1-\sigma_{min})x_0.$$
$$\implies \mathcal{L}_{CFM}(\theta)=\mathbb{E}_{\pi(x_0)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(tx_1+(1-(1-\sigma_{min})t)x_0,t)-(x_1-(1-\sigma_{min})x_0) \rVert^2].$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two problems with conditional flow matching:
&lt;ul&gt;
&lt;li&gt;The estimate of the gradient $\nabla_{\theta} \mathcal{L}_{CFM}(\theta)$ is of high variance since there are many possible data $x_1$ corresponding to a noise $x_0$ due to intersection of probability paths for different realizations of $u_{t|1}(x_t|x_1)$ with different values of the conditioning variable $x_1$.&lt;/li&gt;
&lt;li&gt;Sampling is slow at generation time since it is difficult to integrate ODE with non-straight path induced by the learned marginal vector field $u_{\theta}(u_t,t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Coupling: instead of maping between data $x_1$ and noise $x_0$ (i.e., one-sided conditioning), we can similarly map between any two variables $x_0$ and $x_1$ with two-sided conditioning:
$$p_t(x_t)=\iint p_{t|0,1}(x_t|x_0,x_1)p_{\text{data}}(x_0,x_1)dx_0dx_1,$$
with boundary conditions
$$p_{0|0,1}(x|x_0,x_1)=\delta(x-x_0)\implies p_0(x_0)=p_{\text{data}}(x_0),$$
$$p_{1|0,1}(x|x_0,x_1)=\delta(x-x_1)\implies p_1(x_1)=p_{\text{data}}(x_1).$$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Score Identities</title>
      <link>https://wenlin-chen.github.io/post/score_identities/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/score_identities/</guid>
      <description>&lt;h3 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h3&gt;
&lt;h4 id=&#34;11-problem-setting&#34;&gt;1.1 Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data distribution:
$$p(x)=\frac{\exp(-E(x))}{Z}.$$
&lt;ul&gt;
&lt;li&gt;Intractable normalizing constant:
$$Z=\int \exp(-E(x)) dx.$$&lt;/li&gt;
&lt;li&gt;Tractable score function:
$$\nabla_x \log p(x)=-\nabla_x E(x).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Goal: generate new samples from $p(x)$.&lt;/li&gt;
&lt;li&gt;Sampling:
&lt;ul&gt;
&lt;li&gt;The energy function $E$ is given.&lt;/li&gt;
&lt;li&gt;No available samples from $p(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generative Modeling:
&lt;ul&gt;
&lt;li&gt;Samples from $p(x)$ are available.&lt;/li&gt;
&lt;li&gt;The energy function $E$ is usually unknown and needs to be learned from data.&lt;/li&gt;
&lt;li&gt;In some rare settings, the energy function $E$ is also given.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-diffusion&#34;&gt;1.2 Diffusion&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian convolution kernel:
$$p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I).$$&lt;/li&gt;
&lt;li&gt;Intractable noisy marginal:
$$p(y)=\int p(y|x)p(x)dx.$$&lt;/li&gt;
&lt;li&gt;Intractable denoising posterior:
$$p(x|y)=\frac{p(y|x)p(x)}{p(y)}.$$
&lt;ul&gt;
&lt;li&gt;However, the score function of the denoising posterior is tractable:
$$
\begin{aligned}
\nabla_x \log p(x|y) &amp;amp;= \nabla_x \log p(y|x) + \nabla_x \log p(x) \\
&amp;amp;= \frac{\alpha(y-\alpha x)}{\sigma^2}-\nabla_x E(x).
\end{aligned}
$$
Therefore, it is possible to sample from the denoising posterior using a MCMC sampler.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Intractable noisy score:
$$
\begin{aligned}
\nabla_y\log p(y) &amp;amp;=\nabla_y\log \int p(y|x)p(x)dx \\&amp;amp;
=\nabla_y\log \int \exp\left(-\frac{\lVert y - \alpha x \rVert^2}{2\sigma^2}-E(x)\right)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-score-identities&#34;&gt;2. Score Identities&lt;/h3&gt;
&lt;p&gt;We list some useful score identities below which allow us to calculate the noisy score.&lt;/p&gt;
&lt;h4 id=&#34;21-denoising-score-identity-dsi&#34;&gt;2.1 Denoising Score Identity (DSI)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denoising score identity is a general formula without assumping the form of $p(y|x)$:
$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\nabla_y\log p(y|x)]=\int\nabla_y\log p(y|x) p(x|y)dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By definition, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \frac{\nabla_y p(y)}{p(y)} \\
&amp;amp;= \frac{\nabla_y\int p(y|x)p(x)dx}{p(y)} \\
&amp;amp;= \frac{\int\nabla_y p(y|x)p(x)dx}{p(y)} \\
&amp;amp;= \int\nabla_y\log(y|x)\frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-tweedie-score-identity&#34;&gt;2.2 Tweedie Score Identity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tweedie score identity assumes Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$:
$$\nabla_y\log p(y)=\frac{\alpha\mathbb{E}_{p(x|y)}[ x ]-y}{\sigma^2}=\int\left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By denoising score identity, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= \int\nabla_y\left(-\frac{\lVert y-\alpha x\rVert^2}{2\sigma^2}\right) p(x|y)dx \\
&amp;amp;= \int \left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;23-target-score-identity-tsi&#34;&gt;2.3 Target Score Identity (TSI)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Target score identity assumes a translation-invariant convolution $p(y|x)=p(y-\alpha x)$ (e.g., Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$):
$$\nabla_y\log p(y)=\alpha^{-1}\mathbb{E}_{p(x|y)}[\nabla_x\log p(x)]=\alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By denoising score identity and using the following three identities
$$\nabla_y \log p(y|x)=-\alpha^{-1}\nabla_x \log p(y|x),$$
$$\nabla_x \log p(y|x)=\nabla_x\log p(x|y)-\nabla_x \log p(x),$$
$$\int \nabla_x\log p(x|y)p(x|y)dx=\int \nabla_xp(x|y)dx=\nabla_x\int p(x|y)dx=\nabla_x 1=0,$$
we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= -\alpha^{-1}\int\nabla_x\log p(y|x) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int(\nabla_x \log p(x)-\nabla_x\log p(x|y)) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int\nabla_x \log p(x) p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;24-mixed-score-identity-msi&#34;&gt;2.4 Mixed Score Identity (MSI)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mixed score identity assumes Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$ with a variance-preserving scheme $\sigma^2=1-\alpha^2$:
$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\alpha(x+\nabla_x\log p(x))-y]=\int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx.$$
Proof: Consider a convex combination of the target score identity and Tweedie score identity with coefficients $\alpha^2$ and $1-\alpha^2$:
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int \left((\alpha^2\frac{\nabla_x \log p(x)}{\alpha} + (1-\alpha^2)\frac{\alpha x - y}{\sigma^2}\right)p(x|y)dx \\
&amp;amp;= \int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-applications-in-score-based-sampling&#34;&gt;3. Applications in Score-based Sampling&lt;/h3&gt;
&lt;h4 id=&#34;31-monte-carlo-estimator&#34;&gt;3.1 Monte Carlo Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with Tweedie score identity using Monte Carlo:
$$\nabla_y \log p(y)\approx \frac{\frac{\alpha}{K}\sum_{k=1}^K x_k-y}{\sigma^2},\quad x_k\sim p(x|y).$$&lt;/li&gt;
&lt;li&gt;We may initialize the sampler for $p(x|y)$ with its mean $\mathbb{E}_{p(x|y)}[ x ]$ estimated by importance sampling:
$$\mathbb{E}_{p(x|y)}[ x ]\approx\frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right..$$
This is the standard importance sampling approach. It does not work well in high dimensional space.&lt;/li&gt;
&lt;li&gt;Proof: Using the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\mathbb{E}_{p(x|y)}[ x ]
&amp;amp;= \int x p(x|y)dx \\
&amp;amp;= \int x \frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \frac{\int x p(y|x)p(x)dx}{\int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int x \exp(-E(x))q(x|y)dx}{\int \exp(-E(x))q(x|y)dx} \\
&amp;amp;\approx \frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-importance-sampling-estimator&#34;&gt;3.2 Importance Sampling Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with the target score identity using importance sampling:
$$\nabla_y \log p(y)\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right..$$
This does not work well in practice. It only works for very small noise level.&lt;/li&gt;
&lt;li&gt;Proof: Using target score identity and the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha p(y)} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha \int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) \exp(-E(x)) q(x|y) dx}{\alpha \int \exp(-E(x)) q(x|y)dx} \\
&amp;amp;\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-applications-in-score-based-generative-modeling&#34;&gt;3 Applications in Score-based Generative Modeling&lt;/h3&gt;
&lt;h4 id=&#34;31-denoising-score-matching-dsm&#34;&gt;3.1 Denoising Score Matching (DSM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a noisy score network $s_{\theta}(y)$ by minimizing
$$
\begin{aligned}
\mathcal{L}_{DSM}(\theta)
&amp;amp;= \mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2] \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C.
\end{aligned}
$$
This is the most popular generative modeling apporach. It is inaccurate for very small noise level.&lt;/li&gt;
&lt;li&gt;Proof: Using the denoising score identity, we have
$$
\begin{aligned}
\mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2]
&amp;amp;= \int \lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2 p(y) dy \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \int \left&amp;lt;s_{\theta}(y),\nabla_y \log p(y)\right&amp;gt;p(y)dy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(x|y)p(y)dxdy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(y|x)p(x)dxdy + C&amp;rsquo; \\
&amp;amp;= \iint \left( \lVert s_{\theta}(y) \rVert^2 + \lVert \nabla_y\log p(y|x) \rVert^2 - 2 \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;\right)p(y|x)p(x)dxdy + C \\
&amp;amp;= \iint \lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2 p(y|x)p(x)dxdy + C \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;One problem with denoising score matching is that the score network $s_{\theta}(y)$ is inaccurate for a low noise level $\sigma\approx 0$. To see this, consider the score of the Gaussian convolution kernel:
$$\nabla_{y} \log p(y|x)=-\frac{y-\alpha x}{\sigma^2}=-\frac{z}{\sigma},\quad z\sim \mathcal{N}(0,I),$$
which is unbounded as $\sigma\to 0$. This leads to a huge variance for the estimate of $\mathcal{L}_{DSM}(\theta)$ for small noise level and thus results in optimization difficulty.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-nonparametric-estimator&#34;&gt;3.2 Nonparametric Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with the target score identity using with a KDE-style estimator:
$$\nabla_y \log p(y)\approx\frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x).$$
This is expensive to evaluate and needs to be recomputed for different values of $y$.&lt;/li&gt;
&lt;li&gt;Proof: By definition of noisy score, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \nabla_y\log \int p(y|x)p(x)dx \\
&amp;amp;\approx \nabla_y\log\sum_{k=1}^K p(y|x_k),\quad x_k\sim p(x) \\
&amp;amp;= \frac{\sum_{k=1}^K \nabla_y p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x) \\
&amp;amp;= \frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;33-target-score-matching-tsm&#34;&gt;3.3 Target Score Matching (TSM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a noisy score network $s_{\theta}(y)$ by minimizing
$$
\begin{aligned}
\mathcal{L}_{TSM}(\theta)
&amp;amp;= \mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2] \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \alpha^{-1}\nabla_x \log p(x)\rVert^2]+C.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;This requires to have access to both samples from $p(x)$ and the ground-truth energy or score function. It is only accurate for small noise level.&lt;/li&gt;
&lt;li&gt;Proof: This proof is similar to the proof of denoising score matching but with the target score identity instead.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</title>
      <link>https://wenlin-chen.github.io/publication/wu2023has/</link>
      <pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/wu2023has/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction</title>
      <link>https://wenlin-chen.github.io/publication/chen2023meta/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2023meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimal Client Sampling for Federated Learning</title>
      <link>https://wenlin-chen.github.io/publication/chen2022optimal/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2022optimal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transformers</title>
      <link>https://wenlin-chen.github.io/post/transformers/</link>
      <pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/transformers/</guid>
      <description>&lt;h3 id=&#34;1-attention-mechanism&#34;&gt;1. Attention Mechanism&lt;/h3&gt;
&lt;p&gt;Attention module assigns attention scores (importance weights) for the tokens in a sequence. An output sequence is produced by computing a linear combination of the input tokens weighted by the attention scores.&lt;/p&gt;
&lt;h4 id=&#34;11-attention-formulation&#34;&gt;1.1 Attention Formulation&lt;/h4&gt;
&lt;p&gt;Let $V=[v_1,v_2,\cdots,v_L]^T\in\mathbb{R}^{L\times D_v}$ be a sequence of length $L$. Each element $v_j\in\mathbb{R}^{D_v}$ in the sequence $V$ is a vector of dimension $D_v$.&lt;/p&gt;
&lt;p&gt;The attetion $Y=[y_1,y_2,\cdots,y_L]^T\in\mathbb{R}^{L\times D_v}$ of the sequence $V$ is a weighted sum of all elements in the sequence $V$:
$$Y=AV,$$
$$
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_L
\end{bmatrix}=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1L} \\
a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2L} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{L1} &amp;amp; a_{L2} &amp;amp; \cdots &amp;amp; a_{LL}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_L
\end{bmatrix},
$$
where the attention scores are given by the attention matrix $A\in\mathbb{R}^{L\times L}$. The attention scores are required to be positive:
$$a_{ij}&amp;gt;0,$$
and normalized over its rows:
$$\sum_{j=1}^L a_{ij}=1.$$
Each score $a_{ij}$ specifies the importance of the $j$-th vector $v_j$ in the sequence $V$ to the $i$-th vector $y_i$ in the attention $Y$:
$$y_i=\sum_{j=1}^La_{ij}v_j.$$&lt;/p&gt;
&lt;h4 id=&#34;12-self-attention&#34;&gt;1.2 Self-Attention&lt;/h4&gt;
&lt;p&gt;Self-attention is a simple way to generate an attention matrix $A$ and compute the attention $Y$ from an input sequence $X=[x_1,x_2,\cdots,x_L]^T\in\mathbb{R}^{L\times D}$ alone, where each element $x_i\in\mathbb{R}^D$ is a token.&lt;/p&gt;
&lt;p&gt;We first transform $X$ into query $Q$, key $K$ and value $V$ using three input linear projections $W_q\in\mathbb{R}^{D\times D_{qk}}$, $W_k\in\mathbb{R}^{D\times D_{qk}}$ and $W_v\in\mathbb{R}^{D\times D_v}$, respectively:
$$Q=XW_q:=[q_1,q_2,\cdots,q_L]^T\in\mathbb{R}^{L\times D_{qk}},$$
$$K=XW_k:=[k_1,k_2,\cdots,k_L]^T\in\mathbb{R}^{L\times D_{qk}},$$
$$V=XW_v:=[v_1,v_2,\cdots,v_L]^T\in\mathbb{R}^{L\times D_{v}}.$$&lt;/p&gt;
&lt;p&gt;We then generate unnormalized attention scores $\tilde{A}\in\mathbb{R}^{L\times L}$ by computing the dot-product between query and key:
$$\tilde{a}_{ij}=q_i^Tk_j=(W_q^Tx_i)^T(W_k^Tx_j)=x_i^T(W_qW_k^T)x_j,$$
$$
\tilde{A}=QK^T=
\begin{bmatrix}
q_1^T \\
q_2^T \\
\vdots \\
q_L^T
\end{bmatrix}
\begin{bmatrix}
k_1 &amp;amp; k_2 &amp;amp; \cdots &amp;amp; k_L
\end{bmatrix}=
\begin{bmatrix}
q_1^Tk_1 &amp;amp; q_1^Tk_2 &amp;amp; \cdots &amp;amp; q_1^Tk_L \\
q_2^Tk_1 &amp;amp; q_2^Tk_2 &amp;amp; \cdots &amp;amp; q_2^Tk_L \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
q_L^Tk_1 &amp;amp; q_L^Tk_2  &amp;amp; \cdots &amp;amp; q_L^Tk_L
\end{bmatrix}.
$$
Since $W_k\not=W_q$ in general, $\tilde{A}$ is asymmetric ($\tilde{a}_{ij}\not=\tilde{a}_{ji}$). This makes the model more expressive by allowing having asymmetrical attention scores (i.e., the importance of $x_i$ to $x_j$ can be different from the importance of $x_j$ to $x_i$).&lt;/p&gt;
&lt;p&gt;The attention score $A\in\mathbb{R}^{L\times L}$ is then obtained by applying the softmax function to each row of $\tilde{A}$ to get normalized probabilities:
$$A=\text{softmax}\left(\frac{QK^T}{\sqrt{D_{qk}}}\right),$$
where we divide the dot-product by $\sqrt{D_{qk}}$ to increase numerical stability, which standardizes the dot-product (assuming that the elements in $K$ and $Q$ are independently distributed with unit variance). This is the scaled dot-product attention, which is the most commonly-used attention mechanism in transformers.&lt;/p&gt;
&lt;p&gt;Finally, the attention $Y\in\mathbb{R}^{L\times D_v}$ is obtained by
$$Y=AV=\text{softmax}\left(\frac{QK^T}{\sqrt{D_{qk}}}\right)V.$$&lt;/p&gt;
&lt;p&gt;The MHSA layer essentially allows all tokens in the input sequence to communicate with each other, which can be viewed as a message passing mechanism applied to a complete graph.&lt;/p&gt;
&lt;p&gt;If the sequence $X$ is causal (e.g., a time series), we need to apply the causal mask to $\tilde{A}$ during training by setting all elements in the upper triangular part of $\tilde{A}$ to be $-\infty$ (i.e., $\tilde{a}_{ij}=-\infty$ and thus $a_{ij}=0$ for $j&amp;gt;i$):
$$
\tilde{A}=
\begin{bmatrix}
q_1^Tk_1 &amp;amp; -\infty  &amp;amp; -\infty &amp;amp; \cdots &amp;amp; -\infty &amp;amp; -\infty \\
q_2^Tk_1 &amp;amp; q_2^Tk_2 &amp;amp; -\infty &amp;amp; \cdots &amp;amp; -\infty &amp;amp; -\infty \\
q_3^Tk_1 &amp;amp; q_3^Tk_2 &amp;amp; q_3^Tk_3 &amp;amp; \cdots &amp;amp; -\infty &amp;amp; -\infty \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
q_{L-1}^Tk_1 &amp;amp; q_{L-1}^Tk_2 &amp;amp; q_{L-1}^Tk_3 &amp;amp; \cdots &amp;amp; q_{L-1}^Tk_{L-1} &amp;amp;  -\infty \\
q_L^Tk_1 &amp;amp; q_L^Tk_2 &amp;amp; q_L^Tk_3 &amp;amp; \cdots &amp;amp; q_L^Tk_{L-1} &amp;amp;  q_L^Tk_L \\
\end{bmatrix},
$$
which prevents the model from peeking into future (i.e., using future tokens to compute the attention score for the current token).&lt;/p&gt;
&lt;p&gt;At inference time for autoregressive generation, it is a common practice (called KV-cache) to store the $k_i$ and $v_i$ vectors computed at each time step $i$ to avoid recomputing them in the future time steps $t&amp;gt;i$.&lt;/p&gt;
&lt;h4 id=&#34;13-cross-attention&#34;&gt;1.3 Cross-Attention&lt;/h4&gt;
&lt;p&gt;For cross-attention between two sequences $X$ and $X&amp;rsquo;$, the query Q is computed from a different input sequence $X&amp;rsquo;=[x_1,x_2,\cdots,x_{L&amp;rsquo;}]^T\in\mathbb{R}^{L&amp;rsquo;\times D}$:
$$Q=X&amp;rsquo;W_q:=[q_1,q_2,\cdots,q_L]^T\in\mathbb{R}^{L&amp;rsquo;\times D_{qk}},$$
$$K=XW_k:=[k_1,k_2,\cdots,k_L]^T\in\mathbb{R}^{L\times D_{qk}},$$
$$V=XW_v:=[v_1,v_2,\cdots,v_L]^T\in\mathbb{R}^{L\times D_{v}}.$$&lt;/p&gt;
&lt;p&gt;The attention scores $A\in\mathbb{R}^{L&amp;rsquo;\times L}$ and attention $Y=AV\in\mathbb{R}^{L&amp;rsquo;\times D_v}$ are computed using the same formulae:
$$Y=\text{softmax}\left(\frac{QK^T}{\sqrt{D_{qk}}}\right)V.$$&lt;/p&gt;
&lt;p&gt;Self-attention is a special case of cross attention with $X&amp;rsquo;=X$.&lt;/p&gt;
&lt;h4 id=&#34;14-multi-head-attention&#34;&gt;1.4 Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;Multi-head attention computes multiple single-head attentions to give the model different views. Each head $h$ has its own linear projections $W_q^h$, $W_k^h$ and $W_v^h$ to compute its attention $Y_h$. We then concatenate all heads
$$\tilde{Y}=
\begin{bmatrix}
Y_1 &amp;amp; Y_2 &amp;amp; \cdots &amp;amp; Y_H
\end{bmatrix}\in\mathbb{R}^{L&amp;rsquo;\times HD_v}
$$
and apply an output linear projection $W_o\in\mathbb{R}^{HD_v\times D_o}$ to get the final attention output:
$$O=\tilde{Y}W_o\in\mathbb{R}^{L&amp;rsquo;\times D_o}.$$&lt;/p&gt;
&lt;h4 id=&#34;15-computational-complexity-analysis&#34;&gt;1.5 Computational Complexity Analysis&lt;/h4&gt;
&lt;p&gt;The time and space complexity of a multi-head attention module mainly comes from three parts: input projection, multi-head attention, output projection.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Multi-Head Cross-Attention (MHCA):
&lt;ul&gt;
&lt;li&gt;We consider the most general case of MHCA.&lt;/li&gt;
&lt;li&gt;Time complexity:
$$\mathcal{O}(H(\underbrace{L&amp;rsquo;DD_{qk}}_{X&amp;rsquo;W_q}+\underbrace{LDD_{qk}}_{XW_k}+\underbrace{LDD_v}_{XW_v})+H(\underbrace{L&amp;rsquo;D_{qk}L}_{QK^T}+\underbrace{L&amp;rsquo;LD_v}_{AV})+\underbrace{L&amp;rsquo;HD_vD_o}_{\tilde{Y}W_o}).$$&lt;/li&gt;
&lt;li&gt;Space complexity
$$\mathcal{O}(H(\underbrace{L&amp;rsquo;D_{qk}}_{Q}+\underbrace{LD_{qk}}_{K}+\underbrace{LD_v}_{V})+H(\underbrace{L&amp;rsquo;L}_{A}+\underbrace{L&amp;rsquo;D_v}_{Y})+\underbrace{L&amp;rsquo;D_o}_{O}).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- 2. Multi-Head Self-Attention (MHSA, $L&#39;=L$):
    - Time complexity:
    $$\mathcal{O}(\underbrace{HLD(2D_{qk}+D_v)}\_{\text{input projection}}+\underbrace{HL^2(D_{qk}+D_v)}\_\text{self-attention}+\underbrace{LHD_vD_o}\_\text{output projection}).$$
    - Space complexity
    $$\mathcal{O}(\underbrace{HL(2D_{qk}+D_v)}\_{Q,K,V}+\underbrace{HL(L+D_v)}\_{A,Y}+\underbrace{LD_o}\_{O}).$$ --&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Multi-Head Self-Attention (MHSA):
&lt;ul&gt;
&lt;li&gt;We consider the most commonly used MHSA configuration: $L&amp;rsquo;=L, D_o=D, D_{qk}=D_v$.&lt;/li&gt;
&lt;li&gt;Time complexity:
$$\mathcal{O}(\underbrace{HLDD_{qkv}}_{\text{input projection}}+\underbrace{HL^2D_{qkv}}_\text{self-attention}+\underbrace{HLDD_{qkv}}_\text{output projection}).$$&lt;/li&gt;
&lt;li&gt;Space complexity
$$\mathcal{O}(\underbrace{HLD_{qkv}}_{Q,K,V}+\underbrace{HL(L+D_{qkv})}_{A,Y}+\underbrace{LD}_{O}).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;2-transformer-block&#34;&gt;2. Transformer Block&lt;/h3&gt;
&lt;h4 id=&#34;21-forward-pass&#34;&gt;2.1 Forward Pass&lt;/h4&gt;
&lt;p&gt;Transformer block $T$ is the core building block of transformers. We iteratively apply transformer blocks to process the input sequence $X^{(0)}$:
$$X^{(m)}=T(X^{(m-1)}).$$&lt;/p&gt;
&lt;p&gt;A standard transformer block (with self-attention) consists of an MHSA layer and a point-wise feedforward network (FFN) with residual connections and LayerNorm:
$$
\tilde{O}^{(m)}=X^{(m-1)}+\text{MHSA}(\text{LayerNorm}(X^{(m-1)})),
$$
$$
X^{(m)}=\tilde{O}^{(m)}+\text{FFN}(\text{LayerNorm}(\tilde{O}^{(m)})).
$$&lt;/p&gt;
&lt;p&gt;Note that LayerNorm is applied to the sequences $X^{(m-1)}$ and $\tilde{O}^{(m)}$. We also use residual connection to stablize training and avoid gradient vanishing in deep network architectures.&lt;/p&gt;
&lt;p&gt;After obtaining the attention output $\tilde{O}^{(m)}$, we feed it into a point-wise feedforward network (also with LayerNorm and residual connection) which is an MLP network applied independently to each token in $\tilde{O}^{(m)}$.&lt;/p&gt;
&lt;h4 id=&#34;22-point-wise-feed-forward-network-ffn&#34;&gt;2.2 Point-Wise Feed-Forward Network (FFN)&lt;/h4&gt;
&lt;p&gt;The point-wise FFN processes the representation of each token in the attention output $\tilde{O}^{(m)}$ individually after they communicated with each other in the MHSA layer.&lt;/p&gt;
&lt;p&gt;Point-wise FFN refers to an MLP network with one hidden layer, which is applied independently to each token in $\tilde{O}^{(m)}$:
$$
F:=\text{FFN}(O)=g(OW_{f1})W_{f2},
$$
where $g$ is a non-linear activation function, $W_{f1}\in\mathbb{R}^{D\times4D}$ and $W_{f2}\in\mathbb{R}^{4D\times D}$.&lt;/p&gt;
&lt;h4 id=&#34;23-layer-normalization-layernorm&#34;&gt;2.3 Layer Normalization (LayerNorm)&lt;/h4&gt;
&lt;p&gt;LayerNorm is applied to the sequences $X^{(m-1)}$ and $\tilde{O}^{(m)}$ before feeding them into MHSA/FFN to prevent explosion of the sequence/learned representations.&lt;/p&gt;
&lt;p&gt;For a sequence $Z$, LayerNorm standardizes each token $z_i$ independently by its own mean and standard deviation:
$$
\tilde{z}_i=\frac{z_i-\text{mean}(z_i)}{\sqrt{\text{var}(z_i)+\varepsilon}}*\gamma+\beta,
$$
where $\gamma,\beta\in\mathbb{R}$ are learnable free-parameters.&lt;/p&gt;
&lt;p&gt;Note that LayerNorm in transformers is slightly different from that in CNNs which standardizes each image (including its height, width and channel axes). LayerNorm is preferred for NLP tasks, whereas BatchNorm, which standardizes the data along the batch axis, is more commonly used in computer vision tasks.&lt;/p&gt;
&lt;h4 id=&#34;24-dropout&#34;&gt;2.4 Dropout&lt;/h4&gt;
&lt;p&gt;Dropout is applied to the attention scores $A$, the attention output $O$, and the FFN output $F$ to prevent overfitting for these dense layers.&lt;/p&gt;
&lt;p&gt;Dropout works by randomly zeroing out each element in a sequence/weight matrix $Z$ with a pre-defined probability $q$ during training:
$$\tilde{Z}=\frac{Z\odot M}{1-q},$$
where each element in the mask $M$ is independently sampled from a Bernoulli distribution $m_{ij}\sim\text{Bern}(1-q)$ in each forward pass during training.&lt;/p&gt;
&lt;h4 id=&#34;25-input-embedding-and-position-encoding&#34;&gt;2.5 Input Embedding and Position Encoding&lt;/h4&gt;
&lt;p&gt;For discrete one-hot input tokens $X_{raw}\in\mathbb{R}^{L\times D_{vocab}}$, we use a look-up table $W_{embed}\in\mathbb{R}^{D_{vocab}\times D}$ ($D\ll D_{vocab}$) for input embedding:
$$X^{(0)}=X_{raw}W_{embed}.$$&lt;/p&gt;
&lt;p&gt;Transformers are permutation-invariant since they treat input sequences as set of tokens. For tasks where the order of the tokens within a sequence is important, we need to add an extra position encoding vector $P\in\mathbb{R}^{L\times D}$ to the input embedding to break the permutation invariance:
$$X^{(0)}=X_{raw}W_{embed}+P.$$
There are many options for position encoding, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sinusoid position encoding by using sinusoid of a different phase and frequency for each token position $l$ and feature dimension $d$:
$$P_{l,d}=
\begin{cases}
\sin\left(l\cdot n^{-d/D}\right), &amp;amp; \text{if $d$ is even} \\
\cos\left(l\cdot n^{-(d-1)/D}\right), &amp;amp; \text{if $d$ is odd}
\end{cases}
$$
where $n$ is a hyperparameter usually set to $n=10000$.&lt;/li&gt;
&lt;li&gt;Learned position encoding by treating $P=[p_1, p_2, \cdots, p_L]^T\in\mathbb{R}^{L\times D}$ as a learnable free-parameter.&lt;/li&gt;
&lt;li&gt;Relative position encoding instead learns to encode the relative position offset between any two token positions $i$ and $j$. One way is to reparameterize the dot-product bewteen query and key. Note that for the standard learned position encoding, we have
$$
\begin{aligned}
\tilde{a}_{ij}
&amp;amp;=q_i^{T}k^j=(x_i+p_i)^TW_qW_k^T(x_j+p_j) \\
&amp;amp;=x_i^TW_qW_k^Tx_j + x_i^TW_qW_k^Tp_j + p_i^TW_qW_k^Tx_j + p_i^TW_qW_k^Tp_j.
\end{aligned}
$$
Relative position encoding modifies the above equation:
$$\tilde{a}_{ij}^{rel}=\underbrace{x_i^TW_qW_{k,C}^Tx_j}_\text{standard attention} + \underbrace{x_i^TW_qW_{k,R}^Tp_{i-j}}_\text{content-dependent positional bias} + \underbrace{u^TW_{k,C}^Tx_j}_\text{global content bias} + \underbrace{v^TW_{k,R}^Tp_{i-j}}_\text{global positional bias},$$
where $P$ is the sinusoid poisition encoding, all $p_j$&amp;rsquo;s&amp;rsquo; are replaced by the relative position $p_{i-j}$, $u,v$ are two learnable free-parameters, and the matrix $W_k$ are splitted into two matrices $W_{k,C}$ for content information and $W_{k,R}$ for location information.&lt;/li&gt;
&lt;li&gt;Rotary position encoding (RoPE) ensures that each dot-product $q_i^Tk_j$ only depends on the relative position between $i$ and $j$ by rotating the representations by an angle proportional to its position index with the following rotation matrix:
$$
R_i=
\begin{bmatrix}
\cos(l\theta_1) &amp;amp; -\sin(l\theta_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
\sin(l\theta_1) &amp;amp; \cos(l\theta_1) &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \cos(l\theta_2) &amp;amp; -\sin(l\theta_2) &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp;\sin(l\theta_2) &amp;amp; \cos(l\theta_2) &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \cos(l\theta_{D_{qk}/2}) &amp;amp; -\sin(l\theta_{D_{qk}/2}) \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \sin(l\theta_{D_{qk}/2}) &amp;amp; \cos(l\theta_{D_{qk}/2})
\end{bmatrix}\in\mathbb{R}^{D_{qk}\times D_{qk}},
$$
which formulates the sinusoid position encoding into a rotation matrix, where $\theta_d=n^{-2(d-1)/D_{qk}}$ with $d=\{1,2,\cdots,D_{qk}/2\}$ and $n=10000$. Now, we incorportate this rotation into the dot-product:
$$
\tilde{a}_{ij}^{RoPE}=(R_iW_q^Tx_i)^T(R_jW_k^Tx_j)=x_i^TW_q(R_i^TR_j)W_k^Tx_j=x_i^T(W_qR^{j-i}W_k^T)x_j.
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;26-activation-function&#34;&gt;2.6 Activation Function&lt;/h4&gt;
&lt;p&gt;The original transformer paper uses the Rectifid Linear Unit (ReLU) as the activation function:
$$\text{ReLU}(z)=\max(0,z).$$
Subsequent papers employ smoother activation functions, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian Error Linear Unit (GELU):
$$\text{GELU}(z)=z\odot\Phi(z),$$
where $\Phi(\cdot)$ is the CDF of a standard Gaussian distribution.&lt;/li&gt;
&lt;li&gt;Swish Unit:
$$\text{Swish}(z)=z\odot\text{Sigmoid}(\alpha z),$$
where $\text{Sigmoid}(z)=1/(1+\exp(-z))$. For $\alpha=1$, Swish recovers Sigmoid Linear Unit (SiLU). As $\alpha\to\infty$, Swish recovers ReLU.&lt;/li&gt;
&lt;li&gt;Swish-Gated Linear Unit (SwiGLU):
$$\text{SwiGLU}(z)=\text{Swish}(zW_1)\odot(zW_2).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-transformer-architectures&#34;&gt;3. Transformer Architectures&lt;/h3&gt;
&lt;h4 id=&#34;31-full-transformer&#34;&gt;3.1 Full Transformer&lt;/h4&gt;
&lt;p&gt;The full encoder-decoder transformer architecture is designed to solve sequence-to-sequence modeling tasks such as machine translation.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=full_transformer.png width=&#34;50%&#34; height=&#34;50%&#34;&gt;
  &lt;figcaption&gt;Transformer Encoder-Decoder Architecture (image source: https://github.com/dvgodoy/dl-visuals/).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4 id=&#34;32-bidirectional-encoder-representations-from-transformer-bert&#34;&gt;3.2 Bidirectional Encoder Representations from Transformer (BERT)&lt;/h4&gt;
&lt;p&gt;BERT employs an encoder-only transformer architecture to learn representations for the input sequence, trained by masked token prediction and next sentence prediction.&lt;/p&gt;
&lt;h4 id=&#34;33-generative-pre-trained-transformer-gpt&#34;&gt;3.3 Generative Pre-trained Transformer (GPT)&lt;/h4&gt;
&lt;p&gt;GPT employs a decoder-only transformer architecture to generate text given context, trained by next token prediction.&lt;/p&gt;
&lt;h4 id=&#34;34-vision-transformer-vit&#34;&gt;3.4 Vision Transformer (ViT)&lt;/h4&gt;
&lt;p&gt;ViT is a transformer encoder designed to learn representations for images. ViT converts an image to a sequence of tokens by dividing the image into patches and mapping them to lower dimensional token space with a linear projection. The sequence is then fed into a transformer encoder for representation learning. Compared to CNNs, ViTs are less data efficient but have more capacity due to fewer inductive biases.&lt;/p&gt;
&lt;h3 id=&#34;4-low-rank-adaptation-lora&#34;&gt;4. Low-Rank Adaptation (LoRA)&lt;/h3&gt;
&lt;p&gt;LoRA is an efficient way to fine-tune the weight matrices in transformer models (attention and optionally MLP). The hypothesis behind LoRA is that the adaptation of a large transformer model has a low intrinstic dimension that can be achieved by learning an additional low rank matrix. LoRA freezes all weights in the original model and learns a low rank matrix $\Delta W:=BA$ for each weight matrix $W\in\mathbb{R}^{D_1\times D_2}$ in the original model:
$$W&amp;rsquo;=W+\frac{\alpha}{r}BA,$$
where $B\in\mathbb{R}^{D_1\times r}$ and $A\in\mathbb{R}^{r\times D_2}$ are learnable parameters, and $\alpha$ is a hyperparameter. For $\Delta W$ to be a low rank matrix, we require $r\ll\min(D1,D2)$.&lt;/p&gt;
&lt;p&gt;To use the original model weights as the starting point of fine-tuning, we need to ensure that $\Delta W=0$ at initialization, which is usually achieved by initializing $B=0$ and $A\sim\mathcal{N}(0,\sigma_A^2I)$ with $\sigma_A=\mathcal{O}(D^{-1/2})$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Evaluation Framework for the Objective Functions of De Novo Drug Design Benchmarks</title>
      <link>https://wenlin-chen.github.io/publication/tripp2022evaluation/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/tripp2022evaluation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bilevel Optimization</title>
      <link>https://wenlin-chen.github.io/post/bilevel_optimization/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/bilevel_optimization/</guid>
      <description>&lt;h3 id=&#34;1-bilevel-optimization&#34;&gt;1. Bilevel Optimization&lt;/h3&gt;
&lt;h4 id=&#34;11-problem-setting&#34;&gt;1.1 Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Consider the following bilevel optimization problem:
$$
\begin{aligned}
x^*&amp;amp;=\underset{x}{\operatorname{argmin}}~f(x,y^*(x))\quad\text{(outer optimization)}, \\
s.t.\quad y^*(x)&amp;amp;=\underset{y}{\operatorname{argmin}}~g(x,y)\quad\text{(inner optimization)}.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-understanding-the-bilevel-objective&#34;&gt;1.2 Understanding the Bilevel Objective&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The bilevel optimization objective can be understood by separately considering the two objectives.
&lt;ul&gt;
&lt;li&gt;Inner optimization: for each given value of $x$, the best response function $y^*(x)$ is defined as the value $y$ that minimizes the inner objective $g(x,y)$.&lt;/li&gt;
&lt;li&gt;Outer optimization: given the best response function $y^{*}(x)$, the optimum $x^*$ is defined as the value $x$ that minimizes the outer objective $f(x,y^{*}(x))$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This results in a nested optimization procedure as evaluating the outer objective $f(x,y^*(x))$ at each value of $x$ requires solving an inner optimization problem to obtain the best response function $y^{*}(x)$ at that value of $x$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-solving-bilevel-optimization&#34;&gt;2. Solving Bilevel Optimization&lt;/h3&gt;
&lt;h4 id=&#34;21-gradient-based-optimization&#34;&gt;2.1 Gradient-based Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The outer objective $f(x,y^{*}(x))$ can be seen as an implicit function of $x$ alone.&lt;/li&gt;
&lt;li&gt;We can calculate the hyper-gradient (i.e., total derivative) of $f(x,y^{*}(x))$ with respect to $x$ using the chain rule:
$$\frac{d f(x,y^{*}(x))}{d x}=\frac{\partial f(x,y^{*}(x))}{\partial x}+\frac{\partial f(x,y^{*}(x))}{\partial y^*(x)}\frac{\partial y^*(x)}{\partial x}.$$
&lt;ul&gt;
&lt;li&gt;The two gradients $\frac{\partial f(x,y^{*}(x))}{\partial x}$ and $\frac{\partial f(x,y^{*}(x))}{\partial y^*(x)}$ can be easily calculated by automatic differentiation.&lt;/li&gt;
&lt;li&gt;Calculating the Jacobian $\frac{\partial y^*(x)}{\partial x}$ is tricky, since the best response function $y^*(x)$ itself is defined by an argmin function in the inner optimization.&lt;/li&gt;
&lt;li&gt;If the inner optimization is also solved by gradient-based optimzation, then calculating the Jacobian $\frac{\partial y^*(x)}{\partial x}$ naively by automatic differentiation will require tracking the gradients through many iterations of the inner optimization, which is computationally intractable in practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-implicit-function-theorem-ift&#34;&gt;2.2 Implicit Function Theorem (IFT)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;An important observation is that $y^*(x)$ is a critical point (i.e., minimum) of $g(x,y)$ for each given $x$.&lt;/li&gt;
&lt;li&gt;This enables us to employ Implicit Function Theorem (IFT) to calculate the Jacobian $\frac{\partial y^*(x)}{\partial x}$ for any $x&amp;rsquo;$:
$$\left.\frac{\partial y^*(x)}{\partial x}\right|_{x=x&amp;rsquo;}=-\left.\left(\frac{\partial^2 g(x,y)}{\partial y \partial y^T}\right)^{-1}\frac{\partial^2 g(x,y)}{\partial y \partial x^T}\right|_{x=x&amp;rsquo;,y=y^*(x&amp;rsquo;)}.$$
&lt;ul&gt;
&lt;li&gt;The mixed partial derivatives $\frac{\partial^2 g(x,y)}{\partial y \partial x^T}$ can be easily calculated by automatic differentiation.&lt;/li&gt;
&lt;li&gt;The inverse Hessian $\left(\frac{\partial^2 g(x,y)}{\partial y \partial y^T}\right)^{-1}$ may be calculated exactly by automatic differentiation or approximated by Neumann approximation or conjugate gradient, depending on the size of the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-applications-in-machine-learning&#34;&gt;3. Applications in Machine Learning&lt;/h3&gt;
&lt;h4 id=&#34;31-hyperparmaeter-optimization&#34;&gt;3.1 Hyperparmaeter Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Let $x$ be the hyperparameters and $y$ be the parameters of a machine learning model.&lt;/li&gt;
&lt;li&gt;Let $f$ be the validation loss function and $g$ be the training loss function.&lt;/li&gt;
&lt;li&gt;Inner optimization corresponds to finding the optimal model parameter $y^*(x)$ by minimizing the training loss $g(x,y)$ given current hyperparameters $x$.&lt;/li&gt;
&lt;li&gt;Outer optimization corresponds to finding the optimal hyperparameters $x^*$ that minimizes the validation loss $f(x,y^{*}(x))$ given the optimal model parameters $y^{*}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-meta-learning&#34;&gt;3.2 Meta-learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Let $x$ be the meta-learned parameters shared across all tasks and $y$ be the task-specific parameters.&lt;/li&gt;
&lt;li&gt;Let $f$ be the validation loss function and $g$ be the training loss function.&lt;/li&gt;
&lt;li&gt;Inner optimization corresponds to finding the optimal task-specific parameters $y^*(x)$ for each task by minimizing the training loss $g(x,y)$ given current meta-learned parameters $x$ shared across all tasks.&lt;/li&gt;
&lt;li&gt;Outer optimization corresponds to finding the optimal meta-learned parameters $x^*$ that minimizes the expected validation loss $f(x,y^{*}(x))$ across tasks given the optimal task-specific parameters $y^{*}(x)$ for each task.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Causal Representation Learning for Latent Space Optimization</title>
      <link>https://wenlin-chen.github.io/publication/chen2021causal/</link>
      <pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2021causal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>To Ensemble or Not Ensemble: When Does End-to-End Training Fail?</title>
      <link>https://wenlin-chen.github.io/publication/webb2020ensemble/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/webb2020ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenlin-chen.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
