<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wenlin Chen</title>
    <link>https://wenlin-chen.github.io/</link>
      <atom:link href="https://wenlin-chen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Wenlin Chen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2024 Wenlin Chen</copyright><lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Wenlin Chen</title>
      <link>https://wenlin-chen.github.io/</link>
    </image>
    
    <item>
      <title>Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks</title>
      <link>https://wenlin-chen.github.io/publication/chen2024neural/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024neural/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training Neural Samplers with Reverse Diffusive KL Divergence</title>
      <link>https://wenlin-chen.github.io/publication/he2025training/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/he2025training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging Task Structures for Improved Identifiability in Neural Network Representations</title>
      <link>https://wenlin-chen.github.io/publication/chen2024leveraging/</link>
      <pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024leveraging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling Variability in Human Annotator Simulation</title>
      <link>https://wenlin-chen.github.io/publication/wu2024modelling/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/wu2024modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Mechanics</title>
      <link>https://wenlin-chen.github.io/post/statistical_mechanics/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/statistical_mechanics/</guid>
      <description>&lt;h3 id=&#34;what-is-statistical-mechanics&#34;&gt;What is Statistical Mechanics?&lt;/h3&gt;
&lt;p&gt;Statistical mechanics is not a natural law like Newton&amp;rsquo;s laws of motion which were discovered from experimental observations and are assumed to (approximately) reflect fundamental aspects of reality. Instead, it is a set of mathematical techniques that can be applied to study any physical system. Beyond physics, many ideas in machine learning were also inspired by statistical mechanics.&lt;/p&gt;
&lt;h3 id=&#34;statistical-description-of-physical-systems&#34;&gt;Statistical Description of Physical Systems&lt;/h3&gt;
&lt;h4 id=&#34;microstates-and-macrostates&#34;&gt;Microstates and Macrostates&lt;/h4&gt;
&lt;p&gt;A microstate defines the values of all possible microscopic variables, such as position and momentum of every particle in a classical system of point particles.&lt;/p&gt;
&lt;!-- We make a fundamental assumption that any physical system can exist in a discrete set of microstates (possibly infinite). Note that continious variables such as position and momentum can be discretized. --&gt;
&lt;p&gt;A macrostate defines the values of all possible macroscopic variables, such as the total energy, volumn, pressure, and temperature of a system, and the total number of particles in a system. We are usually more interested in macrostates since the number of microstates is typically huge, which is difficult to measure and in far more details than we usually care about.&lt;/p&gt;
&lt;p&gt;Statistical mechanics aims to create a bridge between the microscopic and macroscopic descriptions of a physical system.&lt;/p&gt;
&lt;h4 id=&#34;density-of-states&#34;&gt;Density of States&lt;/h4&gt;
&lt;p&gt;There can be a huge number of microstates all corresponding to the same macrostate (e.g., there is an enormous number of configurations of individual particles which all add up to the same energy and volumn). The number of microstates corresponding to a macrostate is defined as the density of the (macro)state:
$$
\Omega(E,V,\cdots).
$$
$\Omega$ is a function of all macroscopic variables defining the macrostate and can be thought of as the number of microstates per unit volumn in the space defined by those macroscopic variables.&lt;/p&gt;
&lt;p&gt;As an example, the energy of a system of $N$ free particles is given by
$$E=\frac{|\mathbf{p}|^2}{2m},$$
where $\mathbf{p}$ contains $3N$ momentum variables. Note that each value of $E$ corresponds to the surface of a $(3N-1)$-dimensional hypersphere in this $3N$-dimensional space of momentum. Therefore, the density of the state is proportional to the surface area of the $(3N-1)$-dimensional hypersphere:
$$
\Omega(E)\propto |\mathbf{p}|^{3N-1}\propto E^{(3N-1)/2}.
$$
Note that for a large system with many particles ($N&amp;gt; &amp;gt;1$), even a tiny increase of the energy $E$ will result in a huge increase in the density of states. Therefore, we will work with a smoother and slower changing function $\log\Omega$.&lt;/p&gt;
&lt;h4 id=&#34;time-averages-and-ensemble-averages&#34;&gt;Time Averages and Ensemble Averages&lt;/h4&gt;
&lt;p&gt;We usually have no idea which microstate a system is in even if we know its current macrostate. However, this can be described by probabilities.&lt;/p&gt;
&lt;p&gt;For a system in a given macrostate, there are two approaches to defining the probability of the system being in a microstate.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The degrees of freedom making up the system are constantly changing. Conduct an experiment or simulation over a sufficiently long time interval. Define the probability of the system being in a microstate as the fraction of time spending in that microstate. Averages computed using this definition of probability are called time averages.&lt;/li&gt;
&lt;li&gt;Imagine preparing many identical systems following exactly the same procedure. The set of all these system are called a statistical ensemble, which are in the same macrostate but can be in different microstates. Define the probability of the system being in a microstate as the fraction of systems in the ensemble which are in that microstate. Averages computed using this definition of probability are called ensemble averages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;ergodicity-and-equilibrium&#34;&gt;Ergodicity and Equilibrium&lt;/h4&gt;
&lt;p&gt;A system for which time averages and ensemble averages are equal is said to be ergodic. Ergodicity is a property of the system that it will eventually visit all parts of the space as time evloves and thus cannot be reduced or factored into smaller components.&lt;/p&gt;
&lt;p&gt;The postulate of equal a priori probabilities states that a system has an equal probability of being in any microstate that is consistent with its macrostate. An isolated system that satisfies the postulate of equal a priori probabilities is said to be in equilibrium.&lt;/p&gt;
&lt;p&gt;By the second law of thermodynamics and the maximum entropy property of the uniform distribution, it can be shown that once an isolated system satisfies the postulate of equal a priori probabilities even for a moment, it will continue satisfying it forever. This implies that an isolated system is in equilibrium if the probability distribution over its microstates does not change over time.&lt;/p&gt;
&lt;h4 id=&#34;the-maxwell-boltzmann-distribution&#34;&gt;The Maxwell-Boltzmann Distribution&lt;/h4&gt;
&lt;p&gt;Real systems are usually not isolated. To analyze non-isolated systems, we consider an isolated system which is splitted into two parts A and B. A is the part we care about, on which we do experiments. B is called a heat bath, which is the environment that A is connected to. We only care about B to the extent that it affectes A. Some examples are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A is the gas contained in a box. B is the box along with the whole room where the box is placed.&lt;/li&gt;
&lt;li&gt;A is a test tube with chemicals in it. B is a water bath the test tube is sitting in.&lt;/li&gt;
&lt;li&gt;A is the carbon dioxide molecules in the air in a room. B is everything else in the air in that room.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The total energy $E_T$ of the whole system is a sum of three terms:
$$E_T=E_A+E_B+E_{AB},$$
where $E_A$ depends only on the degrees of freedom that make up A, $E_B$ only depends on the degrees of freedom that make up B, and $E_{AB}$ depends on both parts of the system due to the interaction of A and B.&lt;/p&gt;
&lt;p&gt;We assume that A and B are wealy coupled. This has two implications:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The interaction energy $E_{AB}$ is negligible compared to the individual energies $E_A$ and $E_B$ and their variations. Therefore, the total energy becomes
$$E_T\approx E_A+E_B.$$&lt;/li&gt;
&lt;li&gt;The total degress of freedom are specified independently. Therefore, the density of states factorizes and we are free to choose states for A and B independently:
$$\Omega_T(E_T)\approx\Omega_A(E_A)\Omega_B(E_B).$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also assume that B is much larger than A in the sense that any fluctuation of $E_A$ has negligible effect on $E_B$. This means that we only care about a very small range of values for $E_B$, and therefore the log density of states of B can be approximated by a linear function:
$$\log\Omega_B(E_B)\approx\alpha+\beta E_B.$$&lt;/p&gt;
&lt;p&gt;The Maxwell-Boltzmann distribution answers the following question: if the total energy of the system in equilibrium is $E_T$, what is the probability of A being in a particular microstate $i$?
Since the system is in equilibrium, the probability of A being in the microstate $i$ equals the fraction of microstates of the whole system for which A is in the microstate $i$, which is proportional to the number of microstates of B given A is in the microstate $i$:
$$p_i\propto\Omega_B(E_T-E_i)\propto e^{-\beta E_i},$$
where $\beta$ is called the inverse temperature. We can define the temperature $T$ by
$$\frac{1}{kT}\equiv\beta=\frac{\partial\log\Omega_B(E_B)}{\partial E_B},$$
where $k=1.3806488\times 10^{-23}$ J/K is the Boltzmann constant.&lt;/p&gt;
&lt;p&gt;With the above definition, we can write down the Maxwell-Boltzmann distribution:
$$p_i=\frac{e^{-E_i/kT}}{Z}.$$
The normalizing constant is called the partition function, which is given by
$$Z=\sum_i e^{-E_i/kT},$$
where the sum is taken over all microstates of A.&lt;/p&gt;
&lt;h4 id=&#34;thermodynamic-forces&#34;&gt;Thermodynamic Forces&lt;/h4&gt;
&lt;p&gt;More generally, we usually care about the energy as well as other macroscopic variables.&lt;/p&gt;
&lt;p&gt;Suppose that system A is a balloon filled with helium which can exchange both energy $E$ and volumn $V$ with the surrounding air B. We repeat the same argument above and assume the log density of states of B is linear in both energy and volumn:
$$\log\Omega_B(E_B,V_B)\approx\alpha+\beta E_B+\gamma V_B.$$
Then, the probability of A being in a particular microstate $i$ is given by
$$p_i\propto\Omega_B(E_T-E_i,V_T-V_i)\propto e^{-\beta E_i-\gamma V_i}.$$
Conventionally, we define a variable called pressure as
$$P\equiv kT\frac{\partial\log\Omega_B(E_B,V_B)}{\partial V_B},$$
and rewrite the probability of A being in the microstate $i$ as
$$p_i=\frac{e^{-(E_i+PV_i)/kT}}{Z},$$
where the partition function is given by
$$Z=\sum_i e^{-(E_i+PV_i)/kT}.$$&lt;/p&gt;
&lt;p&gt;Now, suppose that system A is a (fixed volumn) box filled with gas, with a small hole allowing the molecules to diffuse in from and out to the room B where the box is placed. This means that A can exchange both energy $E$ and the number of particles $N$ with B.&lt;/p&gt;
&lt;p&gt;Similarly, we define a variable called chemical potential as
$$\mu\equiv-kT\frac{\partial\log\Omega_B(E_B,N_B)}{\partial N_B}.$$
Then, the probability of A being in the microstate $i$ is given by
$$p_i=\frac{e^{-(E_i-\mu N_i)/kT}}{Z}.$$&lt;/p&gt;
&lt;p&gt;Quantities like pressure $P$ and chemical potential $\mu$ are called thermodynamic forces, which are conjugate to the corresponding macroscopic variables $V$ and $N$ that the log density of states is differentiated with respect to. We say that $(V,P)$ and $(N,\mu)$ form conjugate pairs.&lt;/p&gt;
&lt;h4 id=&#34;entropy-and-probabilities-of-macrostates&#34;&gt;Entropy and Probabilities of Macrostates&lt;/h4&gt;
&lt;p&gt;Having defined the probability of A being in a microstate $i$, we now define the probability of a macrostate with energy $E_A$:
$$p(E_A)=\sum_{i:E_i=E_A}\frac{e^{-E_i/kT}}{Z}=\Omega_A(E_A)\frac{e^{-E_A/kT}}{Z}.$$
We define the entropy of a macrostate as
$$S(E_A)=k\cdot\log\Omega_A(E_A),$$
which is just another way of measuring the number of microstates corresponding to the macrostate with energy $E_A$.&lt;/p&gt;
&lt;p&gt;Given the definition of entropy, we can rewrite the probability of a macrostate as
$$p(E_A)=\frac{e^{-(E_A-TS(E_A))/kT}}{Z}.$$
This leads to the definition of the Helmholtz free energy:
$$F=E-TS(E).$$&lt;/p&gt;
&lt;h4 id=&#34;thermodynamic-potentials-and-free-energy&#34;&gt;Thermodynamic Potentials and Free Energy&lt;/h4&gt;
&lt;p&gt;We have studied how to compute the probability of finding a system in a microstate or macrostate specified by an arbitrary set of macroscopic variables. Interestingly, in every case, the probability takes exactly the same form:
$$p=\frac{e^{-\Phi/kT}}{Z},$$
where $\Phi$ is called thermodynamic potential, which is a energy-like function that defines different kinds of probability distributions for the system.&lt;/p&gt;
&lt;p&gt;Some common thermodynamic potentials with special names are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Energy: $E$.&lt;/li&gt;
&lt;li&gt;Helmholtz free energy: $F=E-TS(E)$.&lt;/li&gt;
&lt;li&gt;Enthalpy: $H=E+PV$.&lt;/li&gt;
&lt;li&gt;Gibbs free energy: $G=E+PV-TS(E,V)$.&lt;/li&gt;
&lt;li&gt;Grand potential: $\Phi_G=E-\mu N-TS(E,N)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The term free energy generally refers to any thermodynamic potential that includes a $-TS$ term, which describes the probability of a macrostate.&lt;/p&gt;
&lt;p&gt;There are two ways of understanding thermodynamic potentials:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We start with $E$ and add in terms based on the ensemble that we want to use. If we want the volumn to be variable, then add the $PV$ term. If we want the number of particles to be variable, then add the $-\mu N$ term. If we want to work with macrostates, then add the $-TS$ term.&lt;/li&gt;
&lt;li&gt;We start with a single potential that includes all possible terms. If the volumn is fixed, then $PV$ is constant and can be ignored. If the number of particles is fixed, then $-\mu N$ is constant and can be ignored. If we want to work with microstates, then $TS=0$ since a microstate can be thought of as a macrostate with a single microstate (so the entropy $S$ is zero).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some common ensembles with special names are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Microcanonical ensemble refers to an isolated system with fixed energy. Therefore, it has equal probability of being in any microstate with that specific energy and no chance of being in any microstate with a different energy.&lt;/li&gt;
&lt;li&gt;Canonical ensemble refers to a system that can exchange energy with a heat bath at a fixed temperature. The corresponding thermodynamic potential is $E$ for microstates and $E-TS(E)$ for macrostates.&lt;/li&gt;
&lt;li&gt;Grand canonical ensemble refers to a system that can exhcange both energy and particles with a heat bath of a fixed temperature and chemical potential. The corresponding thermodynamic potential is $E-\mu N$ for microstates and $E-\mu N-TS(E,N)$ for macrostates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;averages-and-very-large-numbers&#34;&gt;Averages and Very Large Numbers&lt;/h4&gt;
&lt;p&gt;The ensemble average or mean of a quantity $x$ is defined as
$$\left&amp;lt;x\right&amp;gt;=\sum_i x_i p_i=\frac{\sum_i x_i e^{-\Phi_i}/kT}{\sum_i e^{-\Phi_i}/kT},$$
where $x_i$ is the value of the quantity $x$ in the $i$-th state, $p_i$ is the probability of being in that state, and the sum can be over either microstates or macrostates depending on whether $x$ is a microscopic variable or macroscopic variable. It can be shown that
$$\left&amp;lt;x+y\right&amp;gt;=\left&amp;lt;x\right&amp;gt;+\left&amp;lt;y\right&amp;gt;\quad\text{and}\quad\left&amp;lt;Cx\right&amp;gt;=C\left&amp;lt;x\right&amp;gt;.$$&lt;/p&gt;
&lt;p&gt;The variance of a quantity $x$ measures its fluctuations, which is how much $x$ tends to vary about its average:
$$\sigma(x)^2=\left&amp;lt;(x-\left&amp;lt;x\right&amp;gt;)^2\right&amp;gt;=\left&amp;lt;x^2\right&amp;gt;-\left&amp;lt;x\right&amp;gt;^2.$$
The square root of variance is called standard deviation $\sigma(x)$, which also measures the fluctuations of $x$ but has the dimensions as $x$. Roughly speaking, the value of $x$ is inbetween $\left&amp;lt;x\right&amp;gt;\pm\sigma(x)$ about 2/3 of the time.&lt;/p&gt;
&lt;p&gt;Most problems in statistical mechanics tend to involve very large numbers, in which cases averages can simply be treated as exact numbers since any variation about the average value is extremely small. In other words, the chance of measuring any value except the average is negligible. This great simplification is one of the reasons why statistical mechanics is so successful.&lt;/p&gt;
&lt;p&gt;The central limit theorem states that the sum of i.i.d. random variables $x_i$ with mean $\mu_x$ and standard deviation $\sigma_x$ is distributed as a normal distribution with mean $\mu_S=N\mu_x$ and standard deviation $\sigma_S=\sqrt{N}\sigma_x$ in the limit $N\to\infty$:
$$S=\sum_{i=1}^N x_i\sim\mathcal{N}(N\mu_x,\sqrt{N}\sigma_x),\quad\text{as }~N\to\infty.$$
In practice, the convergence rate to the normal distribution depends on the distribution of $x_i$. Usually, $N=10$ already gives a good approximation to a normal distribution.&lt;/p&gt;
&lt;p&gt;We usually care about the the ratio of the standard deviation and the average:
$$\frac{\sigma_S}{\mu_S}\propto\frac{1}{\sqrt{N}},\quad\text{as }~N\to\infty.$$
As an example, a typical sized room contains $N=10^{26}$ oxygen molecules. Let $x_i$ denote whether the $i$-th oxygen molecule is in the left side of the room. The fluctuations over the average of the number of oxygen molecules in the left side of the room is $10^{-13}$, which is negligible as it is too small to measure.&lt;/p&gt;
&lt;h4 id=&#34;partition-function&#34;&gt;Partition Function&lt;/h4&gt;
&lt;p&gt;Recall that the partition function is defined as
$$Z=\sum_i e^{-\beta\Phi_i}=\sum_i e^{-\Phi_i/kT},$$
which has some interesting and useful properties. In particular, the derivatives of $\log Z$ tend to give averages.&lt;/p&gt;
&lt;p&gt;For example, the average thermodynamic potential $\left&amp;lt;\Phi\right&amp;gt;$ can be obtained by
$$
\begin{aligned}
-\frac{\partial\log Z}{\partial\beta}
&amp;amp;=-\frac{1}{Z}\frac{\partial Z}{\partial\beta} \\
&amp;amp;=-\frac{1}{Z}\sum_i \frac{\partial e^{-\beta\Phi_i}}{\partial\beta} \\
&amp;amp;= \frac{1}{Z}\sum_i \Phi_i e^{-\beta\Phi_i} \\
&amp;amp;= \left&amp;lt;\Phi\right&amp;gt;.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Another useful trick is to take the derivative with respect to a continuous microscopic or macroscopic variable $x$, which gives
$$
\begin{aligned}
-kT\frac{\partial\log Z}{\partial x}
&amp;amp;=-\frac{kT}{Z}\frac{\partial Z}{\partial x} \\
&amp;amp;=-\frac{kT}{Z}\int\frac{\partial e^{-\Phi(x)/kT}}{\partial x}dx \\
&amp;amp;=\frac{1}{Z}\int\frac{\partial\Phi(x)}{\partial x}e^{-\Phi(x)/kT}dx \\
&amp;amp;=\left&amp;lt;\frac{\partial\Phi(x)}{\partial x}\right&amp;gt;.
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;interpretation-of-statistical-quantities&#34;&gt;Interpretation of Statistical Quantities&lt;/h3&gt;
&lt;p&gt;We have derived statistical definitions of many abstract mathematical quantities such as temperature. Now we try to build up an intuitive understanding of what they are in the physical world.&lt;/p&gt;
&lt;h4 id=&#34;temperature&#34;&gt;Temperature&lt;/h4&gt;
&lt;p&gt;Temperature measures the microscopic jiggling of atoms: the faster they move, the hotter an object feels. To see this and understand the significance of temperature, we prove the equipartition theorem below.&lt;/p&gt;
&lt;p&gt;Consider a general energy function that can be seperated into the sum of two terms:
$$E(x_1,x_2,\cdots)=E_1(x_1)+E_2(x_2,x_3,\cdots),$$
where the energy is quadratic in $x_1$:
$$E(x_1)=Cx_1^2.$$
One particularly important example is the momentum $p_i$ of a classical particle, which corresponds to the kenetic energy:
$$E_k(p_i)=\frac{p_i^2}{2m}.$$
Now, we compute the average value of $E_1$:
$$
\begin{aligned}
\left&amp;lt;E_1(x_1)\right&amp;gt; &amp;amp;= \frac{\int E_1(x_1)e^{-\beta E(x_1,x_2,\cdots)}dx_1dx_2\cdots}{\int e^{-\beta E(x_1,x_2,\cdots)}dx_1dx_2\cdots} \\
&amp;amp;= \frac{\int e^{-\beta E_2(x_2,x_3,\cdots)}dx_2dx_3\cdots\int E_1(x_1)e^{-\beta E_1(x_1)}dx_1}{e^{-\beta E_2(x_2,x_3,\cdots)}dx_2dx_3\cdots\int e^{-\beta E_1(x_1)}dx_1} \\
&amp;amp;= \frac{\int E_1(x_1)e^{-\beta E_1(x_1)}dx_1}{\int e^{-\beta E_1(x_1)}dx_1} \\
&amp;amp;=-\frac{\partial\log\int e^{-\beta E_1(x_1)}dx_1}{\partial\beta} \\
&amp;amp;=-\frac{\partial\log\int e^{-\beta Cx_1^2}dx_1}{\partial\beta} \\
&amp;amp;=-\frac{\partial(\log\beta^{-1/2}+\log\int e^{-\beta Cx_1^2}d(\beta^{1/2}x_1))}{\partial\beta} \\
&amp;amp;=\frac{1}{2\beta} \\
&amp;amp;=\frac{kT}{2}.
\end{aligned}
$$
This tells us that temperature is a measure of average kinetic energy, which shows how the statistical definition of temperature matches up with the physical one: if a system is in equilibrium with a heat bath at temperature $T$, then its average kinetic energy per degree of freedom is $\left&amp;lt; E_k\right&amp;gt;=kT/2$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diffusive Gibbs Sampling</title>
      <link>https://wenlin-chen.github.io/publication/chen2024diffusive/</link>
      <pubDate>Sun, 21 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2024diffusive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Diffusion and Latent Variable Models</title>
      <link>https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/</guid>
      <description>&lt;h3 id=&#34;latent-variable-models&#34;&gt;Latent Variable Models&lt;/h3&gt;
&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Observed data: $x\in\mathcal{X}\subseteq\mathbb{R}^n$.&lt;/li&gt;
&lt;li&gt;Latent variable: $z\in\mathbb{R}^d$ ($d\leq n$).&lt;/li&gt;
&lt;li&gt;The joint distribution is factorized as the product of likelihood and prior:
$$p_{\theta}(x,z)=p_{\theta}(x|z)p(z).$$
&lt;ul&gt;
&lt;li&gt;The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network:
$$p_{\theta}(x|z)=\mathcal{N}(x|\mu_{\theta}(z),\sigma^2I).$$&lt;/li&gt;
&lt;li&gt;The prior is often chosen to be a simple distribution like standard Gaussian:
$$p(z)=\mathcal{N}(z|0,I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The marginal likelihood (or model evidence) is usually used for model selection:
$$p_{\theta}(x)=\int p_{\theta}(x|z)p(z) dz.$$
&lt;ul&gt;
&lt;li&gt;Maximum marginal likelihood learning $\max_{\theta}\log p_{\theta}(x)$ in intractable due to the intractable integral.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The posterior is given by Bayes&amp;rsquo; rule:
$$p_{\theta}(z|x)=\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(x)}=\frac{p_{\theta}(x|z)p(z)}{\int p_{\theta}(x|z)p(z) dz}.$$
&lt;ul&gt;
&lt;li&gt;The posterior is also intractable due to the intractability of the marginal likelihood.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;variational-autoencoders-vaes&#34;&gt;Variational Autoencoders (VAEs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;VAEs introduce an amortized mean-field Gaussian variational posterior (or encoder/inference model) with mean and diagnoal variance parameterized by neural networks:
$$q_{\phi}(z|x)=\mathcal{N}(z|\tilde{\mu}_{\phi}(x),\text{diag}(\tilde{\sigma}_{\phi}(x)^2))\approx p_{\theta}(z|x).$$&lt;/li&gt;
&lt;li&gt;The parameters of both the generation and inference models are jointly learned by maximizing a tractable evidence lower bound (ELBO) of the log marginal likelihood:
$$\log p_{\theta}(x)\geq\mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\right]=\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]-\text{KL}(q_{\phi}(z|x)||p(z))=\mathcal{F}(\theta,\phi).$$
&lt;ul&gt;
&lt;li&gt;The first term $\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]$ controls the reconstruction error.&lt;/li&gt;
&lt;li&gt;The second term $\text{KL}(q_{\phi}(z|x)||p(z))$ regularizes the approximate posterior to be close to the prior.&lt;/li&gt;
&lt;li&gt;In order to backpropagate through samples $z\sim q_{\phi}(z|x)$, we employ the reparameterization trick:
$$z=\tilde{\mu}_{\phi}(x)+\tilde{\sigma}_{\phi}(x)\odot \epsilon,\quad \epsilon\sim N(0,I).$$&lt;/li&gt;
&lt;li&gt;It is quite hard to train both generation and inference models well in practice due to difficulties in balancing the two terms above during optimization (e.g., the variational posterior can easily collapse to the prior due to variational overpruning).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The gap between the ELBO and log marginal likelihood is given by:
$$\log p_{\theta}(x)-\mathcal{F}(\theta,\phi)=\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\geq 0.$$
&lt;ul&gt;
&lt;li&gt;Maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posteriors.&lt;/li&gt;
&lt;li&gt;The ELBO attains equality when the approximate posterior is exactly the same as the true posterior:
$$\log p_{\theta}(x)=\mathcal{F}(\theta,\phi)\iff\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))=0\iff q_{\phi}(z|x)=p_{\theta}(z|x).$$&lt;/li&gt;
&lt;li&gt;However, the mean-field variational poterior $q_{\phi}(z|x)$ is uni-modal and the reverse KL divergence is known to be mode-seeking instead of mass convering. This means that $q_{\phi}(z|x)$ can only fit to one of the modes in the true posterior $p_{\theta}(z|x)$, which limits the expressivity of VAEs and makes it hard for the aggregated poseterior $q_{\phi}(z)=\int q_{\phi}(z|x)p_{\text{data}}(x)dx$ to be close to the prior $p(z)=\mathcal{N}(0,I)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diffusion-and-hierarchical-latent-variable-models&#34;&gt;Diffusion and Hierarchical Latent Variable Models&lt;/h3&gt;
&lt;h4 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We slightly change the notation following the convention of hierarchical latent variable models.
&lt;ul&gt;
&lt;li&gt;Observed data: $x_0=x\in\mathcal{X}\subseteq\mathbb{R}^n$.&lt;/li&gt;
&lt;li&gt;We employ a sequence of latent variables $x_1,\cdots,x_T\in\mathbb{R}^d$ ($d\leq n$) to capture data representation at different levels.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The results in a new marginal likelihood:
$$p_{\theta}(x_0)=\int p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)dx_{1:T}.$$&lt;/li&gt;
&lt;li&gt;The prior over the last latent variable is a Standard Gaussian:
$$p(x_T)=\mathcal{N}(x_T|0,I).$$&lt;/li&gt;
&lt;li&gt;The likelihoods (or decoders/generation models) are Gaussians with mean parameterized by a time-dependent neural network:
$$p_{\theta}(x_{t-1}|x_t)=\mathcal{N}(x_{t-1}|\mu_{\theta}(x_t,t),\sigma_t^2 I).$$&lt;/li&gt;
&lt;li&gt;The posterior is intractable as before:
$$p_{\theta}(x_{1:T}|x_0)=\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{p_{\theta}(x_0)}.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hierarchical-variational-autoencoders-hvaes&#34;&gt;Hierarchical Variational Autoencoders (HVAEs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HVAEs approximate the intractable posterior with an amortized Gaussian variational posterior (or encoder/inference model):
$$q_{\phi}(x_{1:T}|x_0)\approx p_{\theta}(x_{1:T}|x_0).$$&lt;/li&gt;
&lt;li&gt;There are different design choices for the factorization of the inference model.
&lt;ul&gt;
&lt;li&gt;Bottom-up factorization:
$$q_{\phi}(x_{1:T}|x_0)=\prod_{t=1}^T q_{\phi}(x_t|x_{t-1}).$$&lt;/li&gt;
&lt;li&gt;Top-down factorization:
$$q_{\phi}(x_{1:T}|x_0)=q_{\phi}(x_T|x_0)\prod_{t=2}^T q_{\phi}(x_{t-1}|x_t,x_0).$$&lt;/li&gt;
&lt;li&gt;The mean and diagnoal variance of the factors $q_{\phi}(x_t|x_{t-1})$ and $q_{\phi}(x_{t-1}|x_t,x_0)$ are parameterized by neural networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The tractable ELBO can be derived for HVAEs:
$$\log p_{\theta}(x_0)\geq\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{q_{\phi}(x_{1:T}|x_0)}\right]=\mathcal{F}(\theta,\phi).$$
&lt;ul&gt;
&lt;li&gt;For the bottom-up factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=1}^{T-1}\text{KL}(q_{\phi}(x_t|x_{t-1})||p_{\theta}(x_t|x_{t+1}))-\text{KL}(q_{\phi}(x_T|x_{T-1})||p(x_T))\right].$$&lt;/li&gt;
&lt;li&gt;For the top-down factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q_{\phi}(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q_{\phi}(x_T|x_0)||p(x_T))\right].$$&lt;/li&gt;
&lt;li&gt;The parameters of both generation and inference models are learned jointly by maximizing the ELBO.&lt;/li&gt;
&lt;li&gt;To reduce the model size, parameter sharing between the generation and inference models is possible.&lt;/li&gt;
&lt;li&gt;HVAEs are very flexible and expressive, since only the last latent variable $x_T$ is constrained to be standard Gaussian and the model will need to figure out the intermediate latent variables $x_{1:T-1}$.&lt;/li&gt;
&lt;li&gt;This causes even more difficulties during optimization than VAEs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;denoising-diffusion-probabilistic-models-ddpms&#34;&gt;Denoising Diffusion Probabilistic Models (DDPMs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DDPMs are a class of state-of-the-art deep generative models.&lt;/li&gt;
&lt;li&gt;DDPMs define a fixed bottom-up inference model (or diffusion process):
$$q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}).$$
&lt;ul&gt;
&lt;li&gt;No dimensionality reduction is performed $(d=n)$.&lt;/li&gt;
&lt;li&gt;The factors are predefined Gaussian convolution kernels:
$$q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I).$$
$$\implies q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I),\quad \bar{\alpha}_t=\prod_{s=1}^t \alpha_s.$$&lt;/li&gt;
&lt;li&gt;The top-down form of the inference model is analytically tractable:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1}|\tilde{\mu}(x_t,x_0),\tilde{\sigma}_t^2),$$
where
$$\tilde{\mu}(x_t,x_0)=\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t,$$
$$\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t}.$$
&lt;ul&gt;
&lt;li&gt;Sketch of proof: By Bayes&amp;rsquo; rule and the Markovian property, we have
$$q(x_{t-1}|x_t,x_0)= \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\propto q(x_t|x_{t-1})q(x_{t-1}|x_0).$$
Since everything is Gaussian with analytical expression, $\tilde{\mu}(x_t,x_0)$ and $\tilde{\sigma}_t^2$ can be figured out by moment matching.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The tractable ELBO can be derived for DDPMs:
$$
\begin{aligned}
\mathcal{F}(\theta)
&amp;amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q(x_T|x_0)||p(x_T))\right] \\
&amp;amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T \frac{\lVert\tilde{\mu}(x_t,x_0)-\mu_{\theta}(x_t,t)\rVert^2}{2\sigma_t^2}\right]+const.
\end{aligned}
$$
&lt;ul&gt;
&lt;li&gt;In constrast to HVAEs with flexible intermediate latent variables, the fixed inference model in DDPMs provides extra supervision signals for the intermediate latent variables.&lt;/li&gt;
&lt;li&gt;The objective for each intermediate latent variable looks like a regression objective and is much easier to optimize.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The ELBO can be further simplified using variance reduction techniques with the reparameterization trick:
&lt;ol&gt;
&lt;li&gt;Reparameterize $x_0$ according to the predefined inference model $q(x_t|x_0)$:
$$x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\quad\implies\quad x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}},\quad\epsilon\sim\mathcal{N}(0,I).$$&lt;/li&gt;
&lt;li&gt;Plug in the reparameterized $x_0$ to $\tilde{\mu}(x_t,x_0)$:
$$\tilde{\mu}(x_t,x_0)=\tilde{\mu}\left(x_t,\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right).$$&lt;/li&gt;
&lt;li&gt;Reparameterize $\mu_{\theta}(x_t,t)$ in the same form as $\tilde{\mu}(x_t,x_0)$:
$$\mu_{\theta}(x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right).$$&lt;/li&gt;
&lt;li&gt;The regression objectives can then be simplified to noise matching with a time-dependent noise prediction network $\epsilon_{\theta}(x_t,t)$:
$$\sum_{t=1}^T\lambda(t)\mathbb{E}_{\mathcal{N}(\epsilon|0,I)}[\lVert\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The analytical solution for the weighting function is given by
$$\lambda(t)=\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}.$$&lt;/li&gt;
&lt;li&gt;However, setting $\lambda(t)=1$ makes training much more stable in practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right)+\sigma_t z_t,\quad z_t\sim\mathcal{N}(0,I).$$
&lt;ul&gt;
&lt;li&gt;The variance of $p_{\theta}(x_{t-1}|x_t)$ can be set to
$$\sigma_t^2=1-\alpha_t\quad\text{or}\quad \sigma_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t},$$
which work equally well in practice for large $T$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Connection to score matching and score-based diffuson models:
&lt;ul&gt;
&lt;li&gt;Denoising score matching with a time-dependent score network $s_{\theta}(x_t, t)$:
$$
\begin{aligned}
\mathbb{E}_{q(x_t|x_0)}\left[\lVert\nabla_{x_t}\log q(x_t|x_0)-s_{\theta}(x_t, t)\rVert^2\right]
&amp;amp;=\mathbb{E}_{q(x_t|x_0)}\left[\left\lVert\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{1-\bar{\alpha}_t}+s_{\theta}(x_t, t)\right\rVert^2\right] \\
&amp;amp;= \mathbb{E}_{\mathcal{N}(\epsilon|0,I)}\left[\left\lVert\frac{\epsilon}{\sqrt{1-\bar{\alpha}_t}}+s_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, t)\right\rVert^2\right].
\end{aligned}
$$
Therefore, the score network $s_{\theta}(x_t,t)$ and the noise prediction network $\epsilon_{\theta}(x_t,t)$ have the following connection:
$$s_{\theta}(x_t,t)=-\frac{\epsilon_{\theta}(x_t,t)}{\sqrt{1-\bar{\alpha}_t}}.$$&lt;/li&gt;
&lt;li&gt;The sampling procedure of DDPM corresponds to the predictor step (numerical SDE solution with discretization) in the predictor-corrector sampling precedure of score-based diffusion models. One could also in principle introduce the corrector step in DDPM sampling by leveraging the relationship between $s_{\theta}(x_t, t)$ and $\epsilon_{\theta}(x_t, t)$ above.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;denoising-diffusion-implicit-models-ddims&#34;&gt;Denoising Diffusion Implicit Models (DDIMs)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DDIMs define a fixed top-down inference model:
$$q(x_{1:T}|x_0)=q(x_T|x_0)\prod_{t=2}^T q(x_{t-1}|x_t,x_0).$$
&lt;ul&gt;
&lt;li&gt;No dimensionality reduction is performed $(d=n)$.&lt;/li&gt;
&lt;li&gt;The factors are predefined Gaussians to ensure that $q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0+(1-\bar{\alpha}_t)I)$ as in DDPMs:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}\left(x_{t-1}\left|\sqrt{\bar{\alpha}_{t-1}}x_0+\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}}\sqrt{1-\bar{\alpha}_{t-1}-s_t^2},s_t^2\right)\right..$$
&lt;ul&gt;
&lt;li&gt;Note that the forward diffusion process induced by a DDIM is not necessarily Markovian:
$$q(x_t|x_{t-1},x_0)=\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)},$$
since $x_t$ could depend on both $x_{t-1}$ and $x_0$ for some choices of the variance $s_t^2$.&lt;/li&gt;
&lt;li&gt;The variance $s_t^2$ controls how stochastic the forward diffusion process is.
&lt;ul&gt;
&lt;li&gt;$x_{t-1}$ becomes a deterministic function of $x_t$ and $x_0$ for $s_t=0$.&lt;/li&gt;
&lt;li&gt;If we set the variance $s_t^2$ as in DDPM:
$$s_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t},$$
then DDIM recovers DDPM which has a markovian forward diffusion process:
$$q(x_t|x_{t-1},x_0)=q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_0+(1-\alpha_t)I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The generation models are defined as
$$\begin{aligned}
p_{\theta}(x_{t-1}|x_t)
&amp;amp;=\int q(x_{t-1}|x_t,x_0)p_{\theta}(x_0|x_t)dx_0 \\
&amp;amp;\approx q(x_{t-1}|x_t,\tilde{x}_0(x_t)),\quad \tilde{x}_0(x_t)\sim p_{\theta}(x_0|x_t).
\end{aligned}$$
where we reparameterize $\tilde{x}_0(x_t)$ according to the forward diffusion process:
$$
\tilde{x}_0(x_t)=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)}{\sqrt{\bar{\alpha}_t}}.
$$&lt;/li&gt;
&lt;li&gt;The training objective for DDIM has the identical form (up to an additive constant) to that for DDPM with a different weighting function $\lambda(t)$ for each choice of the variance $s_t^2$ in $q(x_{t-1}|x_t,x_0)$.&lt;/li&gt;
&lt;li&gt;Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)}{\sqrt{\alpha_t}}+\epsilon_{\theta}(x_t, t)\sqrt{1-\bar{\alpha}_{t-1}-s_t^2}+s_t e,\quad e\sim\mathcal{N}(0,I).$$
&lt;ul&gt;
&lt;li&gt;DDPM is a special case with $s_t^2=\tilde{\sigma}_t^2$.&lt;/li&gt;
&lt;li&gt;$s_t=0$ results in a deterministic generation process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Score and Flow Matching</title>
      <link>https://wenlin-chen.github.io/post/score_flow_matching/</link>
      <pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/score_flow_matching/</guid>
      <description>&lt;h3 id=&#34;generative-modeling&#34;&gt;Generative Modeling&lt;/h3&gt;
&lt;h4 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data distribution $p_{\text{data}}(x)$ is unknown.&lt;/li&gt;
&lt;li&gt;Samples from $p_{\text{data}}(x)$ are available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;goals&#34;&gt;Goals&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the density of the data distribution $p_{\text{data}}(x)$.&lt;/li&gt;
&lt;li&gt;Generate new samples from $p_{\text{data}}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;score-matching-for-diffusion-models&#34;&gt;Score Matching for Diffusion Models&lt;/h3&gt;
&lt;h4 id=&#34;notation&#34;&gt;Notation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denote data by $x_0=x$ and noise by $x_T$.&lt;/li&gt;
&lt;li&gt;Wiener process SDE ($dt&amp;gt;0$):
$$dw_t=z\sqrt{dt},\quad z\sim\mathcal{N}(0,I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffusion-process&#34;&gt;Diffusion Process&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Forward diffusion SDE ($dt&amp;gt;0$):
$$dx_t=f_t(x_t)dt+g_tdw_t.$$
&lt;ul&gt;
&lt;li&gt;Forward diffusion SDE corrupts data to noise.&lt;/li&gt;
&lt;li&gt;$f_t(x_t)$ is a vector-valued drift coefficient.&lt;/li&gt;
&lt;li&gt;$g_t$ is a scalar-valued diffusion coefficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reverse diffusion SDE ($dt&amp;lt;0$):
$$dx_t=\left(f_t(x_t)-g_t^2\nabla_{x_t} \log p_t(x_t)\right)dt + g_tdw_t.$$
&lt;ul&gt;
&lt;li&gt;Reverse diffusion SDE recovers data from noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probability flow ODE ($dt&amp;lt;0$):
$$dx_t=\left(f_t(x_t)-\frac{1}{2}g_t^2\nabla_{x_t} \log p_t(x_t)\right)dt.$$
&lt;ul&gt;
&lt;li&gt;Probability flow ODE and reverse diffusion SDE have the same marginal $p_t(x_t)$ at every time $t$.&lt;/li&gt;
&lt;li&gt;Probability flow ODE allows for likelihood evaluation for test or generated samples as it converts a diffusion model to a continuous normalizing flow (see the flow section below for details about likelihood evaluation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;denoising-score-matching&#34;&gt;Denoising Score Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;To generate data using the reverse diffusion SDE or probability flow PDE starting from a tractable noise distribution $\pi(x_T)$, we need to estimate the score $\nabla_{x_t} \log p_t(x_t)$ at every time $t$.&lt;/li&gt;
&lt;li&gt;Learn a time-dependent score network $s_{\theta}(x_t, t)$ by minimizing the score matching objetive:
$$\mathcal{L}_{SM}(\theta) = \mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,T)}[\lambda(t)\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_t(x_t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The score matching objective is intractable since we do not know the true score $\nabla_{x_t} \log p_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We instead minimize the tractable denoising score matching objective:
$$\mathcal{L}_{DSM}(\theta)=\mathbb{E}_{p_{t|0}(x_t|x_0)p_{\text{data}}(x_0)\mathcal{U}(t|0,T)}[\lambda(t)\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_{t|0}(x_t|x_0)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;It can be shown that $\nabla_{\theta}\mathcal{L}_{SM}(\theta)=\nabla_{\theta}\mathcal{L}_{DSM}(\theta).$&lt;/li&gt;
&lt;li&gt;Proof: See my notes on &lt;a href=&#34;https://wenlin-chen.github.io/post/score_identities&#34;&gt;Score Identities for Sampling and Generative Modeling&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Weighting function $\lambda(t)$:
&lt;ul&gt;
&lt;li&gt;Magnitude weighting balances the magnitude of the score matching loss across time $t$:
$$\lambda(t)\propto1/\lVert \nabla_{x_t} \log p_{t|0}(x_t|x_0)\rVert^2.$$&lt;/li&gt;
&lt;li&gt;Likelihood weighting $\lambda(t)=g_t^2$ leads to a nice connection between KL divergence and Fisher divergence:
$$\text{KL}(p_{\text{data}}(x_0)||p_{\theta}(x_0))\leq\frac{T}{2}\mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,T)}[g_t^2\lVert s_{\theta}(x_t, t) - \nabla_{x_t} \log p_t(x_t)\rVert^2]+\text{KL}(p_T(x_T)||\pi(x_T)).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Predictor-corrector samplers for generation: correct the discretization errors in the numerical SDE solution with MCMC.
&lt;ul&gt;
&lt;li&gt;Initialize $x\sim \pi(x)$ and $t\gets T$.&lt;/li&gt;
&lt;li&gt;Repeat the following two steps until $t=0$:
&lt;ul&gt;
&lt;li&gt;Predictor (numerical SDE solver with small discretization step $\Delta t&amp;lt;0$):
$$\Delta x\gets\left(f_t(x)-g_t^2 s_{\theta}(x, t)\right)\Delta t + g_t\sqrt{|\Delta t|}z,\quad z\sim N(0,I).$$
$$x\gets x+\Delta x.$$
$$t\gets t+\Delta t.$$&lt;/li&gt;
&lt;li&gt;Corrector (a few steps of Langevin dynamics with step-size $\eta$):
$$x\gets x+\eta s_{\theta}(x,t)+\sqrt{2\eta}z,\quad z\sim N(0,I).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why do we need multiple noise levels?
&lt;ul&gt;
&lt;li&gt;Large noise corrupts the data distribution $p_{\text{data}}(x_0)$ too much and will produce samples from the wrong distribution.&lt;/li&gt;
&lt;li&gt;When the noise level is small, $p_{\text{data}}(x_0)$ can have many low density regions in real-world applications. Since $\mathcal{L}_{DSM}(\theta)$ is weighted by $p_{\text{data}}(x_0)$, the score estimates in the low density regions can be very poor. Since (random) initial samples are highly likely located in one of the low density regions, it is difficult for Langevin dynamics to move them to the high density regions due to poor score estimates at the beginning.&lt;/li&gt;
&lt;li&gt;Score functions are blind for densities with disconnected support. To see this, consider a mixture of two distributions
$$p_{\text{data}}(x_0)=w r_1(x_0) + (1-w) r_2(x_0)$$
with disjoint supports $\mathcal{X}_1$ and $\mathcal{X}_2$ for the two mixture components. The score function is given by
$$\nabla_x \log p_{\text{data}}(x_0)=
\begin{cases}\nabla_x \log r_1(x_0), &amp;amp;x\in\mathcal{X}_1\\
\nabla_x \log r_2(x_0), &amp;amp;x\in\mathcal{X}_2\end{cases},$$
which contains no information of the mixture weight $w$. In theory, one regularity condition of score matching is $p_{\text{data}}(x_0)&amp;gt;0$ everywhere. In practice, there will also be an issue when the mixture components are weakly connected. Hence, score matching with small noise is unable capture the correct weighting between different modes and will lead to biased samples.&lt;/li&gt;
&lt;li&gt;Therefore, we need to start with a large noise level to make the modes connected, which enables accurate estimation of the weight for each mode. The noise level is gradually reduced to zero so that the samples are refined to be distributed as $p_{\text{data}}(x_0)$ with correct weighting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;flow-matching-for-continuous-normalizing-flows&#34;&gt;Flow Matching for Continuous Normalizing Flows&lt;/h3&gt;
&lt;h4 id=&#34;notation-1&#34;&gt;Notation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denote data by $x_T=x$ and noise by $x_0$
&lt;ul&gt;
&lt;li&gt;This follows the convention in flow matching.&lt;/li&gt;
&lt;li&gt;Note: this is the opposite to the definition in diffusion models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;discrete-normalizing-flow&#34;&gt;Discrete Normalizing Flow&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Discrete normalzing flow with an invertible transformation function $\phi$ (and $T=1$):
$$x_1=\phi(x_0),\quad x_0\sim\pi(x_0),$$
$$\log p(x_1)=\log \pi(\phi^{-1}(x_1)) +\log\left|\det\frac{\partial \phi^{-1}(x_1)}{\partial x_1}\right|.$$&lt;/li&gt;
&lt;li&gt;Compose $T$ discrete normalizing flows:
$$x_{t+1}=\phi_{t}(x_t),$$
$$\phi=\phi_{T-1}\circ\cdots\circ\phi_0,$$
$$\log p(x_T)=\log \pi(\phi^{-1}(x_T)) +\sum_{t=1}^T\log\left|\det\frac{\partial \phi_{t-1}^{-1}(x_t)}{\partial x_{t}}\right|.$$&lt;/li&gt;
&lt;li&gt;Discrete residual flow transformation:
$$\phi_t(x_t)=x_{t+1}=x_t+\delta u_t(x_t).$$
&lt;ul&gt;
&lt;li&gt;$u_t$ needs to be $1/\delta$-Lipschitz to guarantee the invertibility of $\phi_t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn a parametric flow transformation function $\phi_{\theta}$ by MLE:
$$\max_{\theta}~\mathbb{E}_{p_{\text{data}}(x_T)}[\log p_{\theta}(x_T)].$$
&lt;ul&gt;
&lt;li&gt;We need to enforce invertibility in the architecture of $\phi_{\theta}$.&lt;/li&gt;
&lt;li&gt;We need to compute and backpropagate through the inverse and Jacobian for $\phi_{\theta}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;continuous-normalizing-flow&#34;&gt;Continuous Normalizing Flow&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Continuous residual flow transformation:
$$x_{t+\delta}=x_t+\delta u_t(x_t).$$
$$u_t(x_t)=\lim_{\delta\to0}\frac{x_{t+\delta}-x_t}{\delta}=\frac{dx_t}{dt}.$$&lt;/li&gt;
&lt;li&gt;Continuous normalizing flow with a transformation function $\phi_t$ induced by the vector field $u_t$:
$$\frac{dx_t}{dt}=u_t(x_t)\quad\implies\quad x_t=x_0+\int_0^t u_s(x_s)ds,\quad x_0\sim\pi(x_0),$$
&lt;ul&gt;
&lt;li&gt;Set $T=1$ following convention.&lt;/li&gt;
&lt;li&gt;The flow transformation is defined as $x_t=\phi_t(x_0)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The probability path $p_t$ induced by the vector field $u_t$ follows
$$\frac{d}{dt}\log p_t(x_t)=-\nabla_{x_t}\cdot u_t(x_t)\quad\implies\quad\log p_t(x_t)=\log\pi(x_0)-\int_{0}^t\nabla_{x_s}\cdot u_s(x_s)ds.$$
&lt;ul&gt;
&lt;li&gt;Proof: By the transport equation, we have
$$
\begin{aligned}
\frac{\partial}{\partial t} p_t(x_t)
&amp;amp;= -\nabla_{x_t}\cdot(u_t(x_t)p_t(x_t)) \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t)-\left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt;.
\end{aligned}$$
The total derivative is then given by
$$
\begin{aligned}
\frac{d}{d t} p_t(x_t)
&amp;amp;= \frac{\partial}{\partial t} p_t(x_t) + \left&amp;lt;\nabla_{x_t}p_t(x_t),\frac{dx_t}{d t}\right&amp;gt; \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t)-\left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt; + \left&amp;lt;\nabla_{x_t}p_t(x_t),u_t(x_t)\right&amp;gt; \\
&amp;amp;= -p_t(x_t)\nabla_{x_t}\cdot u_t(x_t).
\end{aligned}
$$
Therefore, we have
$$\frac{d}{dt}\log p_t(x_t)=\frac{1}{p_t(x_t)}\frac{d}{d t} p_t(x_t)=-\nabla_{x_t}\cdot u_t(x_t).$$&lt;/li&gt;
&lt;li&gt;The transport equation for diffusion models with $u_t(x_t)=f_t(x_t)-\frac{1}{2}g_t^2\nabla_{x_t} \log p_t(x_t)$ is the Fokker-Planck equation:
$$\frac{\partial}{\partial t} p_t(x_t)+\nabla_{x_t}\cdot(f_t(x_t)p_t(x_t))-\frac{1}{2}g_t^2\nabla_{x_t}^2p_t(x_t)=0.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In practice, both $x_t$ and $\log p_t(x_t)$ can be solved jointly using a numerical ODE integrator:
$$
\frac{d}{dt}
\begin{pmatrix}
x_t \\
\log p_t(x_t)
\end{pmatrix}=
\begin{pmatrix}
u_t(x_t) \\
-\nabla_{x_t}\cdot u_t(x_t)
\end{pmatrix}.
$$&lt;/li&gt;
&lt;li&gt;Note that there are many different vector fields $u_t$ that can induce probability paths between $p_0$ and $p_1$.&lt;/li&gt;
&lt;li&gt;Learn a parameteric time-dependent vector field $u_{\theta}(x_t, t)$ by MLE:
$$\max_{\theta}~\mathbb{E}_{p_{\text{data}}(x_1)}[\log p_{\theta}(x_1)].$$
&lt;ul&gt;
&lt;li&gt;We do not need to choose the number of flow transformations as in composed discrete normalizing flows.&lt;/li&gt;
&lt;li&gt;$u_t$ only needs to be $L$-Lipschitz with any value $L$ to guarantee the invertibility of $\phi_t$.&lt;/li&gt;
&lt;li&gt;Numerical simulation of the ODE with backpropagation makes training very slow and expensive.&lt;/li&gt;
&lt;li&gt;Divergence estimator scales poorly with dimensionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;conditional-flow-matching&#34;&gt;Conditional Flow Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We want a simulation-free objective like score matching.&lt;/li&gt;
&lt;li&gt;Learn a time-dependent vector field $u_{\theta}(x_t, t)$ by minimizing the flow matching objetive:
$$\mathcal{L}_{FM}(\theta)=\mathbb{E}_{p_t(x_t)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(x_t, t)-u_t(x_t)\rVert^2].$$
&lt;ul&gt;
&lt;li&gt;The flow matching objective is intractable since we do not know the true vector field $u_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To circumvent the intractability, we consider a specific probability path $p_t(x_t)$ defined by a conditional probability path $p_{t|1}(x_t|x_1)$:
$$p_t(x_t)=\int p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1,$$
with boundary conditions
$$p_{0|1}(x)=\pi(x)\implies p_0(x_0)=\pi(x_0),$$
$$p_{1|1}(x)=\delta(x-x_1)\implies p_1(x_1)=p_{\text{data}}(x_1).$$&lt;/li&gt;
&lt;li&gt;The marginal vector field $u_t$ can be obtained from the corresponding conditional vector field $u_{t|1}(x_t|x_1)$ and conditional probability path $p_{t|1}(x_t|x_1)$ through the following identity:
$$u_t(x_t)=\mathbb{E}_{p_{1|t}(x_1|x_t)}[u_{t|1}(x_t|x_1)]=\int u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)}dx_1.$$
&lt;ul&gt;
&lt;li&gt;Proof: We verify that this is consistent with the transport equation for the marginals.
$$
\begin{aligned}
\frac{\partial}{\partial t}p_t(x_t)
&amp;amp;= \frac{\partial}{\partial t} \int p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= \int \frac{\partial}{\partial t} p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= - \int \nabla_{x_t}\cdot (u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)) p_{\text{data}}(x_1)dx_1 \\
&amp;amp;= - \int \nabla_{x_t}\cdot (u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)) dx_1 \\
&amp;amp;= - \nabla_{x_t}\cdot  \int  u_{t|1}(x_t|x_1)p_{t|1}(x_t|x_1)p_{\text{data}}(x_1) dx_1  \\
&amp;amp;= - \nabla_{x_t}\cdot \left( \int  u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)} dx_1 p_t(x_t) \right) \\
&amp;amp;= - \nabla_{x_t}\cdot (u_t(x_t) p_t(x_t)). \\
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We now introduce a tractable conditional flow matching objective:
$$\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(x_t,t)-u_{t|1}(x_t|x_1) \rVert^2].$$
&lt;ul&gt;
&lt;li&gt;It can be shown that $\nabla_{\theta}\mathcal{L}_{FM}(\theta)=\nabla_{\theta}\mathcal{L}_{CFM}(\theta)$.&lt;/li&gt;
&lt;li&gt;Proof: We follow a similar idea to the proof of denoising score matching but with the following equality.
$$
\begin{aligned}
\mathbb{E}_{p_t(x_t)}[\left&amp;lt;u_{\theta}(x_t,t),u_t(x_t)\right&amp;gt;]
&amp;amp;= \int \left&amp;lt;u_{\theta}(x_t,t),u_t(x_t)\right&amp;gt; p_t(x_t) dx_t \\
&amp;amp;= \int \left&amp;lt;u_{\theta}(x_t,t),\int u_{t|1}(x_t|x_1)\frac{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}{p_t(x_t)}dx_1\right&amp;gt; p_t(x_t) dx_t \\
&amp;amp;= \iint \left&amp;lt;u_{\theta}(x_t,t),u_{t|1}(x_t|x_1)\right&amp;gt; p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)dx_1 dx_t \\
&amp;amp;= \mathbb{E}_{p_{t|1}(x_t|x_1)p_{\text{data}}(x_1)}[\left&amp;lt;u_{\theta}(x_t,t),u_{t|1}(x_t|x_1)\right&amp;gt;].
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In practice, we may want our conditional vector field $u_{t|1}$ to generate a conditional Gaussian probability path:
$$p_{t|1}(x_t|x_1)=\mathcal{N}(x_t|\mu_t(x_1),\sigma_t(x_1)^2I),$$
with boundary conditions
$$\mu_0(x_1)=0,~\sigma_0(x_1)=1\quad\implies\quad p_{0|1}(x|x_1)=\pi(x)=\mathcal{N}(x|0,I),$$
$$\mu_1(x_1)=x_1,\quad\sigma_1(x_1)=0\quad\implies\quad p_{1|1}(x|x_1)=\delta(x-x_1).$$
&lt;ul&gt;
&lt;li&gt;One correpsonding conditional vector field is given by
$$u_{t|1}(x_t|x_1)=\frac{x-\mu_t(x_1)}{\sigma_t(x_1)}\frac{d\sigma_t(x_1)}{d t}+\frac{d \mu_t(x_1)}{d t},$$
with a conditional flow transformation
$$x_t(x_1)=\phi_{t|1}(x_0|x_1)=\mu_t(x_1)+\sigma_t(x_1)x_0.$$&lt;/li&gt;
&lt;li&gt;Proof: We verify that the following two quantities are identical.
$$\frac{d}{dt}x_t(x_1)=\frac{d}{dt}\mu_t(x_1)+x_0\frac{d}{dt}\sigma_t(x_1),$$
and
$$
\begin{aligned}
u_{t|1}(x_t(x_1)|x_1)
&amp;amp;= \frac{\mu_t(x_1)+\sigma_t(x_1)x_0-\mu_t(x_1)}{\sigma_t(x_1)}\frac{d\sigma_t(x_1)}{d t}+\frac{d \mu_t(x_1)}{d t} \\
&amp;amp;=x_0\frac{d}{dt}\sigma_t(x_1)+\frac{d}{dt}\mu_t(x_1).
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;Reparameterize the conditional flow matching objective:
$$\implies \mathcal{L}_{CFM}(\theta)=\mathbb{E}_{\pi(x_0)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}\left[\left\lVert u_{\theta}(\mu_t(x_1)+\sigma_t(x_1)x_0,t)-\left(\frac{d}{dt}\mu_t(x_1)+x_0\frac{d}{dt}\sigma_t(x_1) \right)\right\rVert^2\right].$$&lt;/li&gt;
&lt;li&gt;Example with optimal transport conditional vector field (linear interpolation):
$$\mu_t(x_1)=tx_1,\quad \sigma_t(x_1)=(1-t)+t\sigma_{min}=1-(1-\sigma_{min})t.$$
$$\implies x_t(x_1)=tx_1+(1-(1-\sigma_{min})t)x_0.$$
$$\implies u_{t|1}(x_t(x_1)|x_1)=x_1-(1-\sigma_{min})x_0.$$
$$\implies \mathcal{L}_{CFM}(\theta)=\mathbb{E}_{\pi(x_0)p_{\text{data}}(x_1)\mathcal{U}(t|0,1)}[\lVert u_{\theta}(tx_1+(1-(1-\sigma_{min})t)x_0,t)-(x_1-(1-\sigma_{min})x_0) \rVert^2].$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two problems with conditional flow matching:
&lt;ul&gt;
&lt;li&gt;The estimate of the gradient $\nabla_{\theta} \mathcal{L}_{CFM}(\theta)$ is of high variance since there are many possible data $x_1$ corresponding to a noise $x_0$ due to intersection of probability paths for different realizations of $u_{t|1}(x_t|x_1)$ with different values of the conditioning variable $x_1$.&lt;/li&gt;
&lt;li&gt;Sampling is slow at generation time since it is difficult to integrate ODE with non-straight path induced by the learned marginal vector field $u_{\theta}(u_t,t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Coupling: instead of maping between data $x_1$ and noise $x_0$ (i.e., one-sided conditioning), we can similarly map between any two variables $x_0$ and $x_1$ with two-sided conditioning:
$$p_t(x_t)=\iint p_{t|0,1}(x_t|x_0,x_1)p_{\text{data}}(x_0,x_1)dx_0dx_1,$$
with boundary conditions
$$p_{0|0,1}(x|x_0,x_1)=\delta(x-x_0)\implies p_0(x_0)=p_{\text{data}}(x_0),$$
$$p_{1|0,1}(x|x_0,x_1)=\delta(x-x_1)\implies p_1(x_1)=p_{\text{data}}(x_1).$$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Score Identities</title>
      <link>https://wenlin-chen.github.io/post/score_identities/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/score_identities/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;h4 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data distribution:
$$p(x)=\frac{\exp(-E(x))}{Z}.$$
&lt;ul&gt;
&lt;li&gt;Intractable normalizing constant:
$$Z=\int \exp(-E(x)) dx.$$&lt;/li&gt;
&lt;li&gt;Tractable score function:
$$\nabla_x \log p(x)=-\nabla_x E(x).$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Goal: generate new samples from $p(x)$.&lt;/li&gt;
&lt;li&gt;Sampling:
&lt;ul&gt;
&lt;li&gt;The energy function $E$ is given.&lt;/li&gt;
&lt;li&gt;No available samples from $p(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generative Modeling:
&lt;ul&gt;
&lt;li&gt;Samples from $p(x)$ are available.&lt;/li&gt;
&lt;li&gt;The energy function $E$ is usually unknown and needs to be learned from data.&lt;/li&gt;
&lt;li&gt;In some rare settings, the energy function $E$ is also given.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;diffusion&#34;&gt;Diffusion&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian convolution kernel:
$$p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I).$$&lt;/li&gt;
&lt;li&gt;Intractable noisy marginal:
$$p(y)=\int p(y|x)p(x)dx.$$&lt;/li&gt;
&lt;li&gt;Intractable denoising posterior:
$$p(x|y)=\frac{p(y|x)p(x)}{p(y)}.$$
&lt;ul&gt;
&lt;li&gt;However, the score function of the denoising posterior is tractable:
$$
\begin{aligned}
\nabla_x \log p(x|y) &amp;amp;= \nabla_x \log p(y|x) + \nabla_x \log p(x) \\
&amp;amp;= \frac{\alpha(y-\alpha x)}{\sigma^2}-\nabla_x E(x).
\end{aligned}
$$
Therefore, it is possible to sample from the denoising posterior using a MCMC sampler.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Intractable noisy score:
$$
\begin{aligned}
\nabla_y\log p(y) &amp;amp;=\nabla_y\log \int p(y|x)p(x)dx \\&amp;amp;
=\nabla_y\log \int \exp\left(-\frac{\lVert y - \alpha x \rVert^2}{2\sigma^2}-E(x)\right)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;score-identities&#34;&gt;Score Identities&lt;/h3&gt;
&lt;p&gt;We list some useful score identities below which allow us to calculate the noisy score.&lt;/p&gt;
&lt;h4 id=&#34;denoising-score-identity&#34;&gt;Denoising Score Identity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Denoising score identity is a general formula without assumping the form of $p(y|x)$:
$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\nabla_y\log p(y|x)]=\int\nabla_y\log p(y|x) p(x|y)dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By definition, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \frac{\nabla_y p(y)}{p(y)} \\
&amp;amp;= \frac{\nabla_y\int p(y|x)p(x)dx}{p(y)} \\
&amp;amp;= \frac{\int\nabla_y p(y|x)p(x)dx}{p(y)} \\
&amp;amp;= \int\nabla_y\log(y|x)\frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;tweedie-score-identity&#34;&gt;Tweedie Score Identity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tweedie score identity assumes Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$:
$$\nabla_y\log p(y)=\frac{\alpha\mathbb{E}_{p(x|y)}[ x ]-y}{\sigma^2}=\int\left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By denoising score identity, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= \int\nabla_y\left(-\frac{\lVert y-\alpha x\rVert^2}{2\sigma^2}\right) p(x|y)dx \\
&amp;amp;= \int \left(\frac{\alpha x-y}{\sigma^2}\right)p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;target-score-identity&#34;&gt;Target Score Identity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Target score identity assumes a translation-invariant convolution $p(y|x)=p(y-\alpha x)$ (e.g., Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$):
$$\nabla_y\log p(y)=\alpha^{-1}\mathbb{E}_{p(x|y)}[\nabla_x\log p(x)]=\alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx.$$&lt;/li&gt;
&lt;li&gt;Proof: By denoising score identity and using the following three identities
$$\nabla_y \log p(y|x)=-\alpha^{-1}\nabla_x \log p(y|x),$$
$$\nabla_x \log p(y|x)=\nabla_x\log p(x|y)-\nabla_x \log p(x),$$
$$\int \nabla_x\log p(x|y)p(x|y)dx=\int \nabla_xp(x|y)dx=\nabla_x\int p(x|y)dx=\nabla_x 1=0,$$
we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int\nabla_y\log p(y|x) p(x|y)dx \\
&amp;amp;= -\alpha^{-1}\int\nabla_x\log p(y|x) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int(\nabla_x \log p(x)-\nabla_x\log p(x|y)) p(x|y)dx \\
&amp;amp;= \alpha^{-1}\int\nabla_x \log p(x) p(x|y)dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mixed-score-identity&#34;&gt;Mixed Score Identity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mixed score identity assumes Gaussian convolution $p(y|x)=\mathcal{N}(y|\alpha x,\sigma^2 I)$ with a variance-preserving scheme $\sigma^2=1-\alpha^2$:
$$\nabla_y\log p(y)=\mathbb{E}_{p(x|y)}[\alpha(x+\nabla_x\log p(x))-y]=\int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx.$$
Proof: Consider a convex combination of the target score identity and Tweedie score identity with coefficients $\alpha^2$ and $1-\alpha^2$:
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \int \left((\alpha^2\frac{\nabla_x \log p(x)}{\alpha} + (1-\alpha^2)\frac{\alpha x - y}{\sigma^2}\right)p(x|y)dx \\
&amp;amp;= \int (\alpha(x+\nabla_x\log p(x))-y) p(x|y) dx.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-in-score-based-sampling&#34;&gt;Applications in Score-based Sampling&lt;/h3&gt;
&lt;h4 id=&#34;monte-carlo-estimator&#34;&gt;Monte Carlo Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with Tweedie score identity using Monte Carlo:
$$\nabla_y \log p(y)\approx \frac{\frac{\alpha}{K}\sum_{k=1}^K x_k-y}{\sigma^2},\quad x_k\sim p(x|y).$$&lt;/li&gt;
&lt;li&gt;We may initialize the sampler for $p(x|y)$ with its mean $\mathbb{E}_{p(x|y)}[ x ]$ estimated by importance sampling:
$$\mathbb{E}_{p(x|y)}[ x ]\approx\frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right..$$
This is the standard importance sampling approach. It does not work well in high dimensional space.&lt;/li&gt;
&lt;li&gt;Proof: Using the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\mathbb{E}_{p(x|y)}[ x ]
&amp;amp;= \int x p(x|y)dx \\
&amp;amp;= \int x \frac{p(y|x)p(x)}{p(y)}dx \\
&amp;amp;= \frac{\int x p(y|x)p(x)dx}{\int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int x \exp(-E(x))q(x|y)dx}{\int \exp(-E(x))q(x|y)dx} \\
&amp;amp;\approx \frac{\sum_{l=1}^L x_l\exp(-E(x_l))}{\sum_{l=1}^L \exp(-E(x_{l}))},\quad x_l\sim q(x|y).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;importance-sampling-estimator&#34;&gt;Importance Sampling Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with the target score identity using importance sampling:
$$\nabla_y \log p(y)\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y)=\mathcal{N}\left(x\left|\frac{y}{\alpha},\left(\frac{\sigma}{\alpha}\right)^2I\right)\right..$$
This does not work well in practice. It only works for very small noise level.&lt;/li&gt;
&lt;li&gt;Proof: Using target score identity and the fact that $q(x|y)\propto p(y|x)$, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \alpha^{-1}\int \nabla_x\log p(x) p(x|y) dx \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha p(y)} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) p(y|x)p(x) dx}{\alpha \int p(y|x)p(x)dx} \\
&amp;amp;= \frac{\int \nabla_x\log p(x) \exp(-E(x)) q(x|y) dx}{\alpha \int \exp(-E(x)) q(x|y)dx} \\
&amp;amp;\approx -\frac{\sum_{k=1}^K \exp(-E(x_k))\nabla_x E(x_k)}{\alpha\sum_{k=1}^K \exp(-E(x_k))},\quad x_k\sim q(x|y).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-in-score-based-generative-modeling&#34;&gt;Applications in Score-based Generative Modeling&lt;/h3&gt;
&lt;h4 id=&#34;denoising-score-matching&#34;&gt;Denoising Score Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a noisy score network $s_{\theta}(y)$ by minimizing
$$
\begin{aligned}
\mathcal{L}_{DSM}(\theta)
&amp;amp;= \mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2] \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C.
\end{aligned}
$$
This is the most popular generative modeling apporach. It is inaccurate for very small noise level.&lt;/li&gt;
&lt;li&gt;Proof: Using the denoising score identity, we have
$$
\begin{aligned}
\mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2]
&amp;amp;= \int \lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2 p(y) dy \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \int \left&amp;lt;s_{\theta}(y),\nabla_y \log p(y)\right&amp;gt;p(y)dy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(x|y)p(y)dxdy + C&amp;rsquo; \\
&amp;amp;= \int \lVert s_{\theta}(y) \rVert^2 p(y) dy - 2 \iint \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;p(y|x)p(x)dxdy + C&amp;rsquo; \\
&amp;amp;= \iint \left( \lVert s_{\theta}(y) \rVert^2 + \lVert \nabla_y\log p(y|x) \rVert^2 - 2 \left&amp;lt;s_{\theta}(y),\nabla_y\log p(y|x) \right&amp;gt;\right)p(y|x)p(x)dxdy + C \\
&amp;amp;= \iint \lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2 p(y|x)p(x)dxdy + C \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \nabla_y \log p(y|x)\rVert^2]+C.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;One problem with denoising score matching is that the score network $s_{\theta}(y)$ is inaccurate for a low noise level $\sigma\approx 0$. To see this, consider the score of the Gaussian convolution kernel:
$$\nabla_{y} \log p(y|x)=-\frac{y-\alpha x}{\sigma^2}=-\frac{z}{\sigma},\quad z\sim \mathcal{N}(0,I),$$
which is unbounded as $\sigma\to 0$. This leads to a huge variance for the estimate of $\mathcal{L}_{DSM}(\theta)$ for small noise level and thus results in optimization difficulty.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nonparametric-estimator&#34;&gt;Nonparametric Estimator&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the noisy score with the target score identity using with a KDE-style estimator:
$$\nabla_y \log p(y)\approx\frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x).$$
This is expensive to evaluate and needs to be recomputed for different values of $y$.&lt;/li&gt;
&lt;li&gt;Proof: By definition of noisy score, we have
$$
\begin{aligned}
\nabla_y\log p(y)
&amp;amp;= \nabla_y\log \int p(y|x)p(x)dx \\
&amp;amp;\approx \nabla_y\log\sum_{k=1}^K p(y|x_k),\quad x_k\sim p(x) \\
&amp;amp;= \frac{\sum_{k=1}^K \nabla_y p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x) \\
&amp;amp;= \frac{\sum_{k=1}^K p(y|x_k)\nabla_y \log p(y|x_k)}{\sum_{k=1}^K p(y|x_k)},\quad x_k\sim p(x).
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;target-score-matching&#34;&gt;Target Score Matching&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a noisy score network $s_{\theta}(y)$ by minimizing
$$
\begin{aligned}
\mathcal{L}_{TSM}(\theta)
&amp;amp;= \mathbb{E}_{p(y)}[\lVert s_{\theta}(y) - \nabla_y \log p(y)\rVert^2] \\
&amp;amp;= \mathbb{E}_{p(y|x)p(x)}[\lVert s_{\theta}(y) - \alpha^{-1}\nabla_x \log p(x)\rVert^2]+C.
\end{aligned}
$$&lt;/li&gt;
&lt;li&gt;This requires to have access to both samples from $p(x)$ and the ground-truth energy or score function. It is only accurate for small noise level.&lt;/li&gt;
&lt;li&gt;Proof: This proof is similar to the proof of denoising score matching but with the target score identity instead.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tensor Algebra</title>
      <link>https://wenlin-chen.github.io/post/tensor_algebra/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/tensor_algebra/</guid>
      <description>&lt;h3 id=&#34;motivation-and-applications-of-tensors&#34;&gt;Motivation and Applications of Tensors&lt;/h3&gt;
&lt;p&gt;Tensors provide insights about how geometry works. Below are some examples that involve tensors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General relativity (metric tensor):
&lt;ul&gt;
&lt;li&gt;curved space-time,&lt;/li&gt;
&lt;li&gt;expanding universe.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quantum mechanics and quantum computing:
&lt;ul&gt;
&lt;li&gt;quantum superposition (linear combination),&lt;/li&gt;
&lt;li&gt;quantum engtanglement (tensor product).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimization.&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;definitions-of-tensors&#34;&gt;Definitions of Tensors&lt;/h3&gt;
&lt;p&gt;Roughtly speaking, tensors are objects defined by the way they transform. Tensor algbra focuses on the analysis of individual tensors, which generalizes linear algebra. Tensor calculus focuses on the analysis of tensor fields, which generalizes multivariate calculus.&lt;/p&gt;
&lt;p&gt;Formally, there are several definitions of tensors from different perspectives.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;(Array Definition) A tensors is a multi-dimensional array, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rank-0 tensor: scalar,&lt;/li&gt;
&lt;li&gt;rank-1 tensor: vector,&lt;/li&gt;
&lt;li&gt;rank-2 tensor: matrix,&lt;/li&gt;
&lt;li&gt;rank-3 tensor: ?,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is not a good definition as arrays are not what tensors fundamentally are. The array definition ignores the geometric meaning behind tensors and is not helpful for understanding geometry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Coordinate Definition) A tensor is an object invariant under a change of coordinates, which has components that change in a special, predictable way under a change of coordinates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tensor object itself is intrinsic and does not depend on the choice of coordinate system.&lt;/li&gt;
&lt;li&gt;The components of a tensor change under different coordinate systems in a specific way that can be figured out. We will get to this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Algebra Definition) A tensor is a collection of vectors and covectors combined together using the tensor product.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a consice and probably the best definition of tensors.&lt;/li&gt;
&lt;li&gt;But&amp;hellip; What are covectors? What is tensor product? We will get to this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Calculus Definition) Tensors are partial derivatives and gradients that transform with the Jacobian matrix.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is also a useful definition, but we will focus on the coordinate and algebra definitions in tensor algebra.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will begin with special examples of tensors that we have seen in linear algebra and then gradually generalize the concept of tensors. For notational simplicity, we will use Einstein&amp;rsquo;s notation, which drops the summation symbols $\sum$ since they can be inferred from the context by contracting all corresponding upper and lower indices.&lt;/p&gt;
&lt;h3 id=&#34;change-of-basis&#34;&gt;Change of Basis&lt;/h3&gt;
&lt;p&gt;Consider an old basis $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ and a new basis $\tilde{E}=[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]$.&lt;/p&gt;
&lt;p&gt;The forward transformation $F$ builds the new basis from the old basis:
$$
\tilde{\mathbf{e}}_i = F^j_i\mathbf{e}_j.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]=[\mathbf{e}_1,\cdots,\mathbf{e}_n]
\begin{bmatrix}
F^1_1 &amp; \cdots &amp; F^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
F^n_1 &amp; \cdots &amp; F^n_n
\end{bmatrix}.
$$ --&gt;
&lt;p&gt;The backward transformation $B$ builds the old basis from the new basis:
$$
\mathbf{e}_i = B^j_i\tilde{\mathbf{e}}_j.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
[\mathbf{e}_1,\cdots,\mathbf{e}_n]=[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]
\begin{bmatrix}
B^1_1 &amp; \cdots &amp; B^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
B^n_1 &amp; \cdots &amp; B^n_n
\end{bmatrix}.
$$ --&gt;
&lt;p&gt;Composing the forward and backward transformation should result in the identity transformation:
$$
\mathbf{e}_i = B^j_i F^k_j\mathbf{e}_k
$$
$$
\implies B^j_i F^k_j =\delta^k_i = F^k_j B^j_i.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
FB=
\begin{bmatrix}
F^1_1 &amp; \cdots &amp; F^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
F^n_1 &amp; \cdots &amp; F^n_n
\end{bmatrix}
\begin{bmatrix}
B^1_1 &amp; \cdots &amp; B^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
B^n_1 &amp; \cdots &amp; B^n_n
\end{bmatrix}=
\begin{bmatrix}
1 &amp; \cdots &amp; 0 \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
0 &amp; \cdots &amp; 1
\end{bmatrix}=I.
$$ --&gt;
&lt;p&gt;Therefore, the forward and backward transformations are inverses of each other.&lt;/p&gt;
&lt;!-- $$
B=F^{-1}\quad\text{and}\quad F=B^{-1}.
$$ --&gt;
&lt;h3 id=&#34;vectors-and-vector-spaces&#34;&gt;Vectors and Vector Spaces&lt;/h3&gt;
&lt;p&gt;A vector space $(V,S,+,\cdot)$ consists of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$V$: a set of vectors,&lt;/li&gt;
&lt;li&gt;$S$: a set of scalars,&lt;/li&gt;
&lt;li&gt;$+$: a vector addtion rule such that $\mathbf{v}+\mathbf{w}\in V,~\forall\mathbf{v},\mathbf{w}\in V$,&lt;/li&gt;
&lt;li&gt;$\cdot$: a vector scaling rule such that $a\cdot\mathbf{v}\in V,~\forall a\in S,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We usually omit $\cdot$ and just write $a\mathbf{v}$ instead.&lt;/p&gt;
&lt;p&gt;Vectors are a kind of tensor. Vectors are invariant to the coordinate systems, but the components of vectors are not.&lt;/p&gt;
&lt;p&gt;Let $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ be a basis of $(V,S,+,\cdot)$. A vector $\mathbf{v}\in V$ can be represented as a linear combination of the basis vectors:
$$
\mathbf{v}= v^i \mathbf{e}_i,
$$
where the components $v^i\in S$ of $\mathbf{v}$ in the basis $E$ are
$$
\mathbf{v}=
\begin{bmatrix}
v^1 \\
\vdots \\
v^n
\end{bmatrix}_{E}
$$
This tells us that column vectors are the array representations of vectors, which is the array definition of vectors.&lt;/p&gt;
&lt;p&gt;Geometrically, vectors are arrows (directed line segments).&lt;/p&gt;
&lt;h3 id=&#34;vector-transformation&#34;&gt;Vector Transformation&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s represent the vector components of $\mathbf{v}$ in a different basis $\tilde{E}$:
$$
\mathbf{v}=\tilde{v}^j \tilde{\mathbf{e}}_j = \tilde{v}^jF^i_j\mathbf{e}_i = (F^i_j\tilde{v}^j)\mathbf{e}_i.
$$
But in the basis $E$, we have $\mathbf{v}=v^i \mathbf{e}_i$. This implies that
$$
v^i = F^i_j\tilde{v}^j.
$$&lt;/p&gt;
&lt;p&gt;Likewise, we have
$$
\tilde{v}^j \tilde{\mathbf{e}}_j=\mathbf{v}=v^i \mathbf{e}_i =v^i  B^j_i\tilde{\mathbf{e}}_j = (B^j_i v^i) \tilde{\mathbf{e}}_j,
$$
$$
\implies \tilde{v}^j = B^j_i v^i.
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare a change of basis to vector transformation:
$$
\tilde{\mathbf{e}}_j = F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j =  B^i_j\tilde{\mathbf{e}}_i
$$
$$
v^i = F^i_j\tilde{v}^j \quad\quad \tilde{v}^i = B^i_j v^j
$$&lt;/p&gt;
&lt;p&gt;It is interesting to note that vector transformation behaves contrarily to a change of basis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change of basis:
&lt;ul&gt;
&lt;li&gt;Forward transformation changes the old basis into the new basis.&lt;/li&gt;
&lt;li&gt;Backward transformation changes the new basis into the old basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vector transformation:
&lt;ul&gt;
&lt;li&gt;Forward transformation changes the vector components from the new basis to the old basis.&lt;/li&gt;
&lt;li&gt;Backward transformation changes the vector components from the old basis to the new basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say that vector components are contravariant because they contra-vary with a change of basis. As a reminder, we always put the indices of the vector components above the letters. We say that basis vectors are covariant and put the indices of the basis vectors below the letters.&lt;/p&gt;
&lt;h3 id=&#34;covectors-and-dual-vector-spaces&#34;&gt;Covectors and Dual Vector Spaces&lt;/h3&gt;
&lt;p&gt;Covectors (linear forms) are functions $\alpha: V\to S$ that map the vectors in $V$ to the scalars in $S$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha(\mathbf{v}+\mathbf{w})=\alpha(\mathbf{v})+\alpha(\mathbf{w}),~\forall \mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$\alpha(n\mathbf{v})=n\alpha(\mathbf{v}),\forall n\in S,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dual vector space $(V^*,S,+&amp;rsquo;,\cdot&amp;rsquo;)$ of a vector space $(V,S,+,\cdot)$ is a vector space with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the same set $S$ of scalars&lt;/li&gt;
&lt;li&gt;a different set $V^*$ of vectors ($\alpha\in V^*$ are covectors),&lt;/li&gt;
&lt;li&gt;different addition ($+&amp;rsquo;$) and scaling ($\cdot&amp;rsquo;$) rules such that
&lt;ol&gt;
&lt;li&gt;$(n\cdot\alpha)(\mathbf{v})=n\alpha(\mathbf{v}),~\forall n\in S,\alpha\in V^*,\mathbf{v}\in V$.&lt;/li&gt;
&lt;li&gt;$(\alpha+\beta)(\mathbf{v})=\alpha(\mathbf{v})+\beta(\mathbf{v}),~\forall\alpha,\beta\in V^*,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As shown above, we usually just write $+$ and $\cdot$ when we add and scale covectors in $V^*$ but need to remember that the addition and scaling rules in $V^*$ are actually different from those in $V$.&lt;/p&gt;
&lt;h3 id=&#34;dual-basis-and-covector-components&#34;&gt;Dual Basis and Covector Components&lt;/h3&gt;
&lt;p&gt;Covectors are a kind of tensor. Covectors are invariant to the coordinate systems, but the components of covectors are not.&lt;/p&gt;
&lt;p&gt;Take the basis $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ for a vector space $V$. We introduce the (Kronecker) dual basis $\mathcal{E}=[\epsilon^1,\cdots,\epsilon^n]^T$ ($\epsilon^i:V\to S$) for its dual vector space $V^*$:
$$
\epsilon^i(\mathbf{e}_j)=\delta^i_j.
$$&lt;/p&gt;
&lt;p&gt;Each covector $\epsilon^i\in V^*$ in the dual basis outputs the corresponding vector component $v^i$ in the basis $E$:
$$
\epsilon^i(\mathbf{v})=\epsilon^i\left( v^j \mathbf{e}_j \right)=v^j \epsilon^i(\mathbf{e}_j)=v^j\delta^i_j=v^i.
$$&lt;/p&gt;
&lt;p&gt;For any covector $\alpha\in V^*$, we define
$$
\alpha(\mathbf{e}_i)=\alpha_i.
$$&lt;/p&gt;
&lt;p&gt;Now, a covector $\alpha\in V^*$ can be represented as
$$
\alpha(\mathbf{v})=\alpha\left(v^i \mathbf{e}_i \right)=v^i \alpha(\mathbf{e}_i) =  \alpha_i \epsilon^i(\mathbf{v})=(\alpha_i \epsilon^i)(\mathbf{v}).
$$
$$
\implies \alpha = \alpha_i \epsilon^i.
$$&lt;/p&gt;
&lt;p&gt;Therefore, the covector components in the dual basis $\mathcal{E}$ are
$$
\alpha=[\alpha_1, \cdots, \alpha_n]_{\mathcal{E}}.
$$&lt;/p&gt;
&lt;p&gt;This tells us that row vectors are the array representation of covectors, which is the array definition of covectors.&lt;/p&gt;
&lt;!-- $$
\alpha(\mathbf{v})=\alpha\left(\sum_{i=1}^n v^i \mathbf{e}\_i \right)=\sum\_{i=1}^n v^i \alpha(\mathbf{e}_i) = \sum\_{i=1}^n \alpha_i v^i.
$$ --&gt;
&lt;p&gt;Geometrically, a covector is a stack of linearly spaced, straight contour lines. A covector maps a vector into a scalar specified by the number of covector contour lines that the vector covers.&lt;/p&gt;
&lt;h3 id=&#34;change-of-dual-basis-and-covector-transformation&#34;&gt;Change of Dual Basis and Covector Transformation&lt;/h3&gt;
&lt;p&gt;Consider an old dual basis $\mathcal{E}=[\epsilon^1,\cdots,\epsilon^n]^T$ and a new dual basis $\tilde{\mathcal{E}}=[\tilde{\epsilon}^1,\cdots,\tilde{\epsilon}^n]^T$ for $V^*$. Let&amp;rsquo;s build the new dual basis from the old dual basis using a transformation $Q$:
$$
\tilde{\epsilon}^i=Q^i_j\epsilon^j.
$$
Applying forward transformation to $\tilde{\mathbf{e}}_k$ in $\tilde{\epsilon}^i(\tilde{\mathbf{e}}_k)$ gives
$$
\tilde{\epsilon}^i(\tilde{\mathbf{e}}_k)=Q^i_j\epsilon^j(\tilde{\mathbf{e}}_k)= Q^i_j\epsilon^j\left(F^l_k\mathbf{e}_l\right)=Q^i_j \epsilon^j(\mathbf{e}_l) F^l_k=Q^i_j \delta^j_l F^l_k= Q^i_j F^j_k.
$$
But we know that $\tilde{\epsilon}^i(\tilde{\mathbf{e}}_k)=\delta^i_k$ by definition. This implies that
$$
Q^i_j F^j_k=\delta^i_k\quad\implies\quad Q=B.
$$
Therefore, we build the new dual basis from the old dual basis using the backward transformation:
$$
\tilde{\epsilon}^i=B^i_j\epsilon^j.
$$
Likewise, we build the old dual basis from the new dual basis using the forward transformation:
$$
\epsilon^i=F^i_j\tilde{\epsilon}^j.
$$
Let&amp;rsquo;s now compare a change of basis to a change of dual basis:
$$
\tilde{\mathbf{e}}_j = F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j = B^i_j\tilde{\mathbf{e}}_i
$$
$$
\epsilon^i=F^i_j\tilde{\epsilon}^j \quad\quad \tilde{\epsilon}^i=B^i_j \epsilon^j
$$
It is interesting to note that a change of dual basis behaves contrarily to a change of basis.&lt;/p&gt;
&lt;p&gt;As for covector component transformation, we first represent a covector in two dual bases:
$$
\alpha = \alpha_i\epsilon^i = \tilde{\alpha}_j\tilde{\epsilon}^j.
$$
But changing the dual basis gives
$$
\alpha = \alpha_i\epsilon^i = \alpha_i F^i_j\tilde{\epsilon}^j= (F^i_j\alpha_i)\tilde{\epsilon}^j.
$$
This implies that the forward transformation changes the covector components from the old dual basis to the new dual basis:
$$
\tilde{\alpha}_j = F^i_j\alpha_i.
$$
Likewise, the backward transformation changes the covector components from the new dual basis to the old dual basis:
$$
\alpha_j = B^i_j\tilde{\alpha}_i.
$$
This tells us covector transformation is covariant to a change of basis and contravariant to a change of dual basis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now summarize what we have learned so far about changes of basis/dual basis and vector/covector transformations:
$$
\tilde{\mathbf{e}}_j = F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j =  B^i_j\tilde{\mathbf{e}}_i \quad\quad\text{and}\quad\quad v^i =  F^i_j\tilde{v}^j \quad\quad \tilde{v}^i = B^i_j v^j
$$
$$
\epsilon^i= F^i_j\tilde{\epsilon}^j \quad\quad ~ \tilde{\epsilon}^i= B^i_j\epsilon^j \quad\quad\text{and}\quad\quad \tilde{\alpha}_j =  F^i_j\alpha_i \quad\quad \alpha_j =  B^i_j\tilde{\alpha}_i
$$&lt;/p&gt;
&lt;h3 id=&#34;linear-maps&#34;&gt;Linear Maps&lt;/h3&gt;
&lt;p&gt;Linear maps $L:V\to W$ map vectors in $V$ to vectors in $W$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$L(\mathbf{v}+\mathbf{w})=L(\mathbf{v})+L(\mathbf{w})$,&lt;/li&gt;
&lt;li&gt;$L(n\mathbf{v})=nL(\mathbf{v})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s consider a speical case where $W=V$ from now on. In the array definition, linear maps are matrices that transform column vectors but do not transform basis. We define the transformation of a copy of each basis vector under a linear map $L$ by
$$
L(\mathbf{e}_i)=L^j_i\mathbf{e}_j.
$$
This defines a matrix which contains the linear map components in the basis $E$:
$$
L=\begin{bmatrix}
L^1_1 &amp;amp; \cdots &amp;amp; L^1_n \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
L^n_1 &amp;amp; \cdots &amp;amp; L^n_n
\end{bmatrix}_{E}.
$$
Let $\mathbf{v}=\sum_{i=1}^n v^i \mathbf{e}_i$ and $\mathbf{w}=\sum_{i=1}^n w^i \mathbf{e}_i$ respectively be the input and output vectors represented in the basis $E$. Then, we have
$$
\mathbf{w}=L(\mathbf{v})=L( v^i \mathbf{e}_i )=v^i L(\mathbf{e}_i)=v^i L^j_i\mathbf{e}_j=(L^j_i v^i ) \mathbf{e}_j,
$$
$$
\implies w^i = L^i_j v^j.
$$
This is essentially the usual matrix-vector multiplication rule:
$$
\begin{bmatrix}
w^1 \\
\vdots \\
w^n
\end{bmatrix}_{E}=
\begin{bmatrix}
L^1_1 &amp;amp; \cdots &amp;amp; L^1_n \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
L^n_1 &amp;amp; \cdots &amp;amp; L^n_n
\end{bmatrix}_{E}
\begin{bmatrix}
v^1 \\
\vdots \\
v^n
\end{bmatrix}_{E}.
$$
It is important to note that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The transformed vectors are in the same basis as the input vectors.&lt;/li&gt;
&lt;li&gt;The $i$-th column vector in a matrix is what a copy of the $i$-th basis vector will be transformed into.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Geometrically, linear maps are spatial transformations that keep lines parallel, keep lines evenly spaced, and keep the origin unchanged. A linear map can be a combination of vector scaling and rotation but not vector translation.&lt;/p&gt;
&lt;h3 id=&#34;linear-map-transformation&#34;&gt;Linear Map Transformation&lt;/h3&gt;
&lt;p&gt;Linear maps are a kind of tensor. Linear maps are invariant to the coordinate systems, but the components of linear maps are not.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s transform a linear map from an old basis $E$ to a new basis $\tilde{E}$. By definition, we have
$$
L(\tilde{\mathbf{e}}_i)=\tilde{L}^l_i \tilde{\mathbf{e}}_l.
$$
But we also have
$$
L(\tilde{\mathbf{e}}_i)=L( F^j_i \mathbf{e}_j )=F^j_i L(\mathbf{e}_j) = F^j_i L^k_j \mathbf{e}_k = F^j_i L^k_j B^l_k \tilde{\mathbf{e}}_l = ( B^l_k L^k_j F^j_i ) \tilde{\mathbf{e}}_l.
$$
This implies that
$$
\tilde{L}^l_i = B^l_k L^k_j F^j_i.
$$&lt;/p&gt;
&lt;p&gt;Note that multiplying by the identity transformation $I$ does not change a matrix:
$$
(LI)^i_k = L^i_j \delta^j_k = L^i_k.
$$
Then, the backward transformation for the linear map is given by
$$
F^s_l \tilde{L}^l_i B^i_t = F^s_l B^l_k L^k_j F^j_i B^i_t = \delta^s_k L^k_j \delta^j_t = L^s_t.
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s classify the tensors we have learned so far. Note that by vectors, covectors, linear maps and any other tensors (excluding the bases and dual bases), we always mean the components.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(0,1)-tensors (covariant):
&lt;ul&gt;
&lt;li&gt;Basis:
$$
\tilde{\mathbf{e}}_j = F^i_j \mathbf{e}_i \quad\quad \mathbf{e}_j = B^i_j \tilde{\mathbf{e}}_i
$$&lt;/li&gt;
&lt;li&gt;Covectors:
$$
\tilde{\alpha}_j = F^i_j \alpha_i \quad\quad \alpha_j = B^i_j \tilde{\alpha}_i
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(1,0)-tensors (contravariant):
&lt;ul&gt;
&lt;li&gt;Dual basis:
$$
\tilde{\epsilon}^i = B^i_j\epsilon^j \quad\quad \epsilon^i = F^i_j\tilde{\epsilon}^j
$$&lt;/li&gt;
&lt;li&gt;Vectors:
$$
\tilde{v}^i = B^i_j v^j \quad\quad v^i = F^i_j\tilde{v}^j
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(1,1)-tensors (one contravariant dimension, one covariant dimension):
&lt;ul&gt;
&lt;li&gt;Linear maps:
$$
\tilde{L}^i_j = B^i_k L^k_l F^l_j \quad\quad L^i_j = F^i_k \tilde{L}^k_l B^l_j
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bilinear-forms-metric-tensors-and-their-transformations&#34;&gt;Bilinear Forms, Metric Tensors and Their Transformations&lt;/h3&gt;
&lt;p&gt;Metric tensors $g:V\times V\to S$ are speical bilinear forms that define dot products between vectors such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$ag(\mathbf{v},\mathbf{w})=g(a\mathbf{v},\mathbf{w})=g(\mathbf{v},a\mathbf{w}),~\forall a\in S,~\forall\mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v}+\mathbf{u},\mathbf{w})=g(\mathbf{v},\mathbf{w}) + g(\mathbf{u},\mathbf{w}),~\forall\mathbf{v},\mathbf{u},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{w}+\mathbf{t})=g(\mathbf{v},\mathbf{w}) + g(\mathbf{v},\mathbf{t}),~\forall\mathbf{v},\mathbf{w},\mathbf{t}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{w})=g(\mathbf{w},\mathbf{v}),~\forall\mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{v})\geq 0,~\forall\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that 1-3 are rules for general bilinear forms. Rules 4-5 are special for metric tensors, so metric tensors must be symmetric (4) and positive semi-definite (5).&lt;/p&gt;
&lt;p&gt;Metric tensors define the dot product between two vectors $\mathbf{v}$ and $\mathbf{w}$ in a basis $E$:
$$
g(\mathbf{v},\mathbf{w})=g(v^i \mathbf{e}_i,w^j \mathbf{e}_j)=v^i w^j g(\mathbf{e}_i,\mathbf{e}_j).
$$
The array representation of a metric tensor $g$ in the basis $E$ is defined as
$$
g_{ij}=g(\mathbf{e}_i,\mathbf{e}_j),
$$
Therefore, the dot product in the basis $E$ becomes
$$
g(\mathbf{v},\mathbf{w})=v^i w^j g_{ij}.
% \begin{bmatrix}
% v^1 &amp;amp; \cdots &amp;amp; v^n
% \end{bmatrix}_{E}
% \begin{bmatrix}
% g_{11} &amp;amp; \cdots &amp;amp; g_{1n} \\
% \vdots &amp;amp; \ddots &amp;amp; \vdots \\
% g_{n1} &amp;amp; \cdots &amp;amp; g_{nn}
% \end{bmatrix}_{E}
% \begin{bmatrix}
% w^1 \\
% \vdots \\
% w^n
% \end{bmatrix}_{E}.
$$&lt;/p&gt;
&lt;p&gt;Using dot products, we respectively define the norm $\lVert\cdot\rVert$ of a vector and the angle $\left&amp;lt;\cdot,\cdot\right&amp;gt;$ between two vectors as
$$
\lVert\mathbf{v}\rVert^2 = g(\mathbf{v},\mathbf{v})=v^i v^j g_{ij}\geq 0,
$$
$$
\cos\left&amp;lt;\mathbf{v},\mathbf{w}\right&amp;gt;=\frac{g(\mathbf{v},\mathbf{w})}{\lVert\mathbf{v}\rVert\lVert\mathbf{w}\rVert}=\frac{v^i w^j g_{ij}}{\sqrt{v^i v^j g_{ij}}\sqrt{w^i w^j g_{ij}}}.
$$&lt;/p&gt;
&lt;p&gt;For an orthonormal basis $E$, the metric tensor is given by the identity matrix $g_{ij}=g(\mathbf{e}_i,\mathbf{e}_j)=\delta_{ij}$. This gives us the usual dot product $g(\mathbf{v},\mathbf{w})=v^i w^i$ and the Pythagorean theorem $\lVert\mathbf{v}\rVert^2=v^iv^i$.&lt;/p&gt;
&lt;p&gt;Metric tensors are invariant to the coordinate systems, but the components of metric tensors are not. Let&amp;rsquo;s now a transform metric tensor (or more generally, a bilinear form) from an old basis $E$ to a new basis $\tilde{E}$:
$$
\tilde{g}_{ij}=g(\tilde{\mathbf{e}}_i,\tilde{\mathbf{e}}_j)=g(F^k_i \mathbf{e}_k,F^l_j \mathbf{e}_l)=F^k_iF^l_j g(\mathbf{e}_k,\mathbf{e}_l)=F^k_iF^l_jg_{kl}.
$$
Likewise, the metric tensor transformation from $\tilde{E}$ to $E$ is given by the inverse of the above transformation
$$
g_{kl}=g(\mathbf{e}_k,\mathbf{e}_l)=g(B^i_k \tilde{\mathbf{e}}_i,B^j_l \tilde{\mathbf{e}}_j)=B^i_k B^j_l g(\tilde{\mathbf{e}}_i,\tilde{\mathbf{e}}_j)=B^i_k B^j_l \tilde{g}_{ij}.
$$
Metric tensors (and general bilinear forms) are (0,2)-tensors since both dimensions in a metric tensor are covariant (to a change of basis).&lt;/p&gt;
&lt;p&gt;We can also show that the results of dot products are identical in different bases:
$$
g(\mathbf{v},\mathbf{w})=v^i w^j g_{ij}=(F^i_k\tilde{v}^k)(F^j_l\tilde{v}^l) B^s_i B^t_j \tilde{g}_{st}=(B^s_i F^i_k)(B^t_jF^j_l)\tilde{v}^k\tilde{v}^l\tilde{g}_{st} = \delta^s_k \delta^t_l \tilde{v}^k\tilde{v}^l\tilde{g}_{st}=\tilde{v}^s\tilde{v}^t\tilde{g}_{st}.
$$&lt;/p&gt;
&lt;h3 id=&#34;general-linear-forms-and-tensors&#34;&gt;General Linear Forms and Tensors&lt;/h3&gt;
&lt;p&gt;In general, we define an $n$-(linear) form to be a linear function $V\times V\times \cdots \times V=V^n\to S$, which is a (0,$n$)-tensor or a rank-$n$ covector. Covectors are linear forms or 1-linear forms. Bilinear forms are 2-linear forms.&lt;/p&gt;
&lt;p&gt;A general ($m$,$n$)-tensor $T$ is a tensor with $m$ contravariant dimensions and $n$ covariant dimensions. General tensor transformations are defined as
$$
\tilde{T}^{abc\cdots}_{xyz\cdots}=(B^a_i B^b_j B^c_k\cdots ) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z\cdots),
$$
$$
T^{ijk\cdots}_{rst\cdots}=(F^i_a F^j_b F^k_c\cdots ) \tilde{T}^{abc\cdots}_{xyz\cdots} (B^x_r B^y_s B^z_t\cdots).
$$
We will derive this transformation rule using tensor products later.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-tensor-products&#34;&gt;Examples of Tensor Products&lt;/h3&gt;
&lt;p&gt;In general, tensors can be constructed by combining vectors and covectors using tensor product. We first give a few examples of redefining tensors that we have learned using tensor products below.&lt;/p&gt;
&lt;p&gt;Linear maps are tensor products of vector-covector pairs:
$$
L=L^i_j\mathbf{e}_i\otimes\epsilon^j.
$$
We can verify the definition of linear maps:
$$
\mathbf{w}=L(\mathbf{v})=L^i_j\mathbf{e}_i\otimes\epsilon^j(v^k\mathbf{e}_k)=L^i_j v^k \mathbf{e}_i\otimes\epsilon^j(\mathbf{e}_k)=L^i_j v^k \mathbf{e}_i \delta^j_k = L^i_j v^j \mathbf{e}_i,
$$
and the transformation of linear maps:
$$
L=L^k_l\mathbf{e}_k\otimes\epsilon^l=L^k_l(B^i_k\tilde{\mathbf{e}}_i)\otimes(F^l_j\tilde{\epsilon}^j)= (B^i_k L^k_l F^l_j) \tilde{\mathbf{e}}_i \otimes \tilde{\epsilon}^j=\tilde{L}^i_j \tilde{\mathbf{e}}_i \otimes \tilde{\epsilon}^j
$$
$$
\implies \tilde{L}^i_j = B^i_k L^k_l F^l_j.
$$&lt;/p&gt;
&lt;p&gt;Bilinear forms (including metric tensors) are tensor products of covector-covector pairs:
$$
\mathcal{B}=\mathcal{B}_{ij}\epsilon^i\otimes\epsilon^j.
$$
We can verify the definition of bilinear forms:
$$
\begin{aligned}
s
&amp;amp;= \mathcal{B}(\mathbf{v},\mathbf{w})=\mathcal{B}_{ij}(\epsilon^i\otimes\epsilon^j)(\mathbf{v},\mathbf{w})=\mathcal{B}_{ij}\epsilon^i(\mathbf{v})\otimes\epsilon^j(\mathbf{w})=\mathcal{B}_{ij}\epsilon^i(v^k\mathbf{e}_k)\otimes\epsilon^j(w^l\mathbf{e}_l) \\
&amp;amp;=\mathcal{B}_{ij}v^k w^l\epsilon^i(\mathbf{e}_k)\otimes\epsilon^j(\mathbf{e}_l)=\mathcal{B}_{ij}v^k w^l \delta^i_k \delta^j_l=\mathcal{B}_{ij}v^i w^j,
\end{aligned}
$$
and the transformation of bilinear forms:
$$
\mathcal{B}=\mathcal{B}_{kl}\epsilon^k\otimes\epsilon^l=\mathcal{B}_{kl}(F^k_i\tilde{\epsilon}^i)\otimes(F^l_j\tilde{\epsilon}^j)=F^k_i F^l_j\mathcal{B}_{kl}\tilde{\epsilon}^i\otimes\tilde{\epsilon}^j=\tilde{\mathcal{B}}_{ij}\tilde{\epsilon}^i\otimes\tilde{\epsilon}^j
$$
$$
\implies \tilde{\mathcal{B}}_{ij} = F^k_i F^l_j\mathcal{B}_{kl}.
$$&lt;/p&gt;
&lt;h3 id=&#34;general-tensor-products-and-kronecker-products&#34;&gt;General Tensor Products and Kronecker Products&lt;/h3&gt;
&lt;p&gt;A general ($m$,$n$)-tensor can be defined by combining $m$ vectors $\mathbf{e}_i,\mathbf{e}_j,\mathbf{e}_k,\cdots$ and $n$ covectors $\epsilon^r,\epsilon^s,\epsilon^t,\cdots$ using tensor products:
$$
T = T^{ijk\cdots}_{rst\cdots}(\mathbf{e}_i\otimes\mathbf{e}_j\otimes\mathbf{e}_k\otimes\cdots)\otimes (\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots).
$$
Applying changes of bases and dual bases, we obtain
$$
\begin{aligned}
T &amp;amp;= T^{ijk\cdots}_{rst\cdots}(B^a_i\tilde{\mathbf{e}}_a\otimes B^b_j\tilde{\mathbf{e}}_b\otimes B^c_k\tilde{\mathbf{e}}_c\otimes\cdots)\otimes (F^r_x\tilde{\epsilon}^x\otimes F^s_y\tilde{\epsilon}^y\otimes F^t_z \tilde{\epsilon}^z\otimes\cdots) \\
&amp;amp;= (B^a_i B^b_j B^c_k\cdots) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z \cdots) (\tilde{\mathbf{e}}_a\otimes \tilde{\mathbf{e}}_b\otimes \tilde{\mathbf{e}}_c\otimes\cdots)\otimes (\tilde{\epsilon}^x\otimes \tilde{\epsilon}^y\otimes \tilde{\epsilon}^z\otimes\cdots).
\end{aligned}
$$
But the definition of the tensor $T$ in the new bases and dual bases is
$$
T = \tilde{T}^{abc\cdots}_{xyz\cdots}(\tilde{\mathbf{e}}_a\otimes \tilde{\mathbf{e}}_b\otimes \tilde{\mathbf{e}}_c\otimes\cdots)\otimes (\tilde{\epsilon}^x\otimes \tilde{\epsilon}^y\otimes \tilde{\epsilon}^z\otimes\cdots).
$$
This gives us the general tensor transformation rule:
$$
\tilde{T}^{abc\cdots}_{xyz\cdots}=(B^a_i B^b_j B^c_k\cdots ) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z\cdots).
$$&lt;/p&gt;
&lt;p&gt;When we apply an ($m$,$n$)-tensor tensor $T$ to an ($n$,0)-tensor (or a rank-$n$ vector) $D$, we will obtain an ($m$,0)-tensor (or a rank-$m$ vector) $T(D)$. This is a generalization of applying a (rank-1) covector (i.e., a (1,0)-tensor) to a $rank-1$ vector (i.e., a (0,1)-tensor) resulting in a scalar (i.e., a (0,0)-tensor).&lt;/p&gt;
&lt;p&gt;Specifically, suppose that
$$
D = D^{opq\cdots}(\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots).
$$
Then, we have
$$
T(D)=T^{ijk\cdots}_{rst\cdots}(\mathbf{e}_i\otimes\mathbf{e}_j\otimes\mathbf{e}_k\otimes\cdots)\otimes (\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots)(D^{opq\cdots}\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots).
$$
Note that there are multiple ways in which we can associate each covector with a vector in $(T_{rst\cdots}\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots)(D^{opq\cdots}\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots)$. To avoid ambiguity, we should always clearly specify such association using Einstein&amp;rsquo;s notation. That is, whether it means $T(D)=T^{ijk\cdots}_{rst\cdots} D^{rst\cdots}$ or $T(D)=T^{ijk\cdots}_{rst\cdots} D^{rts\cdots}$ or anything else.&lt;/p&gt;
&lt;p&gt;Note that a tensor is a multilinear map, which is linear when all inputs except one are held constant:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$T(x_1,\cdots,nx_i,\cdots,x_n)=nT(x_1,\cdots,x_i,\cdots,x_n),~\forall i$,&lt;/li&gt;
&lt;li&gt;$T(x_1,\cdots,x_i+y_i,\cdots,x_n)=T(x_1,\cdots,x_i,\cdots,x_n)+T(x_1,\cdots,y_i,\cdots,x_n),~\forall i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Kronecker products are tensor products expressed in the array representation of tensors. For examples, matrices are Kronecker products of (column vector, row vector) pairs, and metric tensors are Kronecker products of (row vector, row vector) pairs.&lt;/p&gt;
&lt;h3 id=&#34;tensor-product-spaces&#34;&gt;Tensor Product Spaces&lt;/h3&gt;
&lt;p&gt;Let $n\in S$ be a scalar, $\mathbf{v},\mathbf{u}\in V$ be vectors, and $\alpha,\beta\in V^*$ be covectors. Tensor products satisfy the following rules&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$n(\mathbf{v}\otimes\alpha)=(n\mathbf{v})\otimes\alpha=\mathbf{v}\otimes(n\alpha)$,&lt;/li&gt;
&lt;li&gt;$\mathbf{v}\otimes\alpha+\mathbf{v}\otimes\beta=\mathbf{v}\otimes(\alpha+\beta)$,&lt;/li&gt;
&lt;li&gt;$\mathbf{v}\otimes\alpha+\mathbf{u}\otimes\alpha=(\mathbf{v}+\mathbf{u})\otimes\alpha$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can form tensor product spaces $V\otimes V$, $V\times V^*$, $V^*\times V$, and $V^*\times V^*$ using tensor products, and we can continue forming larger tensor products such as $V\otimes V^* \otimes V \otimes \cdots$ and so forth. A tensor space associated with a tensor $T$ contains elements obtained by any number of summations with $T$ in any order.&lt;/p&gt;
&lt;p&gt;For example, consider a tensor space $V^* \otimes V \otimes V^* \otimes V^*$ associated with the tensor $T^{~j}_{i~kl}\epsilon^i\mathbf{e}_j\epsilon^k\epsilon^l$. Below are some example elements in this tensor space:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T^{~j}_{i~kl} v^i \alpha_j w^k u^l\quad V \times V^* \times V \times V \to S$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} U^{ikl}\beta_j\quad (V \otimes V \otimes V)\times V^* \to S$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} \alpha_j D^{kl}\quad V^* \times (V \otimes V) \to V^*$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} L^i_j \quad (V \otimes V^*)\to (V^* \otimes V^*)$,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;musical-isomorphism&#34;&gt;Musical Isomorphism&lt;/h3&gt;
&lt;p&gt;We would like to find a meaningful partner covector $\nu=v_i\epsilon^i\in V^*$ for each vector $\mathbf{v}=v^i\mathbf{e}_i\in V$. It turns out that one such partnership is called musical isomorphism, which is defined using the metric tensor $g\in V^*\otimes V^*$ (i.e., dot product).&lt;/p&gt;
&lt;p&gt;Musical isomorphism defines the partner of the vector $\mathbf{v}\in V$ as the linear function $\nu:V\to S$ such that
$$
\nu=g(\cdot,\mathbf{v})=g_{ik}\epsilon^i\otimes\epsilon^k(v^j\mathbf{e}_j)=g_{ik}v^j\epsilon^i\otimes\epsilon^k(\mathbf{e}_j)=g_{ik} v^j \epsilon^i\delta^k_j=(g_{ij}v^j)\epsilon^i.
$$
But we have $\nu=v_i\epsilon^i$ by definition. This defines the flat operation which lowers the index of the components $v^j$ of the vector $\mathbf{v}$ by
$$
v_i=g_{ij}v^j.
$$
Therefore, the partner covector $\nu$ for the vector $\mathbf{v}$ is given by
$$
\nu=v_i\epsilon^i=g_{ij}v^j\epsilon^i.
$$&lt;/p&gt;
&lt;p&gt;Another way to think about the flat operation for lowering vector component indices $v^i$ is to consider the dot product between the corresponding basis vector $\mathbf{e}_i$ and the vector $\mathbf{v}$:
$$
v_i=g(\mathbf{e}_i,\mathbf{v})=g(\mathbf{e}_i,v^j\mathbf{e}_j)=v^j g(\mathbf{e}_i,\mathbf{e}_j)=g_{ij}v^j.
$$
Geometrically, this tells us that $v_i$ is the projection of the vector $\mathbf{v}$ onto the basis vector $\mathbf{e}_i$. Note that $v^i\not=v_i$ in general. The only exception is in the orthonormal basis where we have
$$
v_i=g_{ij}v^j=\delta_{ij}v^j=v^i.
$$&lt;/p&gt;
&lt;p&gt;This is a meaningful partnership because it holds in any basis by definition:
$$
\tilde{v}_i=\tilde{g}_{ij}\tilde{v}^j \quad\text{and}\quad\nu=\tilde{v}_i\tilde{\epsilon}^i=\tilde{g}_{ij}\tilde{v}^j\tilde{\epsilon}^i.
$$&lt;/p&gt;
&lt;p&gt;We can define the inverse of the metric tensor as $h^{ki}\in V\otimes V$ such that composing $g$ and $h$ results in the identity:
$$
h^{ij}g_{jk}=\delta^i_k.
$$
Then, the sharp operation which raises the index of the components $v_j$ of the covector $\nu$ (i.e., the inverse of the flat operation) can be derived as follows:
$$
h^{ij}v_j=h^{ij}g_{jk}v^k=\delta^i_k v^k=v^i,
$$
$$
\mathbf{v}=v^i\mathbf{e}_i=h^{ij}v_j\mathbf{e}_i.
$$
Likewise, this holds in any basis by definition:
$$
\tilde{v}^i=\tilde{h}^{ij}\tilde{v}_j,
$$
$$
\mathbf{v}=\tilde{v}^i\tilde{\mathbf{e}}_i=\tilde{h}^{ij}\tilde{v}_j\tilde{\mathbf{e}}_i.
$$&lt;/p&gt;
&lt;p&gt;The flat and sharp operations for lowering and raising indices can be applied to tensors of any rank in general. For example, consider the tensor $T=T^i_{jk}\mathbf{e}_i\otimes\epsilon^j\otimes\epsilon^k\in V\otimes V^* \otimes V^*$. We can raise the index $j$ by $T^i_{jk}h^{jl}=T^{il}_k$, which results in a new tensor $T&amp;rsquo;=T^{il}_k \mathbf{e}_i\otimes\mathbf{e}_l\otimes\epsilon^k\in V\otimes V \otimes V^*$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</title>
      <link>https://wenlin-chen.github.io/publication/wu2023has/</link>
      <pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/wu2023has/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction</title>
      <link>https://wenlin-chen.github.io/publication/chen2023meta/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2023meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bilevel Optimization</title>
      <link>https://wenlin-chen.github.io/post/bilevel_optimization/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/bilevel_optimization/</guid>
      <description>&lt;h3 id=&#34;bilevel-optimization&#34;&gt;Bilevel Optimization&lt;/h3&gt;
&lt;h4 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Consider the following bilevel optimization problem:
$$
\begin{aligned}
x^*&amp;amp;=\underset{x}{\operatorname{argmin}}~f(x,y^*(x))\quad\text{(outer optimization)}, \\
s.t.\quad y^*(x)&amp;amp;=\underset{y}{\operatorname{argmin}}~g(x,y)\quad\text{(inner optimization)}.
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;understanding-the-bilevel-objective&#34;&gt;Understanding the Bilevel Objective&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The bilevel optimization objective can be understood by separately considering the two objectives.
&lt;ul&gt;
&lt;li&gt;Inner optimization: for each given value of $x$, the best response function $y^*(x)$ is defined as the value $y$ that minimizes the inner objective $g(x,y)$.&lt;/li&gt;
&lt;li&gt;Outer optimization: given the best response function $y^{*}(x)$, the optimum $x^*$ is defined as the value $x$ that minimizes the outer objective $f(x,y^{*}(x))$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This results in a nested optimization procedure as evaluating the outer objective $f(x,y^*(x))$ at each value of $x$ requires solving an inner optimization problem to obtain the best response function $y^{*}(x)$ at that value of $x$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solving-bilevel-optimization&#34;&gt;Solving Bilevel Optimization&lt;/h3&gt;
&lt;h4 id=&#34;gradient-based-optimization&#34;&gt;Gradient-based Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The outer objective $f(x,y^{*}(x))$ can be seen as an implicit function of $x$ alone.&lt;/li&gt;
&lt;li&gt;We can calculate the hyper-gradient (i.e., total derivative) of $f(x,y^{*}(x))$ with respect to $x$ using the chain rule:
$$\frac{d f(x,y^{*}(x))}{d x}=\frac{\partial f(x,y^{*}(x))}{\partial x}+\frac{\partial f(x,y^{*}(x))}{\partial y^*(x)}\frac{\partial y^*(x)}{\partial x}.$$
&lt;ul&gt;
&lt;li&gt;The two gradients $\frac{\partial f(x,y^{*}(x))}{\partial x}$ and $\frac{\partial f(x,y^{*}(x))}{\partial y^*(x)}$ can be easily calculated by automatic differentiation.&lt;/li&gt;
&lt;li&gt;Calculating the Jacobian $\frac{\partial y^*(x)}{\partial x}$ is tricky, since the best response function $y^*(x)$ itself is defined by an argmin function in the inner optimization.&lt;/li&gt;
&lt;li&gt;If the inner optimization is also solved by gradient-based optimzation, then calculating the Jacobian $\frac{\partial y^*(x)}{\partial x}$ naively by automatic differentiation will require tracking the gradients through many iterations of the inner optimization, which is computationally intractable in practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;implicit-function-theorem-ift&#34;&gt;Implicit Function Theorem (IFT)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;An important observation is that $y^*(x)$ is a critical point (i.e., minimum) of $g(x,y)$ for each given $x$.&lt;/li&gt;
&lt;li&gt;This enables us to employ Implicit Function Theorem (IFT) to calculate the Jacobian $\frac{\partial y^*(x)}{\partial x}$ for any $x&amp;rsquo;$:
$$\left.\frac{\partial y^*(x)}{\partial x}\right|_{x=x&amp;rsquo;}=-\left.\left(\frac{\partial^2 g(x,y)}{\partial y \partial y^T}\right)^{-1}\frac{\partial^2 g(x,y)}{\partial y \partial x^T}\right|_{x=x&amp;rsquo;,y=y^*(x&amp;rsquo;)}.$$
&lt;ul&gt;
&lt;li&gt;The mixed partial derivatives $\frac{\partial^2 g(x,y)}{\partial y \partial x^T}$ can be easily calculated by automatic differentiation.&lt;/li&gt;
&lt;li&gt;The inverse Hessian $\left(\frac{\partial^2 g(x,y)}{\partial y \partial y^T}\right)^{-1}$ may be calculated exactly by automatic differentiation or approximated by Neumann approximation or conjugate gradient, depending on the size of the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-in-machine-learning&#34;&gt;Applications in Machine Learning&lt;/h3&gt;
&lt;h4 id=&#34;hyperparmaeter-optimization&#34;&gt;Hyperparmaeter Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Let $x$ be the hyperparameters and $y$ be the parameters of a machine learning model.&lt;/li&gt;
&lt;li&gt;Let $f$ be the validation loss function and $g$ be the training loss function.&lt;/li&gt;
&lt;li&gt;Inner optimization corresponds to finding the optimal model parameter $y^*(x)$ by minimizing the training loss $g(x,y)$ given current hyperparameters $x$.&lt;/li&gt;
&lt;li&gt;Outer optimization corresponds to finding the optimal hyperparameters $x^*$ that minimizes the validation loss $f(x,y^{*}(x))$ given the optimal model parameters $y^{*}(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;meta-learning&#34;&gt;Meta-learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Let $x$ be the meta-learned parameters shared across all tasks and $y$ be the task-specific parameters.&lt;/li&gt;
&lt;li&gt;Let $f$ be the validation loss function and $g$ be the training loss function.&lt;/li&gt;
&lt;li&gt;Inner optimization corresponds to finding the optimal task-specific parameters $y^*(x)$ for each task by minimizing the training loss $g(x,y)$ given current meta-learned parameters $x$ shared across all tasks.&lt;/li&gt;
&lt;li&gt;Outer optimization corresponds to finding the optimal meta-learned parameters $x^*$ that minimizes the expected validation loss $f(x,y^{*}(x))$ across tasks given the optimal task-specific parameters $y^{*}(x)$ for each task.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Optimal Client Sampling for Federated Learning</title>
      <link>https://wenlin-chen.github.io/publication/chen2022optimal/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2022optimal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Evaluation Framework for the Objective Functions of De Novo Drug Design Benchmarks</title>
      <link>https://wenlin-chen.github.io/publication/tripp2022evaluation/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/tripp2022evaluation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Causal Representation Learning for Latent Space Optimization</title>
      <link>https://wenlin-chen.github.io/publication/chen2021causal/</link>
      <pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2021causal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>To Ensemble or Not Ensemble: When Does End-to-End Training Fail?</title>
      <link>https://wenlin-chen.github.io/publication/webb2020ensemble/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/webb2020ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenlin-chen.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
