<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes | Wenlin Chen</title>
    <link>https://wenlin-chen.github.io/post/</link>
      <atom:link href="https://wenlin-chen.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Notes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2024 Wenlin Chen</copyright><lastBuildDate>Fri, 05 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Notes</title>
      <link>https://wenlin-chen.github.io/post/</link>
    </image>
    
    <item>
      <title>Tensor Algebra</title>
      <link>https://wenlin-chen.github.io/post/tensor-algebra/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/tensor-algebra/</guid>
      <description>&lt;p&gt;These are my learning notes for the lecture series &lt;a href=&#34;https://www.youtube.com/playlist?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tensors for Beginners&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-applications-of-tensors&#34;&gt;Motivation and Applications of Tensors&lt;/h3&gt;
&lt;p&gt;Tensors provide insights about how geometry works. Below are some examples that involve tensors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General relativity (metric tensor):
&lt;ul&gt;
&lt;li&gt;curved space-time,&lt;/li&gt;
&lt;li&gt;expanding universe.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quantum mechanics and quantum computing:
&lt;ul&gt;
&lt;li&gt;quantum superposition (linear combination),&lt;/li&gt;
&lt;li&gt;quantum engtanglement (tensor product).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimization.&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;definitions-of-tensors&#34;&gt;Definitions of Tensors&lt;/h3&gt;
&lt;p&gt;Roughtly speaking, tensors are objects defined by the way they transform. Tensor algbra focuses on the analysis of individual tensors, which generalizes linear algebra. Tensor calculus focuses on the analysis of tensor fields, which generalizes multivariate calculus.&lt;/p&gt;
&lt;p&gt;Formally, there are several definitions of tensors from different perspectives.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;(Array Definition) A tensors is a multi-dimensional array, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rank-0 tensor: scalar,&lt;/li&gt;
&lt;li&gt;rank-1 tensor: vecoter,&lt;/li&gt;
&lt;li&gt;rank-2 tensor: matrix,&lt;/li&gt;
&lt;li&gt;rank-3 tensor: ?,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is not a good definition as arrays are not what tensors fundamentally are. The array definition ignores the geometric meaning behind tensors and is not helpful for understanding geometry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Coordinate Definition) A tensor is an object invariant under a change of coordinates, which has components that change in a special, predictable way under a change of coordinates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tensor object itself is intrinsic and does not depend on the choice of coordinate system.&lt;/li&gt;
&lt;li&gt;The components of a tensor change under different coordinate systems in a specific way that can be figured out. We will get to this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Algebra Definition) A tensors is a collections of vectors and covectors combined together using the tensor product.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a consice and probably the best definition of tensors.&lt;/li&gt;
&lt;li&gt;But&amp;hellip; What are covectors? What is tensor product? We will get to this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Calculus Definition) Tensors are partial derivatives and gradients that transform with the Jacobian matrix.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is also a useful definition, but we will focus on the coordinate and algebra definitions in tensor algebra.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will begin with special examples of tensors that we have seen in linear algebra and then gradually generalize the concept of tensors.&lt;/p&gt;
&lt;h3 id=&#34;change-of-basis&#34;&gt;Change of Basis&lt;/h3&gt;
&lt;p&gt;Consider an old basis $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ and a new basis $\tilde{E}=[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]$.&lt;/p&gt;
&lt;p&gt;The forward transformation $F$ builds the new basis from the old basis:
$$
\tilde{\mathbf{e}}_i = \sum_{j=1}^n F^j_i\mathbf{e}_j.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]=[\mathbf{e}_1,\cdots,\mathbf{e}_n]
\begin{bmatrix}
F^1_1 &amp; \cdots &amp; F^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
F^n_1 &amp; \cdots &amp; F^n_n
\end{bmatrix}.
$$ --&gt;
&lt;p&gt;The backward transformation $B$ builds the old basis from the new basis:
$$
\mathbf{e}_i = \sum_{j=1}^n B^j_i\tilde{\mathbf{e}}_j.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
[\mathbf{e}_1,\cdots,\mathbf{e}_n]=[\tilde{\mathbf{e}}_1,\cdots,\tilde{\mathbf{e}}_n]
\begin{bmatrix}
B^1_1 &amp; \cdots &amp; B^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
B^n_1 &amp; \cdots &amp; B^n_n
\end{bmatrix}.
$$ --&gt;
&lt;p&gt;Composing the forward and backward transformation should result in the identity transformation:
$$
\mathbf{e}_i = \sum_{j=1}^n B^j_i\left(\sum_{k=1}^n F^k_j\mathbf{e}_k\right)=\sum_{k=1}^n\left(\sum_{j=1}^n F^k_j B^j_i\right)\mathbf{e}_k
$$
$$
\implies\sum_{j=1}^n F^k_j B^j_i=\delta^k_i.
$$&lt;/p&gt;
&lt;!-- or in the matrix-vector equation form:
$$
FB=
\begin{bmatrix}
F^1_1 &amp; \cdots &amp; F^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
F^n_1 &amp; \cdots &amp; F^n_n
\end{bmatrix}
\begin{bmatrix}
B^1_1 &amp; \cdots &amp; B^1_n \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
B^n_1 &amp; \cdots &amp; B^n_n
\end{bmatrix}=
\begin{bmatrix}
1 &amp; \cdots &amp; 0 \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
0 &amp; \cdots &amp; 1
\end{bmatrix}=I.
$$ --&gt;
&lt;p&gt;Therefore, the forward and backward transformations are inverses of each other.&lt;/p&gt;
&lt;!-- $$
B=F^{-1}\quad\text{and}\quad F=B^{-1}.
$$ --&gt;
&lt;h3 id=&#34;vectors-and-vector-spaces&#34;&gt;Vectors and Vector Spaces&lt;/h3&gt;
&lt;p&gt;A vector space $(V,S,+,\cdot)$ consists of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$V$: a set of vectors,&lt;/li&gt;
&lt;li&gt;$S$: a set of scalars,&lt;/li&gt;
&lt;li&gt;$+$: a vector addtion rule such that $\mathbf{v}+\mathbf{w}\in V,~\forall\mathbf{v},\mathbf{w}\in V$,&lt;/li&gt;
&lt;li&gt;$\cdot$: a vector scaling rule such that $a\cdot\mathbf{v}\in V,~\forall a\in S,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We usually omit $\cdot$ and just write $a\mathbf{v}$ instead.&lt;/p&gt;
&lt;p&gt;Vectors are a kind of tensor. Vectors are invariant to the coordinate systems, but the components of vectors are not.&lt;/p&gt;
&lt;p&gt;Let $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ be a basis of $(V,S,+,\cdot)$. First, we note that the basis vectors $\mathbf{e}_i$ are vectors. A vector $\mathbf{v}\in V$ can be represented as a linear combination of the basis vectors:
$$
\mathbf{v}=\sum_{i=1}^n v^i \mathbf{e}_i \quad\text{for some } v^i\in S.
$$
The components of $\mathbf{v}$ in the basis $E$ are
$$
\mathbf{v}=
\begin{bmatrix}
v^1 \\
\vdots \\
v^n
\end{bmatrix}_{E}
$$
This tells us that column vectors are the array representations of vectors, which is the array definition of vectors.&lt;/p&gt;
&lt;h3 id=&#34;vector-transformation&#34;&gt;Vector Transformation&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s represent the vector components of $\mathbf{v}$ in a different basis $\tilde{E}$:
$$
\mathbf{v}=\sum_{j=1}^n \tilde{v}^j \tilde{\mathbf{e}}_j = \sum_{j=1}^n \tilde{v}^j\left(\sum_{i=1}^n F^i_j\mathbf{e}_i\right) = \sum_{i=1}^n\left(\sum_{j=1}^n F^i_j\tilde{v}^j\right)\mathbf{e}_i \quad\text{but}\quad \mathbf{v}=\sum_{i=1}^n v^i \mathbf{e}_i
$$
$$
\implies v^i = \sum_{j=1}^n F^i_j\tilde{v}^j.
$$&lt;/p&gt;
&lt;p&gt;Likewise, we have
$$
\mathbf{v}=\sum_{i=1}^n v^i \mathbf{e}_i = \sum_{i=1}^n v^i  \left( \sum_{j=1}^n B^j_i\tilde{\mathbf{e}}_j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n B^j_i v^i \right)\tilde{\mathbf{e}}_j \quad\text{but}\quad \mathbf{v}=\sum_{j=1}^n \tilde{v}^j \tilde{\mathbf{e}}_j
$$
$$
\implies \tilde{v}^j = \sum_{j=1}^n B^j_i v^i.
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now compare a change of basis to vector transformation:
$$
\tilde{\mathbf{e}}_j = \sum_{i=1}^n F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j = \sum_{i=1}^n B^i_j\tilde{\mathbf{e}}_i
$$
$$
v^i = \sum_{j=1}^n F^i_j\tilde{v}^j \quad\quad \tilde{v}^i = \sum_{j=1}^n B^i_j v^j
$$&lt;/p&gt;
&lt;p&gt;It is interesting to note that vector transformation behaves contrarily to a change of basis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change of basis:
&lt;ul&gt;
&lt;li&gt;Forward transformation changes the old basis into the new basis.&lt;/li&gt;
&lt;li&gt;Backward transformation changes the new basis into the old basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vector transformation:
&lt;ul&gt;
&lt;li&gt;Forward transformation changes the vector components from the new basis to the old basis.&lt;/li&gt;
&lt;li&gt;Backward transformation changes the vector components from the old basis to the new basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We say that vector components are contravariant because they contra-vary with a change of basis. As a reminder, we always put the indices of the vector components above the letters. We say that basis vectors are covariant and put the indices of the basis vectors below the letters.&lt;/p&gt;
&lt;h3 id=&#34;covectors-and-dual-vector-spaces&#34;&gt;Covectors and Dual Vector Spaces&lt;/h3&gt;
&lt;p&gt;Covectors (linear forms) are functions $\alpha: V\to S$ that map the vectors in $V$ to the scalars in $S$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha(\mathbf{v}+\mathbf{w})=\alpha(\mathbf{v})+\alpha(\mathbf{w}),~\forall \mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$\alpha(n\mathbf{v})=n\alpha(\mathbf{v}),\forall n\in S,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dual vector space $(V^*,S,+&amp;rsquo;,\cdot&amp;rsquo;)$ of a vector space $(V,S,+,\cdot)$ is a vector space with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the same set $S$ of scalars&lt;/li&gt;
&lt;li&gt;a different set $V^*$ of vectors ($\alpha\in V^*$ are covectors),&lt;/li&gt;
&lt;li&gt;different addition ($+&amp;rsquo;$) and scaling ($\cdot&amp;rsquo;$) rules such that
&lt;ol&gt;
&lt;li&gt;$(n\cdot\alpha)(\mathbf{v})=n\alpha(\mathbf{v}),~\forall n\in S,\alpha\in V^*,\mathbf{v}\in V$.&lt;/li&gt;
&lt;li&gt;$(\alpha+\beta)(\mathbf{v})=\alpha(\mathbf{v})+\beta(\mathbf{v}),~\forall\alpha,\beta\in V^*,\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As shown above, we usually just write $+$ and $\cdot$ when we add and scale covectors in $V^*$ but need to remember that the addition and scaling rules in $V^*$ are actually different from those in $V$.&lt;/p&gt;
&lt;h3 id=&#34;dual-basis-and-covector-components&#34;&gt;Dual Basis and Covector Components&lt;/h3&gt;
&lt;p&gt;Covectors are a kind of tensor. Covectors are invariant to the coordinate systems, but the components of covectors are not.&lt;/p&gt;
&lt;p&gt;Take the basis $E=[\mathbf{e}_1,\cdots,\mathbf{e}_n]$ for a vector space $V$. We introduce the (Kronecker) dual basis $\mathcal{E}=[\epsilon^1,\cdots,\epsilon^n]^T$ ($\epsilon^i:V\to S$) for its dual vector space $V^*$:
$$
\epsilon^i(\mathbf{e}_j)=\delta^i_j.
$$&lt;/p&gt;
&lt;p&gt;Each covector $\epsilon^i\in V^*$ in the dual basis outputs the corresponding vector component $v^i$ in the basis $E$:
$$
\epsilon^i(\mathbf{v})=\epsilon^i\left( \sum_{j=1}^n v^j \mathbf{e}_j \right)=\sum_{j=1}^n v^j \epsilon^i(\mathbf{e}_j)=v^i.
$$&lt;/p&gt;
&lt;p&gt;For any covector $\alpha\in V^*$, we define
$$
\alpha(\mathbf{e}_i)=\alpha_i.
$$&lt;/p&gt;
&lt;p&gt;Now, the covector $\alpha\in V^*$ can be represented as
$$
\alpha(\mathbf{v})=\alpha\left(\sum_{i=1}^n v^i \mathbf{e}_i \right)=\sum_{i=1}^n v^i \alpha(\mathbf{e}_i) = \sum_{i=1}^n \alpha_i \epsilon^i(\mathbf{v})=\left(\sum_{i=1}^n \alpha_i \epsilon^i\right)(\mathbf{v}).
$$
$$
\implies \alpha = \sum_{i=1}^n \alpha_i \epsilon^i.
$$&lt;/p&gt;
&lt;p&gt;Therefore, the covector components in the dual basis $\mathcal{E}$ are
$$
\alpha=[\alpha_1, \cdots, \alpha_n]_{\mathcal{E}}.
$$&lt;/p&gt;
&lt;p&gt;This tells us that row vectors are the array representation of covectors, which is the array definition of covectors.&lt;/p&gt;
&lt;!-- $$
\alpha(\mathbf{v})=\alpha\left(\sum_{i=1}^n v^i \mathbf{e}\_i \right)=\sum\_{i=1}^n v^i \alpha(\mathbf{e}_i) = \sum\_{i=1}^n \alpha_i v^i.
$$ --&gt;
&lt;p&gt;Geometrically, a covector is a stack of linearly spaced, straight contour lines. A covector maps a vector into a scalar specified by the number of covector contour lines that the vector covers.&lt;/p&gt;
&lt;h3 id=&#34;change-of-dual-basis-and-covector-transformation&#34;&gt;Change of Dual Basis and Covector Transformation&lt;/h3&gt;
&lt;p&gt;Consider an old dual basis $\mathcal{E}=[\epsilon^1,\cdots,\epsilon^n]^T$ and a new dual basis $\tilde{\mathcal{E}}=[\tilde{\epsilon}^1,\cdots,\tilde{\epsilon}^n]^T$ for $V^*$. Let&amp;rsquo;s build the new dual basis from the old dual basis using a transformation $Q$:
$$
\tilde{\epsilon}^i=\sum_{j=1}^n Q^i_j\epsilon^j.
$$
Applying forward transformation to $\tilde{\mathbf{e}}_k$ gives
$$
\tilde{\epsilon}^i(\tilde{\mathbf{e}}_k)=\sum_{j=1}^n Q^i_j\epsilon^j(\tilde{\mathbf{e}}_k)=\sum_{j=1}^n Q^i_j\epsilon^j\left(\sum_{l=1}^n F^l_k\mathbf{e}_l\right)=\sum_{j=1}^n \sum_{l=1}^n Q^i_j F^l_k\epsilon^j(\mathbf{e}_l)=\sum_{j=1}^n \sum_{l=1}^n Q^i_j F^l_k\delta^j_l=\sum_{j=1}^n Q^i_j F^j_k.
$$
But we know that $\tilde{\epsilon}^i(\tilde{\mathbf{e}}_k)=\delta^i_k$ by definition. This implies that
$$
\sum_{j=1}^n Q^i_j F^j_k=\delta^i_k\quad\implies\quad Q=B.
$$
Therefore, we build the new dual basis from the old dual basis using the backward transformation:
$$
\tilde{\epsilon}^i=\sum_{j=1}^n B^i_j\epsilon^j.
$$
Likewise, we build the old dual basis from the new dual basis using the forward transformation:
$$
\epsilon^i=\sum_{j=1}^n F^i_j\tilde{\epsilon}^j.
$$
Let&amp;rsquo;s now compare a change of basis to vector transformation:
$$
\tilde{\mathbf{e}}_j = \sum_{i=1}^n F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j = \sum_{i=1}^n B^i_j\tilde{\mathbf{e}}_i
$$
$$
\epsilon^i=\sum_{j=1}^n F^i_j\tilde{\epsilon}^j \quad\quad \tilde{\epsilon}^i=\sum_{j=1}^n B^i_j \epsilon^j
$$
It is interesting to note that a change of dual basis behaves contrarily to a change of basis.&lt;/p&gt;
&lt;p&gt;As for covector transformation, we first represent a covector in two dual bases:
$$
\alpha = \sum_{i=1}^n \alpha_i\epsilon^i = \sum_{j=1}^n \tilde{\alpha}_j\tilde{\epsilon}^j.
$$
But changing the dual basis gives
$$
\alpha = \sum_{i=1}^n \alpha_i\epsilon^i = \sum_{i=1}^n \alpha_i \left(\sum_{j=1}^n F^i_j\tilde{\epsilon}^j\right)=\sum_{j=1}^n \left(\sum_{i=1}^n F^i_j\alpha_i\right)\tilde{\epsilon}^j.
$$
This implies that the forward transformation changes the covector components from the old dual basis to the new dual basis:
$$
\tilde{\alpha}_j = \sum_{i=1}^n F^i_j\alpha_i.
$$
Likewise, the backward transformation changes the covector components from the new dual basis to the old dual basis:
$$
\alpha_j = \sum_{i=1}^n B^i_j\tilde{\alpha}_i.
$$
This tells us covector transformation is covariant to a change of basis and contravariant to a change of dual basis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now summarize what we have learned so far about changes of basis/dual basis and vector/covector transformations:
$$
\tilde{\mathbf{e}}_j = \sum_{i=1}^n F^i_j\mathbf{e}_i \quad\quad \mathbf{e}_j = \sum_{i=1}^n B^i_j\tilde{\mathbf{e}}_i \quad\quad\text{and}\quad\quad v^i = \sum_{j=1}^n F^i_j\tilde{v}^j \quad\quad \tilde{v}^i = \sum_{j=1}^n B^i_j v^j
$$
$$
\epsilon^i=\sum_{j=1}^n F^i_j\tilde{\epsilon}^j \quad\quad \tilde{\epsilon}^i=\sum_{j=1}^n B^i_j\epsilon^j \quad\quad\text{and}\quad\quad \tilde{\alpha}_j = \sum_{i=1}^n F^i_j\alpha_i \quad\quad \alpha_j = \sum_{i=1}^n B^i_j\tilde{\alpha}_i
$$&lt;/p&gt;
&lt;h3 id=&#34;linear-maps&#34;&gt;Linear Maps&lt;/h3&gt;
&lt;p&gt;Linear maps $L:V\to W$ map vectors in $V$ to vectors in $W$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$L(\mathbf{v}+\mathbf{w})=L(\mathbf{v})+L(\mathbf{w})$,&lt;/li&gt;
&lt;li&gt;$L(n\mathbf{v})=nL(\mathbf{v})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s consider a speical case where $W=V$ from now on. In the array definition, linear maps are matrices that transform column vectors but do not transform basis. We define the transformation of a copy of each basis vector under a linear map $L$ by
$$
L(\mathbf{e}_i)=\sum_{j=1}^n L^j_i\mathbf{e}_j,
$$
which essentially defines the matrix components in the basis $E$ (i.e., linear maps in the array representation):
$$
L=\begin{bmatrix}
L^1_1 &amp;amp; \cdots &amp;amp; L^1_n \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
L^n_1 &amp;amp; \cdots &amp;amp; L^n_n
\end{bmatrix}_{E}.
$$
Let $\mathbf{v}=\sum_{i=1}^n v^i \mathbf{e}_i$ and $\mathbf{w}=\sum_{i=1}^n w^i \mathbf{e}_i$ respectively be the input and output vectors represented in the basis $E$. Then, we have
$$
\mathbf{w}=L(\mathbf{v})=L\left( \sum_{i=1}^n v^i \mathbf{e}_i \right)=\sum_{i=1}^n v^i L(\mathbf{e}_i)=\sum_{i=1}^n v^i \left( \sum_{j=1}^n L^j_i\mathbf{e}_j \right)=\sum_{j=1}^n\left(\sum_{i=1}^n L^j_i v^i \right) \mathbf{e}_j.
$$
This implies that
$$
w^i = \sum_{j=1}^n L^i_j v^j,
$$
which is the matrix-vector multiplication rule:
$$
\begin{bmatrix}
w^1 \\
\vdots \\
w^n
\end{bmatrix}_{E}=
\begin{bmatrix}
L^1_1 &amp;amp; \cdots &amp;amp; L^1_n \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
L^n_1 &amp;amp; \cdots &amp;amp; L^n_n
\end{bmatrix}_{E}
\begin{bmatrix}
v^1 \\
\vdots \\
v^n
\end{bmatrix}_{E}.
$$
It is important to note that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The transformed vectors are in the same basis as the input vectors.&lt;/li&gt;
&lt;li&gt;The $i$-th column vector in a matrix is what a copy of the $i$-th basis vector will be transformed into.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Geometrically, linear maps are spatial transformations that keep lines parallel, keep lines evenly spaced, and keep the origin unchanged. A linear map can be a combination of vector scaling and rotation but not vector translation.&lt;/p&gt;
&lt;h3 id=&#34;linear-map-transformation&#34;&gt;Linear Map Transformation&lt;/h3&gt;
&lt;p&gt;Linear maps are a kind of tensor. Linear maps are invariant to the coordinate systems, but the components of linear maps are not.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s transform a linear map from an old basis $E$ to a new basis $\tilde{E}$. By definition, we have
$$
L(\tilde{\mathbf{e}}_i)=\sum_{l=1}^n \tilde{L}^l_i \tilde{\mathbf{e}}_l.
$$
But we also have
$$
L(\tilde{\mathbf{e}}_i)=L\left(  \sum_{j=1}^n F^j_i \mathbf{e}_j \right)=\sum_{j=1}^n F^j_i L(\mathbf{e}_j) = \sum_{j=1}^n F^j_i \sum_{k=1}^n L^k_j \mathbf{e}_k = \sum_{j=1}^n F^j_i \sum_{k=1}^n L^k_j   \sum_{l=1}^n B^l_k \tilde{\mathbf{e}}_l = \sum_{l=1}^n \left( \sum_{j=1}^n \sum_{k=1}^n B^l_k L^k_j F^j_i  \right) \tilde{\mathbf{e}}_l.
$$
This implies that
$$
\tilde{L}^l_i = \sum_{j=1}^n \sum_{k=1}^n B^l_k L^k_j F^j_i.
$$
For notational simplicity, we will use the Einstein&amp;rsquo;s notation from now on, which drops the summation symbols $\sum$ (since they can be inferred from the context by contracting all corresponding upper and lower indices):
$$
\tilde{L}^l_i = B^l_k L^k_j F^j_i.
$$
Note that multiplying by the identity transformation $I$ does not change a matrix:
$$
(LI)^i_k = L^i_j \delta^j_k = L^i_k.
$$
Then, the backward transformation for the linear map is given by
$$
F^s_l \tilde{L}^l_i B^i_t = F^s_l B^l_k L^k_j F^j_i B^i_t = \delta^s_k L^k_j \delta^j_t = L^s_t.
$$&lt;/p&gt;
&lt;h3 id=&#34;tensors-in-einsteins-notation&#34;&gt;Tensors in Einstein&amp;rsquo;s Notation&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s summarize the tensors we have learned so far in Einstein&amp;rsquo;s notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(0,1)-tensors (covariant):
&lt;ul&gt;
&lt;li&gt;Basis:
$$
\tilde{\mathbf{e}}_j = F^i_j \mathbf{e}_i \quad\quad \mathbf{e}_j = B^i_j \tilde{\mathbf{e}}_i
$$&lt;/li&gt;
&lt;li&gt;Covector components:
$$
\tilde{\alpha}_j = F^i_j \alpha_i \quad\quad \alpha_j = B^i_j \tilde{\alpha}_i
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(1,0)-tensors (contravariant):
&lt;ul&gt;
&lt;li&gt;Dual basis:
$$
\tilde{\epsilon}^i = B^i_j\epsilon^j \quad\quad \epsilon^i = F^i_j\tilde{\epsilon}^j
$$&lt;/li&gt;
&lt;li&gt;Vector components:
$$
\tilde{v}^i = B^i_j v^j \quad\quad v^i = F^i_j\tilde{v}^j
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(1,1)-tensors (one contravariant dimension, one covariant dimension):
&lt;ul&gt;
&lt;li&gt;Linear maps:
$$
\tilde{L}^i_j = B^i_k L^k_l F^l_j \quad\quad L^i_j = F^i_k \tilde{L}^k_l B^l_j
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bilinear-forms-metric-tensors-and-their-transformations&#34;&gt;Bilinear Forms, Metric Tensors and Their Transformations&lt;/h3&gt;
&lt;p&gt;Metric tensors $g:V\times V\to S$ are speical bilinear forms that define dot products between vectors such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$ag(\mathbf{v},\mathbf{w})=g(a\mathbf{v},\mathbf{w})=g(\mathbf{v},a\mathbf{w}),~\forall a\in S,~\forall\mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v}+\mathbf{u},\mathbf{w})=g(\mathbf{v},\mathbf{w}) + g(\mathbf{u},\mathbf{w}),~\forall\mathbf{v},\mathbf{u},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{w}+\mathbf{t})=g(\mathbf{v},\mathbf{w}) + g(\mathbf{v},\mathbf{t}),~\forall\mathbf{v},\mathbf{w},\mathbf{t}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{w})=g(\mathbf{w},\mathbf{v}),~\forall\mathbf{v},\mathbf{w}\in V$.&lt;/li&gt;
&lt;li&gt;$g(\mathbf{v},\mathbf{v})\geq 0,~\forall\mathbf{v}\in V$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that 1-3 are rules for general bilinear forms. Rules 4-5 are special for metric tensors, so metric tensors must be symmetric (4) and positive semi-definite (5).&lt;/p&gt;
&lt;p&gt;Metric tensors define the dot product between two vectors $\mathbf{v}$ and $\mathbf{w}$ in a basis $E$:
$$
g(\mathbf{v},\mathbf{w})=g(v^i \mathbf{e}_i,w^j \mathbf{e}_j)=v^i w^j g(\mathbf{e}_i,\mathbf{e}_j).
$$
The array representation of a metric tensor $g$ in the basis $E$ is defined as
$$
g_{ij}=g(\mathbf{e}_i,\mathbf{e}_j),
$$
Therefore, the dot product in the basis $E$ becomes
$$
g(\mathbf{v},\mathbf{w})=v^i w^j g_{ij}.
% \begin{bmatrix}
% v^1 &amp;amp; \cdots &amp;amp; v^n
% \end{bmatrix}_{E}
% \begin{bmatrix}
% g_{11} &amp;amp; \cdots &amp;amp; g_{1n} \\
% \vdots &amp;amp; \ddots &amp;amp; \vdots \\
% g_{n1} &amp;amp; \cdots &amp;amp; g_{nn}
% \end{bmatrix}_{E}
% \begin{bmatrix}
% w^1 \\
% \vdots \\
% w^n
% \end{bmatrix}_{E}.
$$&lt;/p&gt;
&lt;p&gt;Using dot products, we respectively define the norm $\lVert\cdot\rVert$ of a vector and the angle $\left&amp;lt;\cdot,\cdot\right&amp;gt;$ between two vectors as
$$
\lVert\mathbf{v}\rVert^2 = g(\mathbf{v},\mathbf{v})=v^i v^j g_{ij}\geq 0 \quad\text{and}\quad \cos\left&amp;lt;\mathbf{v},\mathbf{w}\right&amp;gt;=\frac{g(\mathbf{v},\mathbf{w})}{\lVert\mathbf{v}\rVert\lVert\mathbf{w}\rVert}=\frac{v^i w^j g_{ij}}{\sqrt{v^i v^j g_{ij}}\sqrt{w^i w^j g_{ij}}}.
$$&lt;/p&gt;
&lt;p&gt;For an orthonormal basis $E$, the metric tensor is given by the identity matrix $g_{ij}=g(\mathbf{e}_i,\mathbf{e}_j)=\delta_{ij}$. This gives us the usual dot product $g(\mathbf{v},\mathbf{w})=v^i w^i$ and the Pythagorean theorem $\lVert\mathbf{v}\rVert^2=v^iv^i$.&lt;/p&gt;
&lt;p&gt;Metric tensors are invariant to the coordinate systems, but the components of metric tensors are not. Let&amp;rsquo;s now a transform metric tensor (or more generally, a bilinear form) from an old basis $E$ to a new basis $\tilde{E}$:
$$
\tilde{g}_{ij}=g(\tilde{\mathbf{e}}_i,\tilde{\mathbf{e}}_j)=g(F^k_i \mathbf{e}_k,F^l_j \mathbf{e}_l)=F^k_iF^l_j g(\mathbf{e}_k,\mathbf{e}_l)=F^k_iF^l_jg_{kl}.
$$
Likewise, the metric tensor transformation from $\tilde{E}$ to $E$ is given by the inverse of the above transformation
$$
g_{kl}=g(\mathbf{e}_k,\mathbf{e}_l)=g(B^i_k \tilde{\mathbf{e}}_i,B^j_l \tilde{\mathbf{e}}_j)=B^i_k B^j_l g(\tilde{\mathbf{e}}_i,\tilde{\mathbf{e}}_j)=B^i_k B^j_l \tilde{g}_{ij}.
$$
Metric tensors (and general bilinear forms) are (0,2)-tensors since both dimensions in a metric tensor are covariant (to a change of basis).&lt;/p&gt;
&lt;p&gt;We can also show that the results of dot products are identical in different bases:
$$
g(\mathbf{v},\mathbf{w})=v^i w^j g_{ij}=(F^i_k\tilde{v}^k)(F^j_l\tilde{v}^l) B^s_i B^t_j \tilde{g}_{st}=(B^s_i F^i_k)(B^t_jF^j_l)\tilde{v}^k\tilde{v}^l\tilde{g}_{st} = \delta^s_k \delta^t_l \tilde{v}^k\tilde{v}^l\tilde{g}_{st}=\tilde{v}^s\tilde{v}^t\tilde{g}_{st}.
$$&lt;/p&gt;
&lt;h3 id=&#34;general-linear-forms-and-tensors&#34;&gt;General Linear Forms and Tensors&lt;/h3&gt;
&lt;p&gt;In general, we define an $n$-(linear) form to be a linear function $V\times V\times \cdots \times V=V^n\to S$, which is a (0,$n$)-tensor or a rank-$n$ covector. Covectors are linear forms or 1-linear forms. Bilinear forms are 2-linear forms.&lt;/p&gt;
&lt;p&gt;A general ($m$,$n$)-tensor $T$ is a tensor with $m$ contravariant dimensions and $n$ covariant dimensions. General tensor transformations are defined as
$$
\tilde{T}^{abc\cdots}_{xyz\cdots}=(B^a_i B^b_j B^c_k\cdots ) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z\cdots),
$$
$$
T^{ijk\cdots}_{rst\cdots}=(F^i_a F^j_b F^k_c\cdots ) \tilde{T}^{abc\cdots}_{xyz\cdots} (B^x_r B^y_s B^z_t\cdots).
$$
We will derive this transformation rule using tensor products later.&lt;/p&gt;
&lt;h3 id=&#34;examples-of-tensor-products&#34;&gt;Examples of Tensor Products&lt;/h3&gt;
&lt;p&gt;In general, tensors can be constructed by combining vectors and covectors using tensor product. We first give a few examples of redefining tensors that we have learned using tensor products below.&lt;/p&gt;
&lt;p&gt;Linear maps are tensor products of vector-covector pairs:
$$
L=L^i_j\mathbf{e}_i\otimes\epsilon^j.
$$
We can verify the definition of linear maps:
$$
\mathbf{w}=L(\mathbf{v})=L^i_j\mathbf{e}_i\otimes\epsilon^j(v^k\mathbf{e}_k)=L^i_j v^k \mathbf{e}_i\otimes\epsilon^j(\mathbf{e}_k)=L^i_j v^k \mathbf{e}_i \delta^j_k = L^i_j v^j \mathbf{e}_i,
$$
and the transformation of linear maps:
$$
L=L^k_l\mathbf{e}_k\otimes\epsilon^l=L^k_l(B^i_k\tilde{\mathbf{e}}_i)\otimes(F^l_j\tilde{\epsilon}^j)= (B^i_k L^k_l F^l_j) \tilde{\mathbf{e}}_i \otimes \tilde{\epsilon}^j=\tilde{L}^i_j \tilde{\mathbf{e}}_i \otimes \tilde{\epsilon}^j
$$
$$
\implies \tilde{L}^i_j = B^i_k L^k_l F^l_j.
$$&lt;/p&gt;
&lt;p&gt;Bilinear forms (including metric tensors) are tensor products of covector-covector pairs:
$$
\mathcal{B}=\mathcal{B}_{ij}\epsilon^i\otimes\epsilon^j.
$$
We can verify the definition of bilinear forms:
$$
s=\mathcal{B}(\mathbf{v},\mathbf{w})=\mathcal{B}_{ij}(\epsilon^i\otimes\epsilon^j)(\mathbf{v},\mathbf{w})=\mathcal{B}_{ij}\epsilon^i(\mathbf{v})\otimes\epsilon^j(\mathbf{w})
$$
$$
=\mathcal{B}_{ij}\epsilon^i(v^k\mathbf{e}_k)\otimes\epsilon^j(w^l\mathbf{e}_l)=\mathcal{B}_{ij}v^k w^l\epsilon^i(\mathbf{e}_k)\otimes\epsilon^j(\mathbf{e}_l)=\mathcal{B}_{ij}v^k w^l \delta^i_k \delta^j_l=\mathcal{B}_{ij}v^i w^j,
$$
and the transformation of bilinear forms:
$$
\mathcal{B}=\mathcal{B}_{kl}\epsilon^k\otimes\epsilon^l=\mathcal{B}_{kl}(F^k_i\tilde{\epsilon}^i)\otimes(F^l_j\tilde{\epsilon}^j)=F^k_i F^l_j\mathcal{B}_{kl}\tilde{\epsilon}^i\otimes\tilde{\epsilon}^j=\tilde{\mathcal{B}}_{ij}\tilde{\epsilon}^i\otimes\tilde{\epsilon}^j
$$
$$
\implies \tilde{\mathcal{B}}_{ij} = F^k_i F^l_j\mathcal{B}_{kl}.
$$&lt;/p&gt;
&lt;h3 id=&#34;general-tensor-products-and-kronecker-products&#34;&gt;General Tensor Products and Kronecker Products&lt;/h3&gt;
&lt;p&gt;A general ($m$,$n$)-tensor can be defined by combining $m$ vectors $\mathbf{e}_i,\mathbf{e}_j,\mathbf{e}_k,\cdots$ and $n$ covectors $\epsilon^r,\epsilon^s,\epsilon^t,\cdots$ using tensor products:
$$
T = T^{ijk\cdots}_{rst\cdots}(\mathbf{e}_i\otimes\mathbf{e}_j\otimes\mathbf{e}_k\otimes\cdots)\otimes (\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots).
$$
Applying changes of bases and dual bases, we obtain
$$
\begin{aligned}
T &amp;amp;= T^{ijk\cdots}_{rst\cdots}(B^a_i\tilde{\mathbf{e}}_a\otimes B^b_j\tilde{\mathbf{e}}_b\otimes B^c_k\tilde{\mathbf{e}}_c\otimes\cdots)\otimes (F^r_x\tilde{\epsilon}^x\otimes F^s_y\tilde{\epsilon}^y\otimes F^t_z \tilde{\epsilon}^z\otimes\cdots) \\
&amp;amp;= (B^a_i B^b_j B^c_k\cdots) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z \cdots) (\tilde{\mathbf{e}}_a\otimes \tilde{\mathbf{e}}_b\otimes \tilde{\mathbf{e}}_c\otimes\cdots)\otimes (\tilde{\epsilon}^x\otimes \tilde{\epsilon}^y\otimes \tilde{\epsilon}^z\otimes\cdots).
\end{aligned}
$$
But the definition of the tensor $T$ in the new bases and dual bases is
$$
T = \tilde{T}^{abc\cdots}_{xyz\cdots}(\tilde{\mathbf{e}}_a\otimes \tilde{\mathbf{e}}_b\otimes \tilde{\mathbf{e}}_c\otimes\cdots)\otimes (\tilde{\epsilon}^x\otimes \tilde{\epsilon}^y\otimes \tilde{\epsilon}^z\otimes\cdots).
$$
This gives us the general tensor transformation rule:
$$
\tilde{T}^{abc\cdots}_{xyz\cdots}=(B^a_i B^b_j B^c_k\cdots ) T^{ijk\cdots}_{rst\cdots} (F^r_x F^s_y F^t_z\cdots).
$$&lt;/p&gt;
&lt;p&gt;When we apply an ($m$,$n$)-tensor tensor $T$ to an ($n$,0)-tensor (or a rank-$n$ vector) $D$, we will obtain an ($m$,0)-tensor (or a rank-$m$ vector) $T(D)$. This is a generalization of applying a (rank-1) covector (i.e., a (1,0)-tensor) to a $rank-1$ vector (i.e., a (0,1)-tensor) resulting in a scalar (i.e., a (0,0)-tensor).&lt;/p&gt;
&lt;p&gt;Specifically, suppose that
$$
D = D^{opq\cdots}(\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots).
$$
Then, we have
$$
T(D)=T^{ijk\cdots}_{rst\cdots}(\mathbf{e}_i\otimes\mathbf{e}_j\otimes\mathbf{e}_k\otimes\cdots)\otimes (\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots)(D^{opq\cdots}\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots).
$$
Note that there are multiple ways in which we can associate each covector with a vector in $(T_{rst\cdots}\epsilon^r\otimes\epsilon^s\otimes\epsilon^t\otimes\cdots)(D^{opq\cdots}\mathbf{e}_o\otimes\mathbf{e}_p\otimes\mathbf{e}_q\otimes\cdots)$. To avoid ambiguity, we should always clearly specify such association using Einstein&amp;rsquo;s notation. That is, whether it means $T(D)=T^{ijk\cdots}_{rst\cdots} D^{rst\cdots}$ or $T(D)=T^{ijk\cdots}_{rst\cdots} D^{rts\cdots}$ or anything else.&lt;/p&gt;
&lt;p&gt;Note that a tensor is a multilinear map, which is linear when all inputs except one are held constant:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$T(x_1,\cdots,nx_i,\cdots,x_n)=nT(x_1,\cdots,x_i,\cdots,x_n),~\forall i$,&lt;/li&gt;
&lt;li&gt;$T(x_1,\cdots,x_i+y_i,\cdots,x_n)=T(x_1,\cdots,x_i,\cdots,x_n)+T(x_1,\cdots,y_i,\cdots,x_n),~\forall i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Kronecker products are tensor products expressed in the array representation of tensors. For examples, matrices are Kronecker products of (column vector, row vector) pairs, and metric tensors are Kronecker products of (row vector, row vector) pairs.&lt;/p&gt;
&lt;h3 id=&#34;tensor-product-spaces&#34;&gt;Tensor Product Spaces&lt;/h3&gt;
&lt;p&gt;Let $n\in S$ be a scalar, $\mathbf{v},\mathbf{u}\in V$ be vectors, and $\alpha,\beta\in V^*$ be covectors. Tensor products satisfy the following rules&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$n(\mathbf{v}\otimes\alpha)=(n\mathbf{v})\otimes\alpha=\mathbf{v}\otimes(n\alpha)$,&lt;/li&gt;
&lt;li&gt;$\mathbf{v}\otimes\alpha+\mathbf{v}\otimes\beta=\mathbf{v}\otimes(\alpha+\beta)$,&lt;/li&gt;
&lt;li&gt;$\mathbf{v}\otimes\alpha+\mathbf{u}\otimes\alpha=(\mathbf{v}+\mathbf{u})\otimes\alpha$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can form tensor product spaces $V\otimes V$, $V\times V^*$, $V^*\times V$, and $V^*\times V^*$ using tensor products, and we can continue forming larger tensor products such as $V\otimes V^* \otimes V \otimes \cdots$ and so forth. A tensor space associated with a tensor $T$ contains elements obtained by any number of summations with $T$ in any order.&lt;/p&gt;
&lt;p&gt;For example, consider a tensor space $V^* \otimes V \otimes V^* \otimes V^*$ associated with the tensor $T^{~j}_{i~kl}\epsilon^i\mathbf{e}_j\epsilon^k\epsilon^l$. Below are some example elements in this tensor space:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T^{~j}_{i~kl} v^i \alpha_j w^k u^l\quad V \times V^* \times V \times V \to S$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} U^{ikl}\beta_j\quad (V \otimes V \otimes V)\times V^* \to S$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} \alpha_j D^{kl}\quad V^* \times (V \otimes V) \to V^*$,&lt;/li&gt;
&lt;li&gt;$T^{~j}_{i~kl} L^i_j \quad (V \otimes V^*)\to (V^* \otimes V^*)$,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;musical-isomorphism&#34;&gt;Musical Isomorphism&lt;/h3&gt;
&lt;p&gt;We would like to find a meaningful partner covector $\nu=v_i\epsilon^i\in V^*$ for each vector $\mathbf{v}=v^i\mathbf{e}_i\in V$. It turns out that one such partnership is called musical isomorphism, which is defined using the metric tensor $g\in V^*\otimes V^*$ (i.e., dot product).&lt;/p&gt;
&lt;p&gt;We define the partner of the vector $\mathbf{v}\in V$ as the linear function $\nu:V\to S$ such that
$$
\nu=g(\mathbf{v},\cdot)=g_{ik}\epsilon^i\otimes\epsilon^k(v^j\mathbf{e}_j)=g_{ik} v^j \epsilon^i\delta^k_j=g_{ij}v^j\epsilon^i.
$$
Therefore, we lower (flat) the index of the components $v^j$ of the vector $\mathbf{v}$ by
$$
v_i=g_{ij}v^j.
$$
This gives us the definition of the partner covector $\nu$ for the vector $\mathbf{v}$:
$$
\nu=v_i\epsilon^i=g_{ij}v^j\epsilon^i.
$$&lt;/p&gt;
&lt;p&gt;Another way to think about the flat operation for lowering vector component indices is that
$$
v_i=g(\mathbf{e}_i,\mathbf{v})=g(\mathbf{e}_i,v^j\mathbf{e}_j)=v^j g(\mathbf{e}_i,\mathbf{e}_j)=g_{ij}v^j.
$$&lt;/p&gt;
&lt;p&gt;This is a meaningful partnership because it holds in any basis by definition:
$$
\tilde{v}_i=\tilde{g}_{ij}\tilde{v}^j \quad\text{and}\quad\nu=\tilde{v}_i\tilde{\epsilon}^i=\tilde{g}_{ij}\tilde{v}^j\tilde{\epsilon}^i.
$$&lt;/p&gt;
&lt;p&gt;Note that in general $v^i\not=v_i$. The only exception is in the orthonormal basis where we have
$$
v_i=g_{ij}v^j=\delta_{ij}v^j=v^i.
$$&lt;/p&gt;
&lt;p&gt;We can define the inverse of the metric tensor as $h^{ki}\in V\otimes V$ such that composing $g$ and $h$ results in the identity:
$$
h^{ij}g_{jk}=\delta^i_k.
$$
Then, we raise (sharp) the index of the components $v_j$ of the covector $\nu$ (i.e., the inverse of the partnership above) by
$$
h^{ij}v_j=h^{ij}g_{jk}v^k=\delta^i_k v^k=v^i\quad\text{and}\quad \mathbf{v}=v^i\mathbf{e}_i=h^{ij}v_j\mathbf{e}_i.
$$
Similarly, this holds in any basis by definition:
$$
\tilde{v}^i=\tilde{h}^{ij}\tilde{v}_j\quad\text{and}\quad \mathbf{v}=\tilde{v}^i\tilde{\mathbf{e}}_i=\tilde{h}^{ij}\tilde{v}_j\tilde{\mathbf{e}}_i.
$$&lt;/p&gt;
&lt;p&gt;The operations of lowering and raising indices can be applied to tensors of any rank in general. For example, consider the tensor $T=T^i_{jk}\mathbf{e}_i\otimes\epsilon^j\otimes\epsilon^k\in V\otimes V^* \otimes V^*$. We can raise the index $j$ by $T^i_{jk}h^{jl}=T^{il}_k$, which results in a new tensor $T&amp;rsquo;=T^{il}_k \mathbf{e}_i\otimes\mathbf{e}_l\otimes\epsilon^k\in V\otimes V \otimes V^*$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
