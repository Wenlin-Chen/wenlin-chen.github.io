<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wenlin Chen</title>
    <link>https://wenlin-chen.github.io/</link>
      <atom:link href="https://wenlin-chen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Wenlin Chen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2023 Wenlin Chen</copyright><lastBuildDate>Sun, 10 Dec 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Wenlin Chen</title>
      <link>https://wenlin-chen.github.io/</link>
    </image>
    
    <item>
      <title>1. Energy-based Models, Score-based Models and Diffusion Models</title>
      <link>https://wenlin-chen.github.io/post/energy-score-diffusion/</link>
      <pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/post/energy-score-diffusion/</guid>
      <description>&lt;h2 id=&#34;generative-modeling&#34;&gt;Generative Modeling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data: $x\in\mathcal{X}\subseteq\mathbb{R}^d$, potentially high dimensional, complex, structured, e.g.,
&lt;ul&gt;
&lt;li&gt;high resolution images and videos,&lt;/li&gt;
&lt;li&gt;audio waveforms,&lt;/li&gt;
&lt;li&gt;3D molecules,&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given: i.i.d. samples ${x_1,\cdots,x_N}$ from an unknown data distribution $p_D(x)$.&lt;/li&gt;
&lt;li&gt;Goal:
&lt;ul&gt;
&lt;li&gt;Draw new samples from $p_D(x)$.&lt;/li&gt;
&lt;li&gt;Evaluate the probability of a given sample $x$ under $p_D(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Approach: model $p_D(x)$ with a parametric model $p_{\theta}(x)$ (e.g., parameterized by a neural network) and optimize $\theta$ such that $p_{\theta}(x)\approx p_D(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;energy-based-models&#34;&gt;Energy-based Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Energy-based Models (EBMs) parameterize an energy function $E_{\theta}(x)\geq0$ with parameters $\theta$:
$$p_{\theta}(x)=\frac{\exp(-E_{\theta}(x))}{Z_{\theta}}.$$
&lt;ul&gt;
&lt;li&gt;the partition function/normalizing constant:
$$Z_{\theta}=\int \exp(-E_{\theta}(x)) dx,$$
&lt;ul&gt;
&lt;li&gt;analytically intractable,&lt;/li&gt;
&lt;li&gt;independent of the input $x$ but dependent on the model parameters $\theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Maximum likelihood estimiation (MLE) with samples from the data distribution $p_D(x)$:
$$\max_{\theta}~\mathbb{E}&lt;em&gt;{p_D(x)}[\log p&lt;/em&gt;{\theta}(x)]\quad\iff\quad\min_{\theta}~\text{KL}[p_D(x)||p_{\theta}(x)].$$&lt;/li&gt;
&lt;li&gt;MLE with gradient-based optimization:
$$\nabla_{\theta}\log p_{\theta}(x)=-\nabla_{\theta}E_{\theta}(x)-\nabla_{\theta}\log Z_{\theta},$$
where the gradient of the log partition is intractable and hard to estimate:
$$\begin{align}
\nabla_{\theta}\log Z_{\theta}
&amp;amp;=\frac{1}{Z_{\theta}}\nabla_{\theta}\int \exp(-E_{\theta}(x)) dx \\
&amp;amp;=\frac{1}{Z_{\theta}}\int \nabla_{\theta} \exp(-E_{\theta}(x)) dx \\
&amp;amp;=\int-\nabla_{\theta}E_{\theta}(x)\frac{\exp(-E_{\theta}(x))}{Z_{\theta}}dx \\
&amp;amp;=\mathbb{E}&lt;em&gt;{p&lt;/em&gt;{\theta}(x)}[-\nabla_{\theta}E_{\theta}(x))].
\end{align}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;score-based-models&#34;&gt;Score-based Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Score function:
$$s_{\theta}(x)=\nabla_x\log p_{\theta}(x)=-\nabla_x E_{\theta}(x),$$
&lt;ul&gt;
&lt;li&gt;independent of $Z_{\theta}$ thus tractable,&lt;/li&gt;
&lt;li&gt;not to be confused with $\nabla_{\theta}\log p_{\theta}(x)$ which depends on $\theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Score-based models (SBMs) directly parameterize the score function $s_{\theta}(x)\in\mathbb{R}^d$.
&lt;ul&gt;
&lt;li&gt;$s_{\theta}(x)$ may not be a conservative vector field (i.e., the gradient of something), but usually not a problem in practice.&lt;/li&gt;
&lt;li&gt;More efficient than EBM which needs to differentiate the energy to get the score.&lt;/li&gt;
&lt;li&gt;U-Net is a common model choice for image generation problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Langevin Dynamics MCMC using score functions:
$$x^{(k+1)}=x^{(k)}+\frac{\epsilon^2}{2}\nabla_x\log p_{\theta}(x)+\epsilon z^{(k)},\quad k=0,1,\cdots,K,$$
or, equivalently,
$$x^{(k+1)}=x^{(k)}+\epsilon\nabla_x\log p_{\theta}(x)+\sqrt{2\epsilon} z^{(k)},\quad k=0,1,\cdots,K,$$
where
&lt;ul&gt;
&lt;li&gt;$\nabla_x\log p_{\theta}(x)=-\nabla_x E_{\theta}(x)$ for EBMs and $\nabla_x\log p_{\theta}(x)=s_{\theta}(x)$ for SBMs.&lt;/li&gt;
&lt;li&gt;$x^{(0)}$ starts from some simple prior distribution (e.g., standard Gaussian),&lt;/li&gt;
&lt;li&gt;$z^{(k)}\sim\mathcal{N}(0,I)$ are independent Gaussian noises,&lt;/li&gt;
&lt;li&gt;$x^{(k)}\sim p_{\theta}(x)$ as $k\to\infty$ and $\epsilon\to 0$ under regularity conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If $p_{\theta}(x)\approx p_D(x)$ after learning, then this allows us to draw samples from $p_D(x)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;score-matching&#34;&gt;Score Matching&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Score matching (SM) by minimizing the Fisher divergence:
$$\min_{\theta}~D_F[p_D(x)||p_{\theta}(x)]=\mathbb{E}&lt;em&gt;{p_D(x)}\left[\frac{1}{2} \Vert \nabla_x \log p_D(x) - s&lt;/em&gt;{\theta}(x)\Vert^2\right],$$
&lt;ul&gt;
&lt;li&gt;intractable due to the unknown the data score $\nabla_x \log p_D(x)$,&lt;/li&gt;
&lt;li&gt;only works for non-degenerate (i.e., full support, $\mathcal{X}=\mathbb{R}^d$) $p_D(x)$:
$$s_{\theta}(x)=\nabla_x \log p_D(x),~\forall x\quad\iff\quad p_{\theta}=p_{D}.$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A tractable score matching objective:
$$D_F[p_D(x)||p_{\theta}(x)]=\mathbb{E}&lt;em&gt;{p_D(x)}\left[\frac{1}{2}\Vert s&lt;/em&gt;{\theta}(x)\Vert^2 + \text{Tr}(J_x s_{\theta}(x))\right]+\text{const.}$$
&lt;ul&gt;
&lt;li&gt;Derived by integration by parts under regularity conditions.&lt;/li&gt;
&lt;li&gt;No unknown data score term.&lt;/li&gt;
&lt;li&gt;The trace of the Jacobian of the score function $\text{Tr}(J_x s_{\theta}(x))$ takes $\mathcal{O}(d^2)$ time to compute, which is still computationally expensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Denoising Score Matching (DSM)
&lt;ul&gt;
&lt;li&gt;Perturbe $p_D(x)$ with a small noise:
$$\tilde{x}=x+\sigma z,\quad z\sim\mathcal{N}(0,I),$$
or, equivalently,
$$p_{\sigma}(\tilde{x})=\int q_{\sigma}(\tilde{x}|x)p_D(x)dx,\quad\text{where } q_{\sigma}(\tilde{x}|x)=\mathcal{N}(\tilde{x}|x,\sigma^2I).$$&lt;/li&gt;
&lt;li&gt;Match the score of the perturbed data distribution $p_{\sigma}(\tilde{x})$:
$$
\begin{align}
\min_{\theta}~D_F[p_{\sigma}(\tilde{x})||p_{\theta}(\tilde{x})]
&amp;amp;= \mathbb{E}&lt;em&gt;{p&lt;/em&gt;{\sigma}(\tilde{x})}\left[\frac{1}{2} \Vert \nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x}) - s_{\theta}(\tilde{x})\Vert^2\right] \\
&amp;amp;= \mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\sigma}(\tilde{x}|x)p_D(x)}\left[\frac{1}{2} \Vert \nabla_{\tilde{x}} \log q_{\sigma}(\tilde{x}|x) - s_{\theta}(\tilde{x})\Vert^2\right] \\
&amp;amp;=\mathbb{E}&lt;em&gt;{\mathcal{N}(z|0,I)p_D(x)}\left[\frac{1}{2} \left\Vert \frac{z}{\sigma} + s&lt;/em&gt;{\theta}(x+\sigma z)\right\Vert^2\right].
\end{align}
$$
&lt;ul&gt;
&lt;li&gt;No unknown data score term.&lt;/li&gt;
&lt;li&gt;$p_{\sigma}(\tilde{x})$ is always non-degenerate regardless of the support of $p_D(x)$, since it is obtained by convolving $p_D(x)$ with (full support) Gaussian noise $\mathcal{N}(0,\sigma^2I)$.&lt;/li&gt;
&lt;li&gt;$\sigma$ needs to be close to zero:
$$s_{\theta}(\tilde{x})=\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x}),~\forall \tilde{x}\quad\iff\quad p_{\theta}=p_{\sigma}\approx p_D.$$&lt;/li&gt;
&lt;li&gt;$\sigma$ cannot be too small for the purpose of numerical stability (e.g., the DSM objective has a a term $z/\sigma$).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pitfalls of DSM:
&lt;ul&gt;
&lt;li&gt;$s_{\theta}(x)$ is inaccurate in low density regions of $p_D(x)$ since very few or no samples are available for the model to train on.&lt;/li&gt;
&lt;li&gt;When drawing new samples via Langevin dynamics, the initial point $x^{(0)}$ is very likely to be in a low density region in high dimensions.&lt;/li&gt;
&lt;li&gt;Prevents us from moving to high density regions (i.e., generating high quality samples) that are representative of $p_D(x)$.&lt;/li&gt;
&lt;li&gt;Need to use large $\sigma$ to populate low density regions, but there is a trade-off:
&lt;ul&gt;
&lt;li&gt;$\sigma$ should be sufficiently large so that $p_{\sigma}(\tilde{x})$ covers low density regions.&lt;/li&gt;
&lt;li&gt;Remember that $\sigma$ should also be sufficiently small so that $p_{\sigma}\approx p_D$.&lt;/li&gt;
&lt;li&gt;Hard to balance the trade-off in practice&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DSM with Annealed Langevin Dynamics
&lt;ul&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;denoising-diffusion-probabilistic-models&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;score-based-diffusion-models&#34;&gt;Score-based Diffusion Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</title>
      <link>https://wenlin-chen.github.io/publication/wu2023has/</link>
      <pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/wu2023has/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging Task Structures for Improved Identifiability in Neural Network Representations</title>
      <link>https://wenlin-chen.github.io/publication/chen2023leveraging/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2023leveraging/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning</title>
      <link>https://wenlin-chen.github.io/publication/chen2023neural/</link>
      <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2023neural/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction</title>
      <link>https://wenlin-chen.github.io/publication/chen2023meta/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2023meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimal Client Sampling for Federated Learning</title>
      <link>https://wenlin-chen.github.io/publication/chen2022optimal/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2022optimal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Evaluation Framework for the Objective Functions of De Novo Drug Design Benchmarks</title>
      <link>https://wenlin-chen.github.io/publication/tripp2022evaluation/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/tripp2022evaluation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Causal Representation Learning for Latent Space Optimization</title>
      <link>https://wenlin-chen.github.io/publication/chen2021causal/</link>
      <pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/chen2021causal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>To Ensemble or Not Ensemble: When Does End-to-End Training Fail?</title>
      <link>https://wenlin-chen.github.io/publication/webb2020ensemble/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/publication/webb2020ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenlin-chen.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wenlin-chen.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
