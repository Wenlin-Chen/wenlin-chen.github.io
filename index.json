[{"authors":null,"categories":null,"content":"I’m a Research Scientist at Bosch AI, based at the Tsinghua-Bosch Joint Research Center on Machine Learning and in collaboration with Prof. Jun Zhu\u0026rsquo;s Research Group at Tsinghua University. I\u0026rsquo;m generally keen on core machine learning research and its applications in the physical world. My current research centers around diffusion models and multimodal foundation models for video generation and world modeling, with application to autonomous driving and embodied AI.\nI obtained my PhD degree in Machine Learning from University of Cambridge and Max Planck Institute for Intelligent Systems under the Cambridge-Tübingen PhD Fellowship, advised by Prof. José Miguel Hernández-Lobato and Prof. Bernhard Schölkopf. During my PhD studies, I interned at Microsoft Research. My PhD research focused on probabilistic machine learning and its scientific applications, with a particular interst in the synergy between deep learning and probabilistic inference. I developed novel meta-learning, generative modeling, and enhanced sampling methods for a range of and molecular modeling tasks. Additionally, I investigated neural network training dynamics and developed efficient neural network optimization algorithms in my previous research.\n","date":1753142400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1753142400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m a Research Scientist at Bosch AI, based at the Tsinghua-Bosch Joint Research Center on Machine Learning and in collaboration with Prof. Jun Zhu\u0026rsquo;s Research Group at Tsinghua University. I\u0026rsquo;m generally keen on core machine learning research and its applications in the physical world.","tags":null,"title":"Wenlin Chen","type":"authors"},{"authors":["Wenlin Chen"],"categories":null,"content":"","date":1753142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753142400,"objectID":"f88cab38c73c23fc5239d18d0a7fd61f","permalink":"https://wenlin-chen.github.io/publication/chen2025bridging/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/publication/chen2025bridging/","section":"publication","summary":"While deep learning has achieved remarkable performance in modelling complex patterns in structured data, a key challenge is its reliance on large datasets. In contrast, probabilistic inference excels in data-scarce settings but suffers from computational inefficiencies for high dimensional data and struggles to model structured data where representation learning is crucial. This thesis focuses on the synergies between deep learning and probabilistic inference, bridging these gaps from two complementary perspectives, which results in novel machine learning methods with improved data efficiency, identifiability, and sampling scalability. In the first part of this thesis, we investigate how probabilistic inference can enhance deep learning. We first introduce a data-efficient meta-learning framework, which combines Gaussian processes and deep neural networks to improve representation learning on related low-data tasks. By formulating this problem in a novel bilevel optimisation framework and solving it with the implicit function theorem, this approach enhances the generalisation capabilities of deep neural networks for few-shot molecular property prediction and optimisation tasks. Next, we analyse the theoretical properties of neural network representations learned across multiple tasks within a probabilistic framework, establishing conditions under which neural networks can recover canonical feature representations that reflect the underlying ground-truth data generating process. Our framework not only ensures linear identifiability in the general multi-task regression setting, but also offers a simple probabilistic inference approach to recovering point-wise identifiable feature representations under certain assumptions of task structures, resulting in stronger theoretical guarantees and empirical performance of identifiability than previous methods on real-world molecular data. In the second part of this thesis, we explore the other direction in their reciprocal relationship: utilising deep learning to improve probabilistic inference. Inspired by diffusion-based modelling techniques, we propose a novel approach for training deep generative models to emulate sampling-based probabilistic inference for unnormalised probability distributions. This enables efficient sampling from multi-modal probability distributions such as Boltzmann distributions for many-body particle systems. Our approach outperforms previous neural samplers while achieving faster training and inference speed. Together, this thesis demonstrates how deep learning and probabilistic inference can be integrated in a mutually reinforcing manner to enhance each other.","tags":["Multi-Task","Diffusion","AI for Science"],"title":"Bridging Deep Learning and Probabilistic Inference: Towards Data Efficiency, Identifiability, and Sampling Scalability","type":"publication"},{"authors":["Severi Rissanen","RuiKang OuYang","Jiajun He","Wenlin Chen","Markus Heinonen","Arno Solin","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1752364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752364800,"objectID":"4f0d7bdc3442566089e121a0bfaef6c3","permalink":"https://wenlin-chen.github.io/publication/rissanen2025progressive/","publishdate":"2025-02-10T00:00:00Z","relpermalink":"/publication/rissanen2025progressive/","section":"publication","summary":"Recent research has focused on designing neural samplers that amortize the process of sampling from unnormalized densities. However, despite significant advancements, they still fall short of the state-of-the-art MCMC approach, Parallel Tempering (PT), when it comes to the efficiency of target evaluations. On the other hand, unlike a well-trained neural sampler, PT yields only dependent samples and needs to be rerun -- at considerable computational cost -- whenever new samples are required. To address these weaknesses, we propose the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion models sequentially across temperatures, leveraging the advantages of PT to improve the training of neural samplers. We also introduce a novel method to combine high-temperature diffusion models to generate approximate lower-temperature samples, which are minimally refined using MCMC and used to train the next diffusion model. PTSD enables efficient reuse of sample information across temperature levels while generating well-mixed, uncorrelated samples. Our method significantly improves target evaluation efficiency, outperforming diffusion-based neural samplers.","tags":["Sampling","Diffusion","AI for Science"],"title":"Progressive Tempering Sampler with Diffusion","type":"publication"},{"authors":["Jiajun He","Wenlin Chen","Mingtian Zhang","David Barber","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1746403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746403200,"objectID":"1bd165406a37c487594886ba7e689661","permalink":"https://wenlin-chen.github.io/publication/he2025training/","publishdate":"2023-10-16T00:00:00Z","relpermalink":"/publication/he2025training/","section":"publication","summary":"Training generative models to sample from unnormalized density functions is an important and challenging task in machine learning. Traditional training methods often rely on the reverse Kullback-Leibler (KL) divergence due to its tractability. However, the mode-seeking behavior of reverse KL hinders effective approximation of multi-modal target distributions. To address this, we propose to minimize the reverse KL along diffusion trajectories of both model and target densities. We refer to this objective as the reverse diffusive KL divergence, which allows the model to capture multiple modes. Leveraging this objective, we train neural samplers that can efficiently generate samples from the target distribution in one step. We demonstrate that our method enhances sampling performance across various Boltzmann distributions, including both synthetic multi-modal densities and n-body particle systems.","tags":["Sampling","Diffusion","AI for Science"],"title":"Training Neural Samplers with Reverse Diffusive KL Divergence","type":"publication"},{"authors":["Mingtian Zhang","Wenlin Chen","Jiajun He","Zijing Ou","José Miguel Hernández-Lobato","Bernhard Schölkopf","David Barber"],"categories":null,"content":"","date":1745625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745625600,"objectID":"bb27d537f9d1a81658ac293bc0e0cf2c","permalink":"https://wenlin-chen.github.io/publication/zhang2025towards/","publishdate":"2025-02-10T00:00:00Z","relpermalink":"/publication/zhang2025towards/","section":"publication","summary":"Recent advances in training one-step diffusion models typically follow a two-stage pipeline: first training a teacher diffusion model and then distilling it into a one-step student model. This process often depends on both the teacher's score function for supervision and its weights for initializing the student model. In this paper, we explore whether one-step diffusion models can be trained directly without this distillation procedure. We introduce a family of new training methods that entirely forgo teacher score supervision, yet outperforms most teacher-guided distillation approaches. This suggests that score supervision is not essential for effective training of one-step diffusion models. However, we find that initializing the student model with the teacher's weights remains critical. Surprisingly, the key advantage of teacher initialization is not due to better latent-to-output mappings, but rather the rich set of feature representations across different noise levels that the teacher diffusion model provides. These insights take us one step closer towards training one-step diffusion models without distillation and provide a better understanding of the roles of teacher supervision and initialization in the distillation process.","tags":["Diffusion"],"title":"Towards Training One-Step Diffusion Models Without Distillation","type":"publication"},{"authors":["Wenlong Chen","Wenlin Chen","Lapo Rastrelli","Yingzhen Li"],"categories":null,"content":"","date":1745625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745625600,"objectID":"17d4792e890e5540ba58343391438c28","permalink":"https://wenlin-chen.github.io/publication/chen2025your/","publishdate":"2023-10-26T00:00:00Z","relpermalink":"/publication/chen2025your/","section":"publication","summary":"Diffusion models, which can be viewed as a special case of hierarchical variational autoencoders (HVAEs), have shown profound success in generating photo-realistic images. In contrast, standard HVAEs often produce images of inferior quality compared to diffusion models. In this paper, we hypothesize that the success of diffusion models can be partly attributed to the additional self-supervision information for their intermediate latent states provided by corrupted images, which along with the original image form a pseudo video. Based on this hypothesis, we explore the possibility of improving other types of generative models with such pseudo videos. Specifically, we first extend a given image generative model to their video generative model counterpart, and then train the video generative model on pseudo videos constructed by applying data augmentation to the original images. Furthermore, we analyze the potential issues of first-order Markov data augmentation methods, which are typically used in diffusion models, and propose to use more expressive data augmentation to construct more useful information in pseudo videos. Our empirical results on the CIFAR10 and CelebA datasets demonstrate that improved image generation quality can be achieved with additional self-supervised information from pseudo videos.","tags":["Diffusion"],"title":"Your Image is Secretly the Last Frame of a Pseudo Video","type":"publication"},{"authors":["Wenlin Chen","Hong Ge"],"categories":null,"content":"","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"8f81276bd0520086c1943085817e4c7a","permalink":"https://wenlin-chen.github.io/publication/chen2024neural/","publishdate":"2023-05-25T00:00:00Z","relpermalink":"/publication/chen2024neural/","section":"publication","summary":"We introduce a novel approach for analyzing the training dynamics of ReLU networks by examining the characteristic activation boundaries of individual ReLU neurons. Our proposed analysis reveals a critical instability in common neural network parameterizations and normalizations during stochastic optimization, which impedes fast convergence and hurts generalization performance. Addressing this, we propose Geometric Parameterization (GmP), a novel neural network parameterization technique that effectively separates the radial and angular components of weights in the hyperspherical coordinate system. We show theoretically that GmP resolves the aforementioned instability issue. We report empirical results on various models and benchmarks to verify GmP's advantages of optimization stability, convergence speed and generalization performance.","tags":["Optimization"],"title":"Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks","type":"publication"},{"authors":["Wenlin Chen","Julien Horwood","Juyeon Heo","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1724457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724457600,"objectID":"51bd119fc40a9367b1d4735678de0282","permalink":"https://wenlin-chen.github.io/publication/chen2024leveraging/","publishdate":"2023-06-27T00:00:00Z","relpermalink":"/publication/chen2024leveraging/","section":"publication","summary":"This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that linear identifiability is achievable in the general multi-task regression setting. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent factors reduces the equivalence class for identifiability to permutations and scaling of the true latent factors, a stronger and more useful result than linear identifiability. Crucially, when we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization, and suggests potential downstream applications to causal representation learning. Empirically, we find that this straightforward optimization procedure enables our model to outperform more general unsupervised models in recovering canonical representations for both synthetic data and real-world molecular data.","tags":["Multi-Task","AI for Science"],"title":"Leveraging Task Structures for Improved Identifiability in Neural Network Representations","type":"publication"},{"authors":["Wen Wu","Wenlin Chen","Chao Zhang","Phil Woodland"],"categories":null,"content":"","date":1723334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723334400,"objectID":"3d127668f789f626cd8694802abe15f4","permalink":"https://wenlin-chen.github.io/publication/wu2024modelling/","publishdate":"2023-09-30T00:00:00Z","relpermalink":"/publication/wu2024modelling/","section":"publication","summary":"Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation tasks such as data annotation and system assessment. It is important to incorporate the variability present in human evaluation into HAS, since it helps capture diverse subjective interpretations and mitigate potential biases and over-representation. This work introduces a novel framework for modelling variability in HAS. Conditional softmax flow (S-CNF) is proposed to model the distribution of subjective human annotations, which leverages diverse human annotations via meta-learning. This enables efficient generation of annotations that exhibit human variability for unlabelled input. In addition, a wide range of evaluation metrics are adopted to assess the capability and efficiency of HAS systems in predicting the aggregated behaviours of human annotators, matching the distribution of human annotations, and simulating the inter-annotator disagreements. Results demonstrate that the proposed method achieves state-of-the-art performance on two real-world human evaluation tasks: emotion recognition and toxic speech detection.","tags":["Uncertainty"],"title":"Modelling Variability in Human Annotator Simulation","type":"publication"},{"authors":["Wenlin Chen","Mingtian Zhang","Brooks Paige","José Miguel Hernández-Lobato","David Barber"],"categories":null,"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721520000,"objectID":"ce0f1bccc5bc8fef905bf5da977085b4","permalink":"https://wenlin-chen.github.io/publication/chen2024diffusive/","publishdate":"2024-02-04T00:00:00Z","relpermalink":"/publication/chen2024diffusive/","section":"publication","summary":"The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics. ","tags":["Sampling","Diffusion","AI for Science"],"title":"Diffusive Gibbs Sampling","type":"publication"},{"authors":["Wen Wu","Wenlin Chen","Chao Zhang","Philip C. Woodland"],"categories":null,"content":"","date":1696032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696032000,"objectID":"a813e18c87043788c6c22b08f0d565dd","permalink":"https://wenlin-chen.github.io/publication/wu2023has/","publishdate":"2023-09-30T00:00:00Z","relpermalink":"/publication/wu2023has/","section":"publication","summary":"Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation such as data annotation and system assessment. Human perception and behaviour during human evaluation exhibit inherent variability due to diverse cognitive processes and subjective interpretations, which should be taken into account in modelling to better mimic the way people perceive and interact with the world. This paper introduces a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which incorporates human variability and allows for the efficient generation of human-like annotations for unlabelled test inputs. Under this framework, we propose two new model classes, conditional integer flows and conditional softmax flows, to account for ordinal and categorical annotations, respectively. The proposed method is evaluated on three real-world human evaluation tasks and shows superior capability and efficiency to predict the aggregated behaviours of human annotators, match the distribution of human annotations, and simulate the inter-annotator disagreements.","tags":["Uncertainty"],"title":"It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation","type":"publication"},{"authors":["Wenlin Chen","Austin Tripp","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"e7e2dce54685980967029d324635a717","permalink":"https://wenlin-chen.github.io/publication/chen2023meta/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/publication/chen2023meta/","section":"publication","summary":"We propose Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT), a novel framework for learning deep kernel Gaussian processes (GPs) by interpolating between meta-learning and conventional deep kernel learning. Our approach employs a bilevel optimization objective where we meta-learn generally useful feature representations across tasks, in the sense that task-specific GP models estimated on top of such features achieve the lowest possible predictive loss on average. We solve the resulting nested optimization problem using the implicit function theorem (IFT). We show that our ADKF-IFT framework contains previously proposed Deep Kernel Learning (DKL) and Deep Kernel Transfer (DKT) as special cases. Although ADKF-IFT is a completely general method, we argue that it is especially well-suited for drug discovery problems and demonstrate that it significantly outperforms previous state-of-the-art methods on a variety of real-world few-shot molecular property prediction tasks and out-of-domain molecular property prediction and optimization tasks.","tags":["Multi-Task","Uncertainty","AI for Science"],"title":"Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction","type":"publication"},{"authors":["Wenlin Chen","Samuel Horváth","Peter Richtárik"],"categories":null,"content":"","date":1661126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661126400,"objectID":"eee04c746fd5fbd08cbed0a0667cb0ed","permalink":"https://wenlin-chen.github.io/publication/chen2022optimal/","publishdate":"2022-08-04T00:00:00Z","relpermalink":"/publication/chen2022optimal/","section":"publication","summary":"It is well understood that client-master communication can be a primary bottleneck in federated learning (FL). In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with \"important\" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which allows for secure aggregation and stateless clients, and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.","tags":["Optimization"],"title":"Optimal Client Sampling for Federated Learning","type":"publication"},{"authors":["Austin Tripp","Wenlin Chen","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1651190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651190400,"objectID":"7c674ec1dda2935eabce28cc88b5af57","permalink":"https://wenlin-chen.github.io/publication/tripp2022evaluation/","publishdate":"2022-04-29T00:00:00Z","relpermalink":"/publication/tripp2022evaluation/","section":"publication","summary":"De novo drug design has recently received increasing attention from the machine learning community. It is important that the field is aware of the actual goals and challenges of drug design and the roles that de novo molecule design algorithms could play in accelerating the process, so that algorithms can be evaluated in a way that reflects how they would be applied in real drug design scenarios. In this paper, we propose a framework for critically assessing the merits of benchmarks, and argue that most of the existing de novo drug design benchmark functions are either highly unrealistic or depend upon a surrogate model whose performance is not well characterized. In order for the field to achieve its long-term goals, we recommend that poor benchmarks (especially logP and QED) be deprecated in favour of better benchmarks. We hope that our proposed framework can play a part in developing new de novo drug design benchmarks that are more realistic and ideally incorporate the intrinsic goals of drug design.","tags":["AI for Science"],"title":"An Evaluation Framework for the Objective Functions of De Novo Drug Design Benchmarks","type":"publication"},{"authors":["Wenlin Chen"],"categories":null,"content":"","date":1629331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629331200,"objectID":"12bedca3d469d735925baf247e9638c5","permalink":"https://wenlin-chen.github.io/publication/chen2021causal/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/publication/chen2021causal/","section":"publication","summary":"In this thesis, we study causal representation learning for latent space optimization, which allows for robust and efficient generation of novel synthetic data with maximal target value.  We assume that the observed data was generated by a few latent factors, some of which are causally related to the target and others of which are spuriously correlated with the target and confounded by an environment variable. Our proposed method consists of three steps, which exploits the structure of the causal graph that describes the assumed underlying data generating process. In the first step, we recover the true data representation (i.e., the latent factors from which the observed data originated). We obtain novel identifiability theory, showing that the true data representation can be recovered up to simple transformations by a generalized version of identifiable variational auto-encoders. In the second step, we identify the causal latent factors of the target, for which we propose a practical causal inference scheme that employs (conditional) independence tests and causal discovery algorithms. Our method does not require having access to the true environment variable, which overcomes a major limitation of existing causal representation learning approaches in the literature. In the final step, we query latent points that correspond to data points with high target values by intervening upon the causal latent factors using standard latent space optimization techniques. We empirically evaluate and thoroughly analyze our method on three different tasks, including a chemical design task. We show that our method can successfully recover the true data representation in the finite data regime and correctly identify the causal latent factors of the target, which results in state-of-the-art performance for black-box optimization.","tags":["Causality","AI for Science"],"title":"Causal Representation Learning for Latent Space Optimization","type":"publication"},{"authors":["Andrew Webb","Charles Reynolds","Wenlin Chen","Henry Reeve","Dan Iliescu","Mikel Luján","Gavin Brown"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"73540bab56f2b0d108a710976f7b0dcb","permalink":"https://wenlin-chen.github.io/publication/webb2020ensemble/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/publication/webb2020ensemble/","section":"publication","summary":"End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue—are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where overparameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.","tags":["Optimization"],"title":"To Ensemble or Not Ensemble: When Does End-to-End Training Fail?","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://wenlin-chen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]