<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Wenlin Chen" />

  
  
  
    
  
  <meta name="description" content="1. What is Statistical Mechanics? Statistical mechanics is not a natural law like Newton&rsquo;s laws of motion which were discovered from experimental observations and are assumed to (approximately) reflect fundamental aspects of reality." />

  
  <link rel="alternate" hreflang="en-us" href="https://wenlin-chen.github.io/post/statistical_mechanics/" />

  









  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.807c460ea42d089b71c3cb26b919dfa0.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://wenlin-chen.github.io/post/statistical_mechanics/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Wenlin Chen" />
  <meta property="og:url" content="https://wenlin-chen.github.io/post/statistical_mechanics/" />
  <meta property="og:title" content="Statistical Mechanics | Wenlin Chen" />
  <meta property="og:description" content="1. What is Statistical Mechanics? Statistical mechanics is not a natural law like Newton&rsquo;s laws of motion which were discovered from experimental observations and are assumed to (approximately) reflect fundamental aspects of reality." /><meta property="og:image" content="https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2024-07-28T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-07-28T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wenlin-chen.github.io/post/statistical_mechanics/"
  },
  "headline": "Statistical Mechanics",
  
  "datePublished": "2024-07-28T00:00:00Z",
  "dateModified": "2024-07-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Wenlin Chen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Wenlin Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "1. What is Statistical Mechanics? Statistical mechanics is not a natural law like Newton\u0026rsquo;s laws of motion which were discovered from experimental observations and are assumed to (approximately) reflect fundamental aspects of reality."
}
</script>

  

  

  

  





  <title>Statistical Mechanics | Wenlin Chen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="90eb613fa6e493cc3f00ccc44d2cc925" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Wenlin Chen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Wenlin Chen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  













  

  
  
  
<div class="container pt-3">
  <h1>Statistical Mechanics</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jul 28, 2024
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    18 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

    





  
</div>


  <div class="container">

    <div class="article-style">
      <h3 id="1-what-is-statistical-mechanics">1. What is Statistical Mechanics?</h3>
<p>Statistical mechanics is not a natural law like Newton&rsquo;s laws of motion which were discovered from experimental observations and are assumed to (approximately) reflect fundamental aspects of reality. Instead, it is a set of mathematical techniques that can be applied to study any physical system. Beyond physics, many ideas in machine learning were also inspired by statistical mechanics.</p>
<p>Statistical mechanics is particularly applicable in the following situations.</p>
<ul>
<li>Ideal predictability is impossible:
<ul>
<li>Initial conditions or laws of motion are not known with infinite precision.</li>
<li>Systems are interacting with outside environments.</li>
</ul>
</li>
<li>Ideal predictability is not useful:
<ul>
<li>It might not be useful to know microscopic variables such as the position and momentum of every particle in the room.</li>
<li>Calculating the macroscopic variables such as the total energy is more useful.</li>
</ul>
</li>
<li>Large numbers are involved:
<ul>
<li>Statistical average becomes precise for large numbers.</li>
<li>Statistical mechanics can be highly precise for macroscopic variables (except for rare events at the tails of probability distributions).</li>
</ul>
</li>
</ul>
<h3 id="2-statistical-description-of-physical-systems">2. Statistical Description of Physical Systems</h3>
<h4 id="21-microstates-and-macrostates">2.1 Microstates and Macrostates</h4>
<p>A microstate defines the values of all possible microscopic variables, such as position and momentum of every particle in a classical system of point particles.</p>
<!-- We make a fundamental assumption that any physical system can exist in a discrete set of microstates (possibly infinite). Note that continious variables such as position and momentum can be discretized. -->
<p>A macrostate defines the values of all possible macroscopic variables, such as the total energy, volumn, pressure, and temperature of a system, and the total number of particles in a system. We are usually more interested in macrostates since the number of microstates is typically huge, which is difficult to measure and in far more details than we usually care about.</p>
<p>Statistical mechanics aims to create a bridge between the microscopic and macroscopic descriptions of a physical system.</p>
<p>For quantum systems, a microstate is a value $\left.|\Phi\right&gt;$ of the wavefunction. The microscopic variables are the probability amplitudes of the basis states. A macroscopic variables is defined as the expectation value of an operator. For an operator (e.g., Hamiltonian $H$) corresponding to a measurable quantity (e.g., energy $E$), the corresponding macroscopic variable is
$$
E\equiv \left&lt;\Phi|H|\Phi\right&gt;.
$$
Conservation of energy $E$ still holds for isolated quantum systems.</p>
<h4 id="22-density-of-states">2.2 Density of States</h4>
<p>There can be a huge number of microstates all corresponding to the same macrostate (e.g., there is an enormous number of configurations of individual particles which all add up to the same energy and volumn). The number of microstates corresponding to a macrostate is defined as the density of the (macro)state:
$$
\Omega(E,V,\cdots).
$$
$\Omega$ is a function of all macroscopic variables defining the macrostate and can be thought of as the number of microstates per unit volumn in the space defined by those macroscopic variables.</p>
<p>As an example, the energy of a classical system of $N$ free particles is given by
$$E=\frac{|\mathbf{p}|^2}{2m},$$
where $\mathbf{p}$ contains $3N$ momentum variables. Note that each value of $E$ corresponds to the surface of a $(3N-1)$-dimensional hypersphere in this $3N$-dimensional space of momentum. Therefore, the density of the state is proportional to the surface area of the $(3N-1)$-dimensional hypersphere:
$$
\Omega(E)\propto |\mathbf{p}|^{3N-1}\propto E^{(3N-1)/2}.
$$
Note that for a large system with many particles ($N&gt; &gt;1$), even a tiny increase of the energy $E$ will result in a huge increase in the density of states. Therefore, we will work with a smoother and slower changing function $\log\Omega$.</p>
<h4 id="23-time-averages-and-ensemble-averages">2.3 Time Averages and Ensemble Averages</h4>
<p>We usually have no idea which microstate a system is in even if we know its current macrostate. However, this can be described by probabilities.</p>
<p>For a system in a given macrostate, there are two approaches to defining the probability of the system being in a microstate.</p>
<ol>
<li>The degrees of freedom making up the system are constantly changing. Conduct an experiment or simulation over a sufficiently long time interval. Define the probability of the system being in a microstate as the fraction of time spending in that microstate. Averages computed using this definition of probability are called time averages.</li>
<li>Imagine preparing many identical systems following exactly the same procedure. The set of all these system are called a statistical ensemble, which are in the same macrostate but can be in different microstates. Define the probability of the system being in a microstate as the fraction of systems in the ensemble which are in that microstate. Averages computed using this definition of probability are called ensemble averages.</li>
</ol>
<p>For quantum systems, we need to be careful about different types of probabilities. In statistical mechanics, probability always refers to either time average (a fraction of time) or ensemble average (a fraction of ensemble members). In quantum mechanics, even for a given state, there is a probability that a measurement will give a certain result. This probability only comes up when you perform a measurement (i.e., interact with an external system). As long as quantum systems iare isolated, they are always in well-defined microstates and macrostates at any time.</p>
<h4 id="24-ergodicity-and-equilibrium">2.4 Ergodicity and Equilibrium</h4>
<p>A system for which time averages and ensemble averages are equal is said to be ergodic. Ergodicity is a property of the system that it will eventually visit all parts of the space as time evloves and thus cannot be reduced or factored into smaller components.</p>
<p>The postulate of equal a priori probabilities states that a system has an equal probability of being in any microstate that is consistent with its macrostate. An isolated system that satisfies the postulate of equal a priori probabilities is said to be in equilibrium.</p>
<p>It can be shown that once an isolated system satisfies the postulate of equal a priori probabilities even for a moment, it will continue satisfying it forever. Suppose that there are $\Omega(E,\cdots)$ microstates consistent with a macrostate. Each of these microstates is associated with a probability $p_i$. The entropy of a probability distribution is defined as
$$
S(E,\cdots)=-k\sum_i p_i\log p_i,
$$
which measures our ignorance (lack of knowledge) of the system. If we maximize the entropy subject to the normalization constraint using Lagrange multiplier:
$$
\max_{p}~L=S/k+\alpha \left(1-\sum_i p_i\right),
$$
we will obtain the uniform distribution:
$$
\frac{\partial L}{\partial p_i}=-\log p_i - 1 -\alpha=0\quad\implies\quad p_i=e^{-(\alpha+1)}=\frac{1}{\Omega(E,\cdots)}.
$$
Since the second law of thermodynamics states that the entropy of a system never decreases, this completes the proof and further implies that an isolated system is in equilibrium if the probability distribution over its microstates does not change over time.</p>
<p>The above calculation also reveals that the entropy of a macrostate of a system in equilibrium is given by
$$
S(E,\cdots)=k\log \Omega(E,\cdots).
$$</p>
<h4 id="25-the-maxwell-boltzmann-distribution">2.5 The Maxwell-Boltzmann Distribution</h4>
<p>Real systems are usually not isolated. To analyze non-isolated systems, we consider an isolated system which is splitted into two parts A and B. A is the part we care about, on which we do experiments. B is called a heat bath, which is the environment that A is connected to. We only care about B to the extent that it affectes A. Some examples are</p>
<ul>
<li>A is the gas contained in a box. B is the box along with the whole room where the box is placed.</li>
<li>A is a test tube with chemicals in it. B is a water bath the test tube is sitting in.</li>
<li>A is the carbon dioxide molecules in the air in a room. B is everything else in the air in that room.</li>
</ul>
<p>The total energy $E_T$ of the whole system is a sum of three terms:
$$E_T=E_A+E_B+E_{AB},$$
where $E_A$ depends only on the degrees of freedom that make up A, $E_B$ only depends on the degrees of freedom that make up B, and $E_{AB}$ depends on both parts of the system due to the interaction of A and B.</p>
<p>We assume that A and B are wealy coupled. This has two implications:</p>
<ol>
<li>The interaction energy $E_{AB}$ is negligible compared to the individual energies $E_A$ and $E_B$ and their variations. Therefore, the total energy becomes
$$E_T\approx E_A+E_B.$$</li>
<li>The total degress of freedom are specified independently. Therefore, the density of states factorizes and we are free to choose states for A and B independently:
$$\Omega_T(E_T)=\Omega_A(E_A)\Omega_B(E_B).$$
This implies that entropy is additive:
$$S_T(E_T)=k\log\Omega_T(E_T)=k\log\Omega_A(E_A)+k\log\Omega_B(E_B)=S_A(E_A)+S_B(E_B).$$</li>
</ol>
<p>We also assume that B is much larger than A in the sense that any fluctuation of $E_A$ has negligible effect on $E_B$. This means that we only care about a very small range of values for $E_B$, and therefore the log density of states of B can be approximated by a linear function:
$$\log\Omega_B(E_B)\approx\alpha+\beta E_B.$$</p>
<p>The Maxwell-Boltzmann distribution answers the following question: if the total energy of the system in equilibrium is $E_T$, what is the probability of A being in a particular microstate $i$?
Since the system is in equilibrium, the probability of A being in the microstate $i$ equals the fraction of microstates of the whole system for which A is in the microstate $i$, which is proportional to the number of microstates of B given A is in the microstate $i$:
$$p_i\propto\Omega_B(E_T-E_i)\propto e^{-\beta E_i},$$
where $\beta$ is called the inverse temperature. We can define the temperature $T$ by
$$\frac{1}{kT}\equiv\beta=\frac{\partial\log\Omega_B(E_B)}{\partial E_B}=\frac{1}{k}\frac{\partial S_B(E_B)}{\partial E_B},$$
where $k=1.3806488\times 10^{-23}$ J/K is the Boltzmann constant that converts human unit of temperature (Kelvin) to physical unit of temperature (Joules, which is the unit of energy).</p>
<p>With the above definition, we can write down the Maxwell-Boltzmann distribution:
$$p_i=\frac{e^{-E_i/kT}}{Z}.$$
The normalizing constant is called the partition function, which is given by
$$Z=\sum_i e^{-E_i/kT},$$
where the sum is taken over all microstates of A.</p>
<h4 id="26-thermodynamic-forces-and-entropy">2.6 Thermodynamic Forces and Entropy</h4>
<p>More generally, we usually care about the energy as well as other macroscopic variables.</p>
<p>Suppose that system A is a balloon filled with helium which can exchange both energy $E$ and volumn $V$ with the surrounding air B. We repeat the same argument above and assume the log density of states of B is linear in both energy and volumn:
$$\log\Omega_B(E_B,V_B)\approx\alpha+\beta E_B+\gamma V_B.$$
Then, the probability of A being in a particular microstate $i$ is given by
$$p_i\propto\Omega_B(E_T-E_i,V_T-V_i)\propto e^{-\beta E_i-\gamma V_i}.$$
Conventionally, we define a variable called pressure as
$$P\equiv kT\frac{\partial\log\Omega_B(E_B,V_B)}{\partial V_B}=T\frac{\partial S_B(E_B, V_B)}{\partial V_B}.,$$
and rewrite the probability of A being in the microstate $i$ as
$$p_i=\frac{e^{-(E_i+PV_i)/kT}}{Z},$$
where the partition function is given by
$$Z=\sum_i e^{-(E_i+PV_i)/kT}.$$</p>
<p>Now, suppose that system A is a (fixed volumn) box filled with gas, with a small hole allowing the molecules to diffuse in from and out to the room B where the box is placed. This means that A can exchange both energy $E$ and the number of particles $N$ with B.</p>
<p>Similarly, we define a variable called chemical potential as
$$\mu\equiv-kT\frac{\partial\log\Omega_B(E_B,N_B)}{\partial N_B}=-T\frac{\partial S_B(E_B, V_B)}{\partial N_B}.$$
Then, the probability of A being in the microstate $i$ is given by
$$p_i=\frac{e^{-(E_i-\mu N_i)/kT}}{Z}.$$</p>
<p>Quantities like pressure $P$ and chemical potential $\mu$ are called thermodynamic forces, which correspond to the tendency of changing the macroscopic variables, volume $V$ and the number of particles $N$, with respect to which the entropy is differentiated. We say that $(V,P)$ and $(N,\mu)$ form conjugate pairs.</p>
<p>We can now write down the total differential of the entropy $S$ as a function of the (internal) energy $E$, volume $V$ and the number of i-th particles $N_i$:
$$
\begin{aligned}
dS(E,V,N)&amp;=\left.\frac{\partial S}{\partial E}\right|_{V,N}dE+\left.\frac{\partial S}{\partial V}\right|_{E,N}dV+\sum_i\left.\frac{\partial S}{\partial N_i}\right|_{E,V}dN_i \\
&amp;\equiv \frac{1}{T}dE+\frac{P}{T}dV-\sum_i\frac{\mu_i}{T}dN_i.
\end{aligned}
$$</p>
<h4 id="27-probabilities-of-macrostates-and-free-energy">2.7 Probabilities of Macrostates and Free Energy</h4>
<p>Having defined the probability of A being in a microstate $i$, we now define the probability of a macrostate with energy $E_A$:
$$p(E_A)=\sum_{i:E_i=E_A}\frac{e^{-E_i/kT}}{Z}=\Omega_A(E_A)\frac{e^{-E_A/kT}}{Z}.$$
Recall that the definition of the entropy of a macrostate is
$$S(E_A)=k\log\Omega_A(E_A),$$
which is just another way of measuring the number of microstates corresponding to the macrostate with energy $E_A$.</p>
<p>Given the definition of entropy, we can rewrite the probability of a macrostate as
$$p(E_A)=\frac{e^{-(E_A-TS(E_A))/kT}}{Z}.$$
This leads to the definition of the Helmholtz free energy:
$$F=E-TS(E).$$</p>
<h4 id="28-thermodynamic-potentials-and-free-energy">2.8 Thermodynamic Potentials and Free Energy</h4>
<p>We have studied how to compute the probability of finding a system in a microstate or macrostate specified by an arbitrary set of macroscopic variables. Interestingly, in every case, the probability takes exactly the same form:
$$p=\frac{e^{-\Phi/kT}}{Z},$$
where $\Phi$ is called thermodynamic potential, which is a energy-like function that defines different kinds of probability distributions for the system.</p>
<p>Some common thermodynamic potentials with special names are</p>
<ul>
<li>Energy: $E_A$.</li>
<li>Helmholtz free energy: $F_A=E_A-T_BS_A(E_A)$.</li>
<li>Enthalpy: $H_A=E_A+P_BV_A$.</li>
<li>Gibbs free energy: $G_A=E_A+P_BV_A-T_BS_A(E_A,V_A)$.</li>
<li>Grand potential: $\Phi_{G_A}=E_A-\mu_B N_A-T_BS_A(E_A,N_A)$.</li>
</ul>
<p>Note that the temperature $T_B$, pressure $P_B$ and chemical potential $\mu_B$ are defined in terms of the entropy $S_B$ of the heat bath, and hence the subscript $B$. The energy $E_A$, volume $V_A$, number of particles $N_A$ are the macroscopic variables of the small system of interest, and hence the subscript $A$.</p>
<p>The term free energy generally refers to any thermodynamic potential that includes a $-TS$ term, which describes the probability of a macrostate.</p>
<p>There are two ways of understanding thermodynamic potentials:</p>
<ol>
<li>We start with $E$ and add in terms based on the ensemble that we want to use. If we want the volumn to be variable, then add the $PV$ term. If we want the number of particles to be variable, then add the $-\mu N$ term. If we want to work with macrostates, then add the $-TS$ term.</li>
<li>We start with a single potential that includes all possible terms. If the volumn is fixed, then $PV$ is constant and can be ignored. If the number of particles is fixed, then $-\mu N$ is constant and can be ignored. If we want to work with microstates, then $TS=0$ since a microstate can be thought of as a macrostate with a single microstate (so the entropy $S$ is zero).</li>
</ol>
<p>A thermodynamics potential is a measure how probable it is for a system to be in a particular microstate or macrostate since it appears in the exponent of the Maxwell-Boltzmann distribution (lower values of thermodynamics potential indicate more likely states). The probability is balanced by all terms in the thermodynamics potential. For example, lower energy corresponds to a more probable microstate, but there may be fewer of such microstates when forming their corresponding macrostate.</p>
<p>Some common ensembles with special names are</p>
<ul>
<li>Microcanonical ensemble refers to an isolated system with fixed energy. Therefore, it has equal probability of being in any microstate with that specific energy and no chance of being in any microstate with a different energy.</li>
<li>Canonical ensemble refers to a system that can exchange energy with a heat bath at a fixed temperature. The corresponding thermodynamic potential is $E$ for microstates and $E-TS(E)$ for macrostates.</li>
<li>Grand canonical ensemble refers to a system that can exhcange both energy and particles with a heat bath of a fixed temperature and chemical potential. The corresponding thermodynamic potential is $E-\mu N$ for microstates and $E-\mu N-TS(E,N)$ for macrostates.</li>
</ul>
<h4 id="29-averages-and-very-large-numbers">2.9 Averages and Very Large Numbers</h4>
<p>The ensemble average or mean of a quantity $x$ is defined as
$$\left&lt;x\right&gt;=\sum_i x_i p_i=\frac{\sum_i x_i e^{-\Phi_i/kT}}{\sum_i e^{-\Phi_i/kT}},$$
where $x_i$ is the value of the quantity $x$ in the $i$-th state, $p_i$ is the probability of being in that state, and the sum can be over either microstates or macrostates depending on whether $x$ is a microscopic variable or macroscopic variable. It can be shown that
$$\left&lt;x+y\right&gt;=\left&lt;x\right&gt;+\left&lt;y\right&gt;\quad\text{and}\quad\left&lt;Cx\right&gt;=C\left&lt;x\right&gt;.$$</p>
<p>The variance of a quantity $x$ measures its fluctuations, which is how much $x$ tends to vary about its average:
$$\sigma(x)^2=\left&lt;(x-\left&lt;x\right&gt;)^2\right&gt;=\left&lt;x^2\right&gt;-\left&lt;x\right&gt;^2.$$
The square root of variance is called standard deviation $\sigma(x)$, which also measures the fluctuations of $x$ but has the dimensions as $x$. Roughly speaking, the value of $x$ is inbetween $\left&lt;x\right&gt;\pm\sigma(x)$ about 2/3 of the time.</p>
<p>Most problems in statistical mechanics tend to involve very large numbers, in which cases averages can simply be treated as exact numbers since any variation about the average value is extremely small. In other words, the chance of measuring any value except the average is negligible. This great simplification is one of the reasons why statistical mechanics is so successful.</p>
<p>The central limit theorem states that the sum of i.i.d. random variables $x_i$ with mean $\mu_x$ and standard deviation $\sigma_x$ is distributed as a normal distribution with mean $\mu_S=N\mu_x$ and standard deviation $\sigma_S=\sqrt{N}\sigma_x$ in the limit $N\to\infty$:
$$S=\sum_{i=1}^N x_i\sim\mathcal{N}(N\mu_x,\sqrt{N}\sigma_x),\quad\text{as }~N\to\infty.$$
In practice, the convergence rate to the normal distribution depends on the distribution of $x_i$. Usually, $N=10$ already gives a good approximation to a normal distribution.</p>
<p>We usually care about the the ratio of the standard deviation and the average:
$$\frac{\sigma_S}{\mu_S}\propto\frac{1}{\sqrt{N}},\quad\text{as }~N\to\infty.$$
As an example, a typical sized room contains $N=10^{26}$ oxygen molecules. Let $x_i$ denote whether the $i$-th oxygen molecule is in the left side of the room. The fluctuations over the average of the number of oxygen molecules in the left side of the room is $10^{-13}$, which is negligible as it is too small to measure.</p>
<h4 id="210-partition-function">2.10 Partition Function</h4>
<p>Recall that the partition function is defined as
$$Z=\sum_i e^{-\beta\Phi_i}=\sum_i e^{-\Phi_i/kT},$$
which has some interesting and useful properties. In particular, the derivatives of $\log Z$ tend to give averages.</p>
<p>For example, the average thermodynamic potential $\left&lt;\Phi\right&gt;$ can be obtained by
$$
\begin{aligned}
-\frac{\partial\log Z}{\partial\beta}
&amp;=-\frac{1}{Z}\frac{\partial Z}{\partial\beta} \\
&amp;=-\frac{1}{Z} \frac{\partial}{\partial\beta} \sum_i e^{-\beta\Phi_i} \\
&amp;=-\frac{1}{Z}\sum_i \frac{\partial e^{-\beta\Phi_i}}{\partial\beta} \\
&amp;= \frac{1}{Z}\sum_i \Phi_i e^{-\beta\Phi_i} \\
&amp;= \left&lt;\Phi\right&gt;.
\end{aligned}
$$</p>
<p>Another useful trick is to take the derivative with respect to a continuous microscopic or macroscopic variable $x$, which gives
$$
\begin{aligned}
-kT\frac{\partial\log Z}{\partial x}
&amp;=-\frac{kT}{Z}\frac{\partial Z}{\partial x} \\
&amp;=-\frac{kT}{Z}\int\frac{\partial e^{-\Phi(x)/kT}}{\partial x}dx \\
&amp;=\frac{1}{Z}\int\frac{\partial\Phi(x)}{\partial x}e^{-\Phi(x)/kT}dx \\
&amp;=\left&lt;\frac{\partial\Phi(x)}{\partial x}\right&gt;.
\end{aligned}
$$</p>
<h4 id="211-alternative-derivation-of-the-boltzmann-distribution-and-free-energy">2.11 Alternative Derivation of the Boltzmann Distribution and Free Energy</h4>
<p>A system in contact with a heat bath at a fixed temperature $T$ can be thought of as $N$ ($N\to\infty$) replicas of the same system wealy connected together (i.e., a canonical ensemble). The first replica is of interest to us, and the rest provide the heat bath. Each replica is in a microstate $i$ with energy $E_i$. Just like the system can exchange energy with the heat bath, these replicas can exchange energy with each other. The total energy $E_{total}=N\left&lt;E\right&gt;$ of the $N$ replicas is conserved. In equilibrium, what is the probability of the first system among those $N$ replicas is in macrostate $i$?</p>
<p>The postulate of equal a priori probabilities tells us that each way of redistributing these $N$ replicas among the possible microstates that is consistent with a given total energy is equally probable. Let $n_i$ be the occupation number, which denotes the number of replicas that are in state $i$. The most probable set of occupation numbers $n=(n_1,n_2,\cdots)$ is the one that corresponds to the maximum number of ways of redistributing these $N$ replicas among possible states subject to the following constraints:
$$
\sum_i n_i = N,
$$
$$
\sum_i n_iE_i = E_{total}.
$$</p>
<p>The number of ways of redistributing the $N$ relicas is called the multilicative number:
$$
\Omega=\frac{N!}{\prod_i n!}.
$$
With Stiring&rsquo;s approximation:
$$\log n! =\sum_{x=1}^n\log x\approx\int_{1}^n \log xdx\sim n\log n-n,$$
the multiplicative number can be approximated by
$$
\log \Omega=\log N!-\sum_i\log n!\approx N\log N - N - \sum_i n_i\log n_i + \sum_i n_i = N\log N - \sum_i n_i\log n_i.
$$
Now, we turn the counts into probabilities. The log multilicative number can be rewritten as the total entropy of the $N$ replicas:
$$
k\log \Omega\approx -k\sum_i n_i\log\frac{n_i}{N}=-Nk\sum_i\frac{n_i}{N}\log\frac{n_i}{N}=NS.
$$
The two constraints on counts can be turned into constraints of probability and average energy:
$$
\sum_i p_i=\frac{\sum_i n_i}{N}=1,
$$
$$
\sum_i p_iE_i = \frac{\sum_i n_iE_i}{N}=\frac{E_{total}}{N}=\left&lt;E\right&gt;.
$$
Finally, we need to solve the following constrained optimization problem to obtain the Boltzmann distribution:
$$
\max_{p_i}~S/k=-\sum_i p_i\log p_i,
$$
$$
s.t.~\sum_i p_i=1\quad\text{and}\quad\sum_i p_iE_i =\left&lt;E\right&gt;.
$$
Using the Lagrange multiplier, we have the following unconstrained optimization problem:
$$
\max_{p_i,\alpha,\beta} ~L=S/k + \alpha\left(1-\sum_i p_i\right) + \beta\left(\left&lt;E\right&gt;-\sum_i p_iE_i\right).
$$
We set the derivative of $L$ w.r.t. $p_i$ to zero.
$$
\frac{\partial L}{\partial p_i} = -\log p_i-1-\alpha-\beta E_i = 0.
$$
This results in the Boltzmann distribution:
$$
p_i=e^{-(\alpha+1)-\beta E_i}=\frac{e^{-\beta E_i}}{Z},
$$
where $\beta\equiv1/kT$ as the inverse temperature, and the partition function is defined as
$$
Z=\sum_i e^{-\beta E_i}.
$$</p>
<p>This probabilistic formulation with the average energy constraint emphasizes the fact that equilibrium does not mean static. A system in equilibrium is still moving and fluctuating but with a statistical balance of motions. In our example, our system in equilibrium is still constantly exchanging energy with the heat bath and switching among different states, but with a constant average energy $\left&lt;E\right&gt;$.</p>
<p>We can further express the Helmholtz free energy in terms of the partion function. Note that the entropy can be rewritten as
$$
\begin{aligned}
S&amp;=-k\sum_i p_i \log p_i \\
&amp;=k\frac{1}{Z}\sum_i e^{-E_i/kT} \left(\frac{E_i}{kT}+\log Z\right) \\
&amp;=\frac{\left&lt;E\right&gt;}{T}+k\log Z.
\end{aligned}
$$
Therefore, the Helmholtz free energy can be expressed as
$$
F = \left&lt;E\right&gt;-TS=-kT\log Z.
$$</p>
<h3 id="3-interpretation-of-statistical-quantities">3. Interpretation of Statistical Quantities</h3>
<p>We have derived statistical definitions of many abstract mathematical quantities such as temperature. Now we try to build up an intuitive understanding of what they are in the physical world.</p>
<h4 id="31-temperature">3.1 Temperature</h4>
<p>Temperature measures the microscopic jiggling of atoms: the faster they move, the hotter an object feels. To see this and understand the significance of temperature, we prove the equipartition theorem below.</p>
<p>Consider a general energy function that can be seperated into the sum of two terms:
$$E(y_1,y_2,\cdots)=E_1(y_1)+E_2(y_2,y_3,\cdots),$$
where the energy is quadratic in $y_1$:
$$E(y_1)=Cy_1^2.$$
Now, we compute the average value of $E_1$:
$$
\begin{aligned}
\left&lt;E_1(y_1)\right&gt; &amp;= \frac{\int E_1(y_1)e^{-\beta E(y_1,y_2,\cdots)}dy_1dy_2\cdots}{\int e^{-\beta E(y_1,y_2,\cdots)}dy_1dy_2\cdots} \\
&amp;= \frac{\int E_1(y_1)e^{-\beta E_1(y_1)}dy_1}{\int e^{-\beta E_1(y_1)}dy_1} \\
&amp;=-\frac{\partial}{\partial\beta}\log\int e^{-\beta E_1(y_1)}dy_1 \\
&amp;=-\frac{\partial}{\partial\beta} \log\int e^{-\beta Cy_1^2}dy_1 \\
&amp;=-\frac{\partial}{\partial\beta} \left(\log\beta^{-1/2}+\log\int e^{-\beta Cy_1^2}d(\beta^{1/2}y_1)\right) \\
&amp;=\frac{1}{2\beta}=\frac{kT}{2}.
\end{aligned}
$$
One particularly important example is the momentum $p$ of a classical particle, which corresponds to the kenetic energy:
$$E_{kinetic}(p)=\frac{p^2}{2m}.$$
This tells us that temperature is a measure of average kinetic energy, which shows how the statistical definition of temperature matches up with the physical one: if a system is in equilibrium with a heat bath at temperature $T$, then its average kinetic energy per degree of freedom is $\left&lt; E_{kinetic}\right&gt;=kT/2$. For examples, a gas of N rigid diatomic moelcules at temperature $T$ has $5N$ degrees of freedom (each rigid diatomic molecule has $2\times3-1=5$ degrees of freedom), whose kinetic energy is $\left&lt; E_{kinetic}\right&gt;=5N\cdot kT/2$.</p>
<p>Another example is the position $x$ of a harmonic oscillator, which corresponds to the potential energy:
$$E_{potential}(x)=k_hx^2.$$
Therefore, the average potential energy of a harmonic oscillator in equilibrium at temperature $T$ is $\left&lt; E_{potential}\right&gt;=kT/2$ per degree of freedom. More generally, the total energy of a system in equilibrium tends to roughly evenly distributed across its degrees of freedoms.</p>

    </div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://wenlin-chen.github.io/post/statistical_mechanics/&amp;text=Statistical%20Mechanics" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://wenlin-chen.github.io/post/statistical_mechanics/&amp;t=Statistical%20Mechanics" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Statistical%20Mechanics&amp;body=https://wenlin-chen.github.io/post/statistical_mechanics/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://wenlin-chen.github.io/post/statistical_mechanics/&amp;title=Statistical%20Mechanics" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Statistical%20Mechanics%20https://wenlin-chen.github.io/post/statistical_mechanics/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://wenlin-chen.github.io/post/statistical_mechanics/&amp;title=Statistical%20Mechanics" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://wenlin-chen.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu27f7dc34c9db8fd2b063f3fba6be9864_216943_270x270_fill_q75_lanczos_center.jpg" alt="Wenlin Chen"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://wenlin-chen.github.io/">Wenlin Chen</a></h5>
      <h6 class="card-subtitle">PhD Student in Machine Learning</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=A4zbE2IAAAAJ" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/wenlin-chen/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Wenlin-Chen" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/WenlinChen_" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

    










  
    
    
    
    
    













    
    <p class="powered-by">
      © 2024 Wenlin Chen
    </p>
    
  
    <p class="powered-by">
      
      
      
    </p>
  </footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.9592335d574f7a97010f99b90ad0f310.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.2cc80485e7b9001edba5cdf5b39a1f97.js"></script>

    






</body>
</html>
