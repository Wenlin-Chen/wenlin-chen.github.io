<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Wenlin Chen" />

  
  
  
    
  
  <meta name="description" content="1. Latent Variable Models 1.1 Introduction Observed data: $x\in\mathcal{X}\subseteq\mathbb{R}^n$. Latent variable: $z\in\mathbb{R}^d$ ($d\leq n$). The joint distribution is factorized as the product of likelihood and prior: $$p_{\theta}(x,z)=p_{\theta}(x|z)p(z).$$ The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network: $$p_{\theta}(x|z)=\mathcal{N}(x|\mu_{\theta}(z),\sigma^2I)." />

  
  <link rel="alternate" hreflang="en-us" href="https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/" />

  









  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.807c460ea42d089b71c3cb26b919dfa0.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Wenlin Chen" />
  <meta property="og:url" content="https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/" />
  <meta property="og:title" content="Diffusion and Latent Variable Models | Wenlin Chen" />
  <meta property="og:description" content="1. Latent Variable Models 1.1 Introduction Observed data: $x\in\mathcal{X}\subseteq\mathbb{R}^n$. Latent variable: $z\in\mathbb{R}^d$ ($d\leq n$). The joint distribution is factorized as the product of likelihood and prior: $$p_{\theta}(x,z)=p_{\theta}(x|z)p(z).$$ The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network: $$p_{\theta}(x|z)=\mathcal{N}(x|\mu_{\theta}(z),\sigma^2I)." /><meta property="og:image" content="https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2024-06-12T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-06-12T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/"
  },
  "headline": "Diffusion and Latent Variable Models",
  
  "datePublished": "2024-06-12T00:00:00Z",
  "dateModified": "2024-06-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Wenlin Chen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Wenlin Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wenlin-chen.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "1. Latent Variable Models 1.1 Introduction Observed data: $x\\in\\mathcal{X}\\subseteq\\mathbb{R}^n$. Latent variable: $z\\in\\mathbb{R}^d$ ($d\\leq n$). The joint distribution is factorized as the product of likelihood and prior: $$p_{\\theta}(x,z)=p_{\\theta}(x|z)p(z).$$ The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network: $$p_{\\theta}(x|z)=\\mathcal{N}(x|\\mu_{\\theta}(z),\\sigma^2I)."
}
</script>

  

  

  

  





  <title>Diffusion and Latent Variable Models | Wenlin Chen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="85e73f94555f323fbd8d619256569259" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Wenlin Chen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Wenlin Chen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/resume/"><span>Resume</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  













  

  
  
  
<div class="container pt-3">
  <h1>Diffusion and Latent Variable Models</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 12, 2024
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

    





  
</div>


  <div class="container">

    <div class="article-style">
      <h3 id="1-latent-variable-models">1. Latent Variable Models</h3>
<h4 id="11-introduction">1.1 Introduction</h4>
<ul>
<li>Observed data: $x\in\mathcal{X}\subseteq\mathbb{R}^n$.</li>
<li>Latent variable: $z\in\mathbb{R}^d$ ($d\leq n$).</li>
<li>The joint distribution is factorized as the product of likelihood and prior:
$$p_{\theta}(x,z)=p_{\theta}(x|z)p(z).$$
<ul>
<li>The likelihood (or decoder/generation model) is a Gaussian distribution with mean parameterized by a neural network:
$$p_{\theta}(x|z)=\mathcal{N}(x|\mu_{\theta}(z),\sigma^2I).$$</li>
<li>The prior is often chosen to be a simple distribution like standard Gaussian:
$$p(z)=\mathcal{N}(z|0,I).$$</li>
</ul>
</li>
<li>The marginal likelihood (or model evidence) is usually used for model selection:
$$p_{\theta}(x)=\int p_{\theta}(x|z)p(z) dz.$$
<ul>
<li>Maximum marginal likelihood learning $\max_{\theta}\log p_{\theta}(x)$ in intractable due to the intractable integral.</li>
</ul>
</li>
<li>The posterior is given by Bayes&rsquo; rule:
$$p_{\theta}(z|x)=\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(x)}=\frac{p_{\theta}(x|z)p(z)}{\int p_{\theta}(x|z)p(z) dz}.$$
<ul>
<li>The posterior is also intractable due to the intractability of the marginal likelihood.</li>
</ul>
</li>
</ul>
<h4 id="12-variational-autoencoders-vaes">1.2 Variational Autoencoders (VAEs)</h4>
<ul>
<li>VAEs introduce an amortized mean-field Gaussian variational posterior (or encoder/inference model) with mean and diagnoal variance parameterized by neural networks:
$$q_{\phi}(z|x)=\mathcal{N}(z|\tilde{\mu}_{\phi}(x),\text{diag}(\tilde{\sigma}_{\phi}(x)^2))\approx p_{\theta}(z|x).$$</li>
<li>The parameters of both the generation and inference models are jointly learned by maximizing a tractable evidence lower bound (ELBO) of the log marginal likelihood:
$$\log p_{\theta}(x)\geq\mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\right]=\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]-\text{KL}(q_{\phi}(z|x)||p(z))=\mathcal{F}(\theta,\phi).$$
<ul>
<li>The first term $\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]$ controls the reconstruction error.</li>
<li>The second term $\text{KL}(q_{\phi}(z|x)||p(z))$ regularizes the approximate posterior to be close to the prior.</li>
<li>In order to backpropagate through samples $z\sim q_{\phi}(z|x)$, we employ the reparameterization trick:
$$z=\tilde{\mu}_{\phi}(x)+\tilde{\sigma}_{\phi}(x)\odot \epsilon,\quad \epsilon\sim N(0,I).$$</li>
<li>It is quite hard to train both generation and inference models well in practice due to difficulties in balancing the two terms above during optimization (e.g., the variational posterior can easily collapse to the prior due to variational overpruning).</li>
</ul>
</li>
<li>The gap between the ELBO and log marginal likelihood is given by:
$$\log p_{\theta}(x)-\mathcal{F}(\theta,\phi)=\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\geq 0.$$
<ul>
<li>Maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posteriors.</li>
<li>The ELBO attains equality when the approximate posterior is exactly the same as the true posterior:
$$\log p_{\theta}(x)=\mathcal{F}(\theta,\phi)\iff\text{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))=0\iff q_{\phi}(z|x)=p_{\theta}(z|x).$$</li>
<li>However, the mean-field variational poterior $q_{\phi}(z|x)$ is uni-modal and the reverse KL divergence is known to be mode-seeking instead of mass convering. This means that $q_{\phi}(z|x)$ can only fit to one of the modes in the true posterior $p_{\theta}(z|x)$, which limits the expressivity of VAEs and makes it hard for the aggregated poseterior $q_{\phi}(z)=\int q_{\phi}(z|x)p_{\text{data}}(x)dx$ to be close to the prior $p(z)=\mathcal{N}(0,I)$.</li>
</ul>
</li>
</ul>
<h3 id="2-diffusion-and-hierarchical-latent-variable-models">2. Diffusion and Hierarchical Latent Variable Models</h3>
<h4 id="21-introduction">2.1 Introduction</h4>
<ul>
<li>We slightly change the notation following the convention of hierarchical latent variable models.
<ul>
<li>Observed data: $x_0=x\in\mathcal{X}\subseteq\mathbb{R}^n$.</li>
<li>We employ a sequence of latent variables $x_1,\cdots,x_T\in\mathbb{R}^d$ ($d\leq n$) to capture data representation at different levels.</li>
</ul>
</li>
<li>The results in a new marginal likelihood:
$$p_{\theta}(x_0)=\int p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)dx_{1:T}.$$</li>
<li>The prior over the last latent variable is a Standard Gaussian:
$$p(x_T)=\mathcal{N}(x_T|0,I).$$</li>
<li>The likelihoods (or decoders/generation models) are Gaussians with mean parameterized by a time-dependent neural network:
$$p_{\theta}(x_{t-1}|x_t)=\mathcal{N}(x_{t-1}|\mu_{\theta}(x_t,t),\sigma_t^2 I).$$</li>
<li>The posterior is intractable as before:
$$p_{\theta}(x_{1:T}|x_0)=\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{p_{\theta}(x_0)}.$$</li>
</ul>
<h4 id="22-hierarchical-variational-autoencoders-hvaes">2.2 Hierarchical Variational Autoencoders (HVAEs)</h4>
<ul>
<li>HVAEs approximate the intractable posterior with an amortized Gaussian variational posterior (or encoder/inference model):
$$q_{\phi}(x_{1:T}|x_0)\approx p_{\theta}(x_{1:T}|x_0).$$</li>
<li>There are different design choices for the factorization of the inference model.
<ul>
<li>Bottom-up factorization:
$$q_{\phi}(x_{1:T}|x_0)=\prod_{t=1}^T q_{\phi}(x_t|x_{t-1}).$$</li>
<li>Top-down factorization:
$$q_{\phi}(x_{1:T}|x_0)=q_{\phi}(x_T|x_0)\prod_{t=2}^T q_{\phi}(x_{t-1}|x_t,x_0).$$</li>
<li>The mean and diagnoal variance of the factors $q_{\phi}(x_t|x_{t-1})$ and $q_{\phi}(x_{t-1}|x_t,x_0)$ are parameterized by neural networks.</li>
</ul>
</li>
<li>The tractable ELBO can be derived for HVAEs:
$$\log p_{\theta}(x_0)\geq\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log\frac{p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}{q_{\phi}(x_{1:T}|x_0)}\right]=\mathcal{F}(\theta,\phi).$$
<ul>
<li>For the bottom-up factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=1}^{T-1}\text{KL}(q_{\phi}(x_t|x_{t-1})||p_{\theta}(x_t|x_{t+1}))-\text{KL}(q_{\phi}(x_T|x_{T-1})||p(x_T))\right].$$</li>
<li>For the top-down factorization, the ELBO becomes:
$$\mathcal{F}(\theta,\phi)=\mathbb{E}_{q_{\phi}(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q_{\phi}(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q_{\phi}(x_T|x_0)||p(x_T))\right].$$</li>
<li>The parameters of both generation and inference models are learned jointly by maximizing the ELBO.</li>
<li>To reduce the model size, parameter sharing between the generation and inference models is possible.</li>
<li>HVAEs are very flexible and expressive, since only the last latent variable $x_T$ is constrained to be standard Gaussian and the model will need to figure out the intermediate latent variables $x_{1:T-1}$.</li>
<li>This causes even more difficulties during optimization than VAEs.</li>
</ul>
</li>
</ul>
<h4 id="23-denoising-diffusion-probabilistic-models-ddpms">2.3 Denoising Diffusion Probabilistic Models (DDPMs)</h4>
<ul>
<li>DDPMs are a class of state-of-the-art deep generative models.</li>
<li>DDPMs define a fixed bottom-up inference model (or diffusion process):
$$q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}).$$
<ul>
<li>No dimensionality reduction is performed $(d=n)$.</li>
<li>The factors are predefined Gaussian convolution kernels:
$$q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I).$$
$$\implies q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I),\quad \bar{\alpha}_t=\prod_{s=1}^t \alpha_s.$$</li>
<li>The top-down form of the inference model is analytically tractable:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1}|\tilde{\mu}(x_t,x_0),\tilde{\sigma}_t^2),$$
where
$$\tilde{\mu}(x_t,x_0)=\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t,$$
$$\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t}.$$
<ul>
<li>Sketch of proof: By Bayes&rsquo; rule and the Markovian property, we have
$$q(x_{t-1}|x_t,x_0)= \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\propto q(x_t|x_{t-1})q(x_{t-1}|x_0).$$
Since everything is Gaussian with analytical expression, $\tilde{\mu}(x_t,x_0)$ and $\tilde{\sigma}_t^2$ can be figured out by moment matching.</li>
</ul>
</li>
</ul>
</li>
<li>The tractable ELBO can be derived for DDPMs:
$$
\begin{aligned}
\mathcal{F}(\theta)
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T\text{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\text{KL}(q(x_T|x_0)||p(x_T))\right] \\
&amp;= \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\theta}(x_0|x_1)-\sum_{t=2}^T \frac{\lVert\tilde{\mu}(x_t,x_0)-\mu_{\theta}(x_t,t)\rVert^2}{2\sigma_t^2}\right]+const.
\end{aligned}
$$
<ul>
<li>In constrast to HVAEs with flexible intermediate latent variables, the fixed inference model in DDPMs provides extra supervision signals for the intermediate latent variables.</li>
<li>The objective for each intermediate latent variable looks like a regression objective and is much easier to optimize.</li>
</ul>
</li>
<li>The ELBO can be further simplified using variance reduction techniques with the reparameterization trick:
<ol>
<li>Reparameterize $x_0$ according to the predefined inference model $q(x_t|x_0)$:
$$x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\quad\implies\quad x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}},\quad\epsilon\sim\mathcal{N}(0,I).$$</li>
<li>Plug in the reparameterized $x_0$ to $\tilde{\mu}(x_t,x_0)$:
$$\tilde{\mu}(x_t,x_0)=\tilde{\mu}\left(x_t,\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right).$$</li>
<li>Reparameterize $\mu_{\theta}(x_t,t)$ in the same form as $\tilde{\mu}(x_t,x_0)$:
$$\mu_{\theta}(x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right).$$</li>
<li>The regression objectives can then be simplified to noise matching with a time-dependent noise prediction network $\epsilon_{\theta}(x_t,t)$:
$$\sum_{t=1}^T\lambda(t)\mathbb{E}_{\mathcal{N}(\epsilon|0,I)}[\lVert\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t)\rVert^2].$$
<ul>
<li>The analytical solution for the weighting function is given by
$$\lambda(t)=\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}.$$</li>
<li>However, setting $\lambda(t)=1$ makes training much more stable in practice.</li>
</ul>
</li>
</ol>
</li>
<li>Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t,t)\right)+\sigma_t z_t,\quad z_t\sim\mathcal{N}(0,I).$$
<ul>
<li>The variance of $p_{\theta}(x_{t-1}|x_t)$ can be set to
$$\sigma_t^2=1-\alpha_t\quad\text{or}\quad \sigma_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\alpha_t)}{1-\bar{\alpha}_t},$$
which work equally well in practice for large $T$.</li>
<li>It can be helpful to have a better approximation for the true covariance $\Sigma_{\theta}(x_t, t)$ of $p_{\theta}(x_{t-1}|x_t)$ especially when $T$ is small.</li>
</ul>
</li>
<li>Connection to score matching and score-based diffuson models:
<ul>
<li>Denoising score matching with a time-dependent score network $s_{\theta}(x_t, t)$:
$$
\begin{aligned}
\mathbb{E}_{q(x_t|x_0)}\left[\lVert\nabla_{x_t}\log q(x_t|x_0)-s_{\theta}(x_t, t)\rVert^2\right]
&amp;=\mathbb{E}_{q(x_t|x_0)}\left[\left\lVert\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{1-\bar{\alpha}_t}+s_{\theta}(x_t, t)\right\rVert^2\right] \\
&amp;= \mathbb{E}_{\mathcal{N}(\epsilon|0,I)}\left[\left\lVert\frac{\epsilon}{\sqrt{1-\bar{\alpha}_t}}+s_{\theta}(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, t)\right\rVert^2\right].
\end{aligned}
$$
Therefore, the score network $s_{\theta}(x_t,t)$ and the noise prediction network $\epsilon_{\theta}(x_t,t)$ have the following connection:
$$s_{\theta}(x_t,t)=-\frac{\epsilon_{\theta}(x_t,t)}{\sqrt{1-\bar{\alpha}_t}}.$$</li>
<li>The sampling procedure of DDPM corresponds to the predictor step (numerical SDE solution with discretization) in the predictor-corrector sampling precedure of score-based diffusion models. One could also in principle introduce the corrector step in DDPM sampling by leveraging the relationship between $s_{\theta}(x_t, t)$ and $\epsilon_{\theta}(x_t, t)$ above.</li>
</ul>
</li>
</ul>
<h4 id="24-denoising-diffusion-implicit-models-ddims">2.4 Denoising Diffusion Implicit Models (DDIMs)</h4>
<ul>
<li>DDIMs define a fixed top-down inference model:
$$q(x_{1:T}|x_0)=q(x_T|x_0)\prod_{t=2}^T q(x_{t-1}|x_t,x_0).$$
<ul>
<li>No dimensionality reduction is performed $(d=n)$.</li>
<li>The factors are predefined Gaussians to ensure that $q(x_t|x_0)=\mathcal{N}(x_t|\sqrt{\bar{\alpha}_t}x_0+(1-\bar{\alpha}_t)I)$ as in DDPMs:
$$q(x_{t-1}|x_t,x_0)=\mathcal{N}\left(x_{t-1}\left|\sqrt{\bar{\alpha}_{t-1}}x_0+\frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}}\sqrt{1-\bar{\alpha}_{t-1}-s_t^2},s_t^2\right)\right..$$
<ul>
<li>Note that the forward diffusion process induced by a DDIM is not necessarily Markovian:
$$q(x_t|x_{t-1},x_0)=\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)},$$
since $x_t$ could depend on both $x_{t-1}$ and $x_0$ for some choices of the variance $s_t^2$.</li>
<li>The variance $s_t^2$ controls how stochastic the forward diffusion process is.
<ul>
<li>$x_{t-1}$ becomes a deterministic function of $x_t$ and $x_0$ for $s_t=0$.</li>
<li>If we set the variance $s_t^2$ as in DDPM:
$$s_t^2=\tilde{\sigma}_t^2=\frac{(1-\bar{\alpha}_{t-1})(1-\bar{\alpha}_{t}/\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}},$$
then DDIM recovers DDPM which has a markovian forward diffusion process:
$$q(x_t|x_{t-1},x_0)=q(x_t|x_{t-1})=\mathcal{N}(x_t|\sqrt{\alpha_t}x_0+(1-\alpha_t)I).$$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>The generation models are defined as
$$\begin{aligned}
p_{\theta}(x_{t-1}|x_t)
&amp;=\int q(x_{t-1}|x_t,x_0)p_{\theta}(x_0|x_t)dx_0 \\
&amp;\approx q(x_{t-1}|x_t,\tilde{x}_0(x_t)),\quad \tilde{x}_0(x_t)\sim p_{\theta}(x_0|x_t).
\end{aligned}$$
where we reparameterize $\tilde{x}_0(x_t)$ according to the forward diffusion process:
$$
\tilde{x}_0(x_t)=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)}{\sqrt{\bar{\alpha}_t}}.
$$</li>
<li>The training objective for DDIM has the identical form (up to an additive constant) to that for DDPM with a different weighting function $\lambda(t)$ for each choice of the variance $s_t^2$ in $q(x_{t-1}|x_t,x_0)$.</li>
<li>Sampling/generation is simply initializing $x_T\sim \mathcal{N}(0,I)$ and drawing $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T,\cdots,1$:
$$x_{t-1}=\sqrt{\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_t}}\left(x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)\right)+\epsilon_{\theta}(x_t, t)\sqrt{1-\bar{\alpha}_{t-1}-s_t^2}+s_t e,\quad e\sim\mathcal{N}(0,I).$$
<ul>
<li>DDPM is a special case with $s_t^2=\tilde{\sigma}_t^2$.</li>
<li>$s_t=0$ results in a deterministic generation process.</li>
<li>Accelarated sampling can be achieved by considering a subset of time steps $\{\tau_1,\cdots,\tau_T\}\subseteq\{1,\cdots,T\}$ at generation time:
$$x_{\tau_{i-1}}=\sqrt{\frac{\bar{\alpha}_{\tau_{i-1}}}{\bar{\alpha}_{\tau_{i}}}}\left(x_{\tau_{i}}-\sqrt{1-\bar{\alpha}_{\tau_{i}}}\epsilon_{\theta}(x_{\tau_{i}}, \tau_i)\right)+\epsilon_{\theta}(x_{\tau_{i}}, \tau_i)\sqrt{1-\bar{\alpha}_{\tau_{i-1}}-s_{\tau_{i}}^2}+s_{\tau_{i}} e,\quad e\sim\mathcal{N}(0,I).$$
For the deterministic generation process, we set $s_{\tau_{i}}=0$ again. For DDPM, we have
$$s_{\tau_{i}}^2=\frac{(1-\bar{\alpha}_{\tau_{i-1}})(1-\bar{\alpha}_{\tau_{i}}/\bar{\alpha}_{\tau_{i-1}})}{1-\bar{\alpha}_{\tau_{i}}}.$$</li>
</ul>
</li>
</ul>

    </div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/&amp;text=Diffusion%20and%20Latent%20Variable%20Models" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/&amp;t=Diffusion%20and%20Latent%20Variable%20Models" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Diffusion%20and%20Latent%20Variable%20Models&amp;body=https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/&amp;title=Diffusion%20and%20Latent%20Variable%20Models" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Diffusion%20and%20Latent%20Variable%20Models%20https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://wenlin-chen.github.io/post/diffusion_and_latent_variable_models/&amp;title=Diffusion%20and%20Latent%20Variable%20Models" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://wenlin-chen.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu27f7dc34c9db8fd2b063f3fba6be9864_216943_270x270_fill_q75_lanczos_center.jpg" alt="Wenlin Chen"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://wenlin-chen.github.io/">Wenlin Chen</a></h5>
      <h6 class="card-subtitle">PhD Student in Machine Learning</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?hl=en&amp;user=A4zbE2IAAAAJ" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/wenlin-chen/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Wenlin-Chen" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/resume/" >
        <i class="fas fa-file"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

    










  
    
    
    
    
    













    
    <p class="powered-by">
      © 2025 Wenlin Chen
    </p>
    
  
    <p class="powered-by">
      
      
      
    </p>
  </footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.9592335d574f7a97010f99b90ad0f310.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.2cc80485e7b9001edba5cdf5b39a1f97.js"></script>

    






</body>
</html>
