[{"authors":null,"categories":null,"content":"I am a PhD student in Machine Learning at University of Cambridge (Machine Learning Group, Computational and Biological Learning Lab) and Max Planck Institute for Intelligent Systems (Department of Empirical Inference), under the Cambridge-Tübingen PhD Fellowship in Machine Learning. My supervisors are Professor José Miguel Hernández-Lobato, Professor Bernhard Schölkopf, and Dr Hong Ge.\nI am keen on basic research in machine learning and its scientific applications. My research interest lies at the intersection of probabilistic methods, deep learning, and causal inference. I aim to develop efficient machine learning methods for robust prediction and realistic data generation.\n","date":1696032000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1696032000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD student in Machine Learning at University of Cambridge (Machine Learning Group, Computational and Biological Learning Lab) and Max Planck Institute for Intelligent Systems (Department of Empirical Inference), under the Cambridge-Tübingen PhD Fellowship in Machine Learning.","tags":null,"title":"Wenlin Chen","type":"authors"},{"authors":null,"categories":null,"content":"These are my learning notes for the lecture series Tensors for Beginners.\nMotivation and Applications of Tensors Tensors provide insights about how geometry works. Below are some examples that involve tensors.\nGeneral relativity (metric tensor): curved space-time, expanding universe. Quantum mechanics and quantum computing: quantum superposition (linear combination), quantum engtanglement (tensor product). Optimization. \u0026hellip; Definitions of Tensors Roughtly speaking, tensors are objects defined by the way they transform. Tensor algbra focuses on the analysis of individual tensors, which generalizes linear algebra. Tensor calculus focuses on the analysis of tensor fields, which generalizes multivariate calculus.\nFormally, there are several definitions of tensors from different perspectives.\n(Array Definition) A tensors is a multi-dimensional array, such as\nrank-0 tensor: scalar, rank-1 tensor: vector, rank-2 tensor: matrix, rank-3 tensor: ?, \u0026hellip; This is not a good definition as arrays are not what tensors fundamentally are. The array definition ignores the geometric meaning behind tensors and is not helpful for understanding geometry.\n(Coordinate Definition) A tensor is an object invariant under a change of coordinates, which has components that change in a special, predictable way under a change of coordinates.\nA tensor object itself is intrinsic and does not depend on the choice of coordinate system. The components of a tensor change under different coordinate systems in a specific way that can be figured out. We will get to this. (Algebra Definition) A tensors is a collections of vectors and covectors combined together using the tensor product.\nThis is a consice and probably the best definition of tensors. But\u0026hellip; What are covectors? What is tensor product? We will get to this. (Calculus Definition) Tensors are partial derivatives and gradients that transform with the Jacobian matrix.\nThis is also a useful definition, but we will focus on the coordinate and algebra definitions in tensor algebra. We will begin with special examples of tensors that we have seen in linear algebra and then gradually generalize the concept of tensors. For notational simplicity, we will use Einstein\u0026rsquo;s notation, which drops the summation symbols $\\sum$ since they can be inferred from the context by contracting all corresponding upper and lower indices.\nChange of Basis Consider an old basis $E=[\\mathbf{e}_1,\\cdots,\\mathbf{e}_n]$ and a new basis $\\tilde{E}=[\\tilde{\\mathbf{e}}_1,\\cdots,\\tilde{\\mathbf{e}}_n]$.\nThe forward transformation $F$ builds the new basis from the old basis: $$ \\tilde{\\mathbf{e}}_i = F^j_i\\mathbf{e}_j. $$\nThe backward transformation $B$ builds the old basis from the new basis: $$ \\mathbf{e}_i = B^j_i\\tilde{\\mathbf{e}}_j. $$\nComposing the forward and backward transformation should result in the identity transformation: $$ \\mathbf{e}_i = B^j_i F^k_j\\mathbf{e}_k $$ $$ \\implies B^j_i F^k_j =\\delta^k_i = F^k_j B^j_i. $$\nTherefore, the forward and backward transformations are inverses of each other.\nVectors and Vector Spaces A vector space $(V,S,+,\\cdot)$ consists of\n$V$: a set of vectors, $S$: a set of scalars, $+$: a vector addtion rule such that $\\mathbf{v}+\\mathbf{w}\\in V,~\\forall\\mathbf{v},\\mathbf{w}\\in V$, $\\cdot$: a vector scaling rule such that $a\\cdot\\mathbf{v}\\in V,~\\forall a\\in S,\\mathbf{v}\\in V$. We usually omit $\\cdot$ and just write $a\\mathbf{v}$ instead.\nVectors are a kind of tensor. Vectors are invariant to the coordinate systems, but the components of vectors are not.\nLet $E=[\\mathbf{e}_1,\\cdots,\\mathbf{e}_n]$ be a basis of $(V,S,+,\\cdot)$. First, we note that the basis vectors $\\mathbf{e}_i$ are vectors. A vector $\\mathbf{v}\\in V$ can be represented as a linear combination of the basis vectors: $$ \\mathbf{v}= v^i \\mathbf{e}_i, $$ where the components $v^i\\in S$ of $\\mathbf{v}$ in the basis $E$ are $$ \\mathbf{v}= \\begin{bmatrix} v^1 \\\\ \\vdots \\\\ v^n \\end{bmatrix}_{E} $$ This tells us that column vectors are the array representations of vectors, which is the array definition of vectors.\nVector Transformation Let\u0026rsquo;s represent the vector components of $\\mathbf{v}$ in a different basis $\\tilde{E}$: $$ \\mathbf{v}=\\tilde{v}^j \\tilde{\\mathbf{e}}_j = \\tilde{v}^jF^i_j\\mathbf{e}_i = (F^i_j\\tilde{v}^j)\\mathbf{e}_i. $$ But in the basis $E$, we have $\\mathbf{v}=v^i \\mathbf{e}_i$. This implies that $$ v^i = F^i_j\\tilde{v}^j. $$\nLikewise, we have $$ \\tilde{v}^j \\tilde{\\mathbf{e}}_j=\\mathbf{v}=v^i \\mathbf{e}_i =v^i B^j_i\\tilde{\\mathbf{e}}_j = (B^j_i v^i) \\tilde{\\mathbf{e}}_j, $$ $$ \\implies \\tilde{v}^j = B^j_i v^i. $$\nLet\u0026rsquo;s now compare a change of basis to vector transformation: $$ \\tilde{\\mathbf{e}}_j = F^i_j\\mathbf{e}_i \\quad\\quad \\mathbf{e}_j = B^i_j\\tilde{\\mathbf{e}}_i $$ $$ v^i = F^i_j\\tilde{v}^j \\quad\\quad \\tilde{v}^i = B^i_j v^j $$\nIt is interesting to note that vector transformation behaves contrarily to a change of basis.\nChange of basis: Forward transformation changes the old basis into the new basis. Backward transformation changes the new basis into the old basis. Vector transformation: Forward transformation changes the vector components from the new basis to the old basis. Backward transformation changes the vector components from the old basis to the new basis. We say that vector components are contravariant because they contra-vary with a change of basis. As a reminder, we always put the indices of the vector components above the letters. We say that basis vectors are covariant and put the indices of the basis vectors below the letters.\nCovectors and Dual Vector Spaces Covectors (linear forms) are functions $\\alpha: V\\to S$ that map the vectors in $V$ to the scalars in $S$ such that\n$\\alpha(\\mathbf{v}+\\mathbf{w})=\\alpha(\\mathbf{v})+\\alpha(\\mathbf{w}),~\\forall \\mathbf{v},\\mathbf{w}\\in V$. $\\alpha(n\\mathbf{v})=n\\alpha(\\mathbf{v}),\\forall n\\in S,\\mathbf{v}\\in V$. The dual vector space $(V^*,S,+\u0026rsquo;,\\cdot\u0026rsquo;)$ of a vector space $(V,S,+,\\cdot)$ is a vector space with\nthe same set $S$ of scalars a different set $V^*$ of vectors ($\\alpha\\in V^*$ are covectors), different addition ($+\u0026rsquo;$) and scaling ($\\cdot\u0026rsquo;$) rules such that $(n\\cdot\\alpha)(\\mathbf{v})=n\\alpha(\\mathbf{v}),~\\forall n\\in S,\\alpha\\in V^*,\\mathbf{v}\\in V$. $(\\alpha+\\beta)(\\mathbf{v})=\\alpha(\\mathbf{v})+\\beta(\\mathbf{v}),~\\forall\\alpha,\\beta\\in V^*,\\mathbf{v}\\in V$. As shown above, we usually just write $+$ and $\\cdot$ when we add and scale covectors in $V^*$ but need to remember that the addition and scaling rules in $V^*$ are actually different from those in $V$.\nDual Basis and Covector Components Covectors are a kind of tensor. Covectors are invariant to the coordinate systems, but the components of covectors are not.\nTake the basis $E=[\\mathbf{e}_1,\\cdots,\\mathbf{e}_n]$ for a vector space $V$. We introduce the (Kronecker) dual basis $\\mathcal{E}=[\\epsilon^1,\\cdots,\\epsilon^n]^T$ ($\\epsilon^i:V\\to S$) for its dual vector space $V^*$: $$ \\epsilon^i(\\mathbf{e}_j)=\\delta^i_j. $$\nEach covector $\\epsilon^i\\in V^*$ in the dual basis outputs the corresponding vector component $v^i$ in the basis $E$: $$ \\epsilon^i(\\mathbf{v})=\\epsilon^i\\left( v^j \\mathbf{e}_j \\right)=v^j \\epsilon^i(\\mathbf{e}_j)=v^j\\delta^i_j=v^i. $$\nFor any covector $\\alpha\\in V^*$, we define $$ \\alpha(\\mathbf{e}_i)=\\alpha_i. $$\nNow, a covector $\\alpha\\in V^*$ can be represented as $$ \\alpha(\\mathbf{v})=\\alpha\\left(v^i \\mathbf{e}_i \\right)=v^i \\alpha(\\mathbf{e}_i) = \\alpha_i \\epsilon^i(\\mathbf{v})=(\\alpha_i \\epsilon^i)(\\mathbf{v}). $$ $$ \\implies \\alpha = \\alpha_i \\epsilon^i. $$\nTherefore, the covector components in the dual basis $\\mathcal{E}$ are $$ \\alpha=[\\alpha_1, \\cdots, \\alpha_n]_{\\mathcal{E}}. $$\nThis tells us that row vectors are the array representation of covectors, which is the array definition of covectors.\nGeometrically, a covector is a stack of linearly spaced, straight contour lines. A covector maps a vector into a scalar specified by the number of covector contour lines that the vector covers.\nChange of Dual Basis and Covector Transformation Consider an old dual basis $\\mathcal{E}=[\\epsilon^1,\\cdots,\\epsilon^n]^T$ and a new dual basis $\\tilde{\\mathcal{E}}=[\\tilde{\\epsilon}^1,\\cdots,\\tilde{\\epsilon}^n]^T$ for $V^*$. Let\u0026rsquo;s build the new dual basis from the old dual basis using a transformation $Q$: $$ \\tilde{\\epsilon}^i=Q^i_j\\epsilon^j. $$ Applying forward transformation to $\\tilde{\\mathbf{e}}_k$ in $\\tilde{\\epsilon}^i(\\tilde{\\mathbf{e}}_k)$ gives $$ \\tilde{\\epsilon}^i(\\tilde{\\mathbf{e}}_k)=Q^i_j\\epsilon^j(\\tilde{\\mathbf{e}}_k)= Q^i_j\\epsilon^j\\left(F^l_k\\mathbf{e}_l\\right)=Q^i_j \\epsilon^j(\\mathbf{e}_l) F^l_k=Q^i_j \\delta^j_l F^l_k= Q^i_j F^j_k. $$ But we know that $\\tilde{\\epsilon}^i(\\tilde{\\mathbf{e}}_k)=\\delta^i_k$ by definition. This implies that $$ Q^i_j F^j_k=\\delta^i_k\\quad\\implies\\quad Q=B. $$ Therefore, we build the new dual basis from the old dual basis using the backward transformation: $$ \\tilde{\\epsilon}^i=B^i_j\\epsilon^j. $$ Likewise, we build the old dual basis from the new dual basis using the forward transformation: $$ \\epsilon^i=F^i_j\\tilde{\\epsilon}^j. $$ Let\u0026rsquo;s now compare a change of basis to a change of dual basis: $$ \\tilde{\\mathbf{e}}_j = F^i_j\\mathbf{e}_i \\quad\\quad \\mathbf{e}_j = B^i_j\\tilde{\\mathbf{e}}_i $$ $$ \\epsilon^i=F^i_j\\tilde{\\epsilon}^j \\quad\\quad \\tilde{\\epsilon}^i=B^i_j \\epsilon^j $$ It is interesting to note that a change of dual basis behaves contrarily to a change of basis.\nAs for covector component transformation, we first represent a covector in two dual bases: $$ \\alpha = \\alpha_i\\epsilon^i = \\tilde{\\alpha}_j\\tilde{\\epsilon}^j. $$ But changing the dual basis gives $$ \\alpha = \\alpha_i\\epsilon^i = \\alpha_i F^i_j\\tilde{\\epsilon}^j= (F^i_j\\alpha_i)\\tilde{\\epsilon}^j. $$ This implies that the forward transformation changes the covector components from the old dual basis to the new dual basis: $$ \\tilde{\\alpha}_j = F^i_j\\alpha_i. $$ Likewise, the backward transformation changes the covector components from the new dual basis to the old dual basis: $$ \\alpha_j = B^i_j\\tilde{\\alpha}_i. $$ This tells us covector transformation is covariant to a change of basis and contravariant to a change of dual basis.\nLet\u0026rsquo;s now summarize what we have learned so far about changes of basis/dual basis and vector/covector transformations: $$ \\tilde{\\mathbf{e}}_j = F^i_j\\mathbf{e}_i \\quad\\quad \\mathbf{e}_j = B^i_j\\tilde{\\mathbf{e}}_i \\quad\\quad\\text{and}\\quad\\quad v^i = F^i_j\\tilde{v}^j \\quad\\quad \\tilde{v}^i = B^i_j v^j $$ $$ \\epsilon^i= F^i_j\\tilde{\\epsilon}^j \\quad\\quad ~ \\tilde{\\epsilon}^i= B^i_j\\epsilon^j \\quad\\quad\\text{and}\\quad\\quad \\tilde{\\alpha}_j = F^i_j\\alpha_i \\quad\\quad \\alpha_j = B^i_j\\tilde{\\alpha}_i $$\nLinear Maps Linear maps $L:V\\to W$ map vectors in $V$ to vectors in $W$ such that\n$L(\\mathbf{v}+\\mathbf{w})=L(\\mathbf{v})+L(\\mathbf{w})$, $L(n\\mathbf{v})=nL(\\mathbf{v})$. Let\u0026rsquo;s consider a speical case where $W=V$ from now on. In the array definition, linear maps are matrices that transform column vectors but do not transform basis. We define the transformation of a copy of each basis vector under a linear map $L$ by $$ L(\\mathbf{e}_i)=L^j_i\\mathbf{e}_j. $$ This defines a matrix which contains the linear map components in the basis $E$ (i.e., matrices are the array representations of linear maps): $$ L=\\begin{bmatrix} L^1_1 \u0026amp; \\cdots \u0026amp; L^1_n \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ L^n_1 \u0026amp; \\cdots \u0026amp; L^n_n \\end{bmatrix}_{E}. $$ Let $\\mathbf{v}=\\sum_{i=1}^n v^i \\mathbf{e}_i$ and $\\mathbf{w}=\\sum_{i=1}^n w^i \\mathbf{e}_i$ respectively be the input and output vectors represented in the basis $E$. Then, we have $$ \\mathbf{w}=L(\\mathbf{v})=L( v^i \\mathbf{e}_i )=v^i L(\\mathbf{e}_i)=v^i L^j_i\\mathbf{e}_j=(L^j_i v^i ) \\mathbf{e}_j, $$ $$ \\implies w^i = L^i_j v^j. $$ This is essentially the usual matrix-vector multiplication rule: $$ \\begin{bmatrix} w^1 \\\\ \\vdots \\\\ w^n \\end{bmatrix}_{E}= \\begin{bmatrix} L^1_1 \u0026amp; \\cdots \u0026amp; L^1_n \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ L^n_1 \u0026amp; \\cdots \u0026amp; L^n_n \\end{bmatrix}_{E} \\begin{bmatrix} v^1 \\\\ \\vdots \\\\ v^n \\end{bmatrix}_{E}. $$ It is important to note that\nThe transformed vectors are in the same basis as the input vectors. The $i$-th column vector in a matrix is what a copy of the $i$-th basis vector will be transformed into. Geometrically, linear maps are spatial transformations that keep lines parallel, keep lines evenly spaced, and keep the origin unchanged. A linear map can be a combination of vector scaling and rotation but not vector translation.\nLinear Map Transformation Linear maps are a kind of tensor. Linear maps are invariant to the coordinate systems, but the components of linear maps are not.\nLet\u0026rsquo;s transform a linear map from an old basis $E$ to a new basis $\\tilde{E}$. By definition, we have $$ L(\\tilde{\\mathbf{e}}_i)=\\tilde{L}^l_i \\tilde{\\mathbf{e}}_l. $$ But we also have $$ L(\\tilde{\\mathbf{e}}_i)=L( F^j_i \\mathbf{e}_j )=F^j_i L(\\mathbf{e}_j) = F^j_i L^k_j \\mathbf{e}_k = F^j_i L^k_j B^l_k \\tilde{\\mathbf{e}}_l = ( B^l_k L^k_j F^j_i ) \\tilde{\\mathbf{e}}_l. $$ This implies that $$ \\tilde{L}^l_i = B^l_k L^k_j F^j_i. $$\nNote that multiplying by the identity transformation $I$ does not change a matrix: $$ (LI)^i_k = L^i_j \\delta^j_k = L^i_k. $$ Then, the backward transformation for the linear map is given by $$ F^s_l \\tilde{L}^l_i B^i_t = F^s_l B^l_k L^k_j F^j_i B^i_t = \\delta^s_k L^k_j \\delta^j_t = L^s_t. $$\nLet\u0026rsquo;s classify the tensors we have learned so far:\n(0,1)-tensors (covariant): Basis: $$ \\tilde{\\mathbf{e}}_j = F^i_j \\mathbf{e}_i \\quad\\quad \\mathbf{e}_j = B^i_j \\tilde{\\mathbf{e}}_i $$ Covector components: $$ \\tilde{\\alpha}_j = F^i_j \\alpha_i \\quad\\quad \\alpha_j = B^i_j \\tilde{\\alpha}_i $$ (1,0)-tensors (contravariant): Dual basis: $$ \\tilde{\\epsilon}^i = B^i_j\\epsilon^j \\quad\\quad \\epsilon^i = F^i_j\\tilde{\\epsilon}^j $$ Vector components: $$ \\tilde{v}^i = B^i_j v^j \\quad\\quad v^i = F^i_j\\tilde{v}^j $$ (1,1)-tensors (one contravariant dimension, one covariant dimension): Linear maps: $$ \\tilde{L}^i_j = B^i_k L^k_l F^l_j \\quad\\quad L^i_j = F^i_k \\tilde{L}^k_l B^l_j $$ Bilinear Forms, Metric Tensors and Their Transformations Metric tensors $g:V\\times V\\to S$ are speical bilinear forms that define dot products between vectors such that\n$ag(\\mathbf{v},\\mathbf{w})=g(a\\mathbf{v},\\mathbf{w})=g(\\mathbf{v},a\\mathbf{w}),~\\forall a\\in S,~\\forall\\mathbf{v},\\mathbf{w}\\in V$. $g(\\mathbf{v}+\\mathbf{u},\\mathbf{w})=g(\\mathbf{v},\\mathbf{w}) + g(\\mathbf{u},\\mathbf{w}),~\\forall\\mathbf{v},\\mathbf{u},\\mathbf{w}\\in V$. $g(\\mathbf{v},\\mathbf{w}+\\mathbf{t})=g(\\mathbf{v},\\mathbf{w}) + g(\\mathbf{v},\\mathbf{t}),~\\forall\\mathbf{v},\\mathbf{w},\\mathbf{t}\\in V$. $g(\\mathbf{v},\\mathbf{w})=g(\\mathbf{w},\\mathbf{v}),~\\forall\\mathbf{v},\\mathbf{w}\\in V$. $g(\\mathbf{v},\\mathbf{v})\\geq 0,~\\forall\\mathbf{v}\\in V$. Note that 1-3 are rules for general bilinear forms. Rules 4-5 are special for metric tensors, so metric tensors must be symmetric (4) and positive semi-definite (5).\nMetric tensors define the dot product between two vectors $\\mathbf{v}$ and $\\mathbf{w}$ in a basis $E$: $$ g(\\mathbf{v},\\mathbf{w})=g(v^i \\mathbf{e}_i,w^j \\mathbf{e}_j)=v^i w^j g(\\mathbf{e}_i,\\mathbf{e}_j). $$ The array representation of a metric tensor $g$ in the basis $E$ is defined as $$ g_{ij}=g(\\mathbf{e}_i,\\mathbf{e}_j), $$ Therefore, the dot product in the basis $E$ becomes $$ g(\\mathbf{v},\\mathbf{w})=v^i w^j g_{ij}. % \\begin{bmatrix} % v^1 \u0026amp; \\cdots \u0026amp; v^n % \\end{bmatrix}_{E} % \\begin{bmatrix} % g_{11} \u0026amp; \\cdots \u0026amp; g_{1n} \\\\ % \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ % g_{n1} \u0026amp; \\cdots \u0026amp; g_{nn} % \\end{bmatrix}_{E} % \\begin{bmatrix} % w^1 \\\\ % \\vdots \\\\ % w^n % \\end{bmatrix}_{E}. $$\nUsing dot products, we respectively define the norm $\\lVert\\cdot\\rVert$ of a vector and the angle $\\left\u0026lt;\\cdot,\\cdot\\right\u0026gt;$ between two vectors as $$ \\lVert\\mathbf{v}\\rVert^2 = g(\\mathbf{v},\\mathbf{v})=v^i v^j g_{ij}\\geq 0 \\quad\\text{and}\\quad \\cos\\left\u0026lt;\\mathbf{v},\\mathbf{w}\\right\u0026gt;=\\frac{g(\\mathbf{v},\\mathbf{w})}{\\lVert\\mathbf{v}\\rVert\\lVert\\mathbf{w}\\rVert}=\\frac{v^i w^j g_{ij}}{\\sqrt{v^i v^j g_{ij}}\\sqrt{w^i w^j g_{ij}}}. $$\nFor an orthonormal basis $E$, the metric tensor is given by the identity matrix $g_{ij}=g(\\mathbf{e}_i,\\mathbf{e}_j)=\\delta_{ij}$. This gives us the usual dot product $g(\\mathbf{v},\\mathbf{w})=v^i w^i$ and the Pythagorean theorem $\\lVert\\mathbf{v}\\rVert^2=v^iv^i$.\nMetric tensors are invariant to the coordinate systems, but the components of metric tensors are not. Let\u0026rsquo;s now a transform metric tensor (or more generally, a bilinear form) from an old basis $E$ to a new basis $\\tilde{E}$: $$ \\tilde{g}_{ij}=g(\\tilde{\\mathbf{e}}_i,\\tilde{\\mathbf{e}}_j)=g(F^k_i \\mathbf{e}_k,F^l_j \\mathbf{e}_l)=F^k_iF^l_j g(\\mathbf{e}_k,\\mathbf{e}_l)=F^k_iF^l_jg_{kl}. $$ Likewise, the metric tensor transformation from $\\tilde{E}$ to $E$ is given by the inverse of the above transformation $$ g_{kl}=g(\\mathbf{e}_k,\\mathbf{e}_l)=g(B^i_k \\tilde{\\mathbf{e}}_i,B^j_l \\tilde{\\mathbf{e}}_j)=B^i_k B^j_l g(\\tilde{\\mathbf{e}}_i,\\tilde{\\mathbf{e}}_j)=B^i_k B^j_l \\tilde{g}_{ij}. $$ Metric tensors (and general bilinear forms) are (0,2)-tensors since both dimensions in a metric tensor are covariant (to a change of basis).\nWe can also show that the results of dot products are identical in different bases: $$ g(\\mathbf{v},\\mathbf{w})=v^i w^j g_{ij}=(F^i_k\\tilde{v}^k)(F^j_l\\tilde{v}^l) B^s_i B^t_j \\tilde{g}_{st}=(B^s_i F^i_k)(B^t_jF^j_l)\\tilde{v}^k\\tilde{v}^l\\tilde{g}_{st} = \\delta^s_k \\delta^t_l \\tilde{v}^k\\tilde{v}^l\\tilde{g}_{st}=\\tilde{v}^s\\tilde{v}^t\\tilde{g}_{st}. $$\nGeneral Linear Forms and Tensors In general, we define an $n$-(linear) form to be a linear function $V\\times V\\times \\cdots \\times V=V^n\\to S$, which is a (0,$n$)-tensor or a rank-$n$ covector. Covectors are linear forms or 1-linear forms. Bilinear forms are 2-linear forms.\nA general ($m$,$n$)-tensor $T$ is a tensor with $m$ contravariant dimensions and $n$ covariant dimensions. General tensor transformations are defined as $$ \\tilde{T}^{abc\\cdots}_{xyz\\cdots}=(B^a_i B^b_j B^c_k\\cdots ) T^{ijk\\cdots}_{rst\\cdots} (F^r_x F^s_y F^t_z\\cdots), $$ $$ T^{ijk\\cdots}_{rst\\cdots}=(F^i_a F^j_b F^k_c\\cdots ) \\tilde{T}^{abc\\cdots}_{xyz\\cdots} (B^x_r B^y_s B^z_t\\cdots). $$ We will derive this transformation rule using tensor products later.\nExamples of Tensor Products In general, tensors can be constructed by combining vectors and covectors using tensor product. We first give a few examples of redefining tensors that we have learned using tensor products below.\nLinear maps are tensor products of vector-covector pairs: $$ L=L^i_j\\mathbf{e}_i\\otimes\\epsilon^j. $$ We can verify the definition of linear maps: $$ \\mathbf{w}=L(\\mathbf{v})=L^i_j\\mathbf{e}_i\\otimes\\epsilon^j(v^k\\mathbf{e}_k)=L^i_j v^k \\mathbf{e}_i\\otimes\\epsilon^j(\\mathbf{e}_k)=L^i_j v^k \\mathbf{e}_i \\delta^j_k = L^i_j v^j \\mathbf{e}_i, $$ and the transformation of linear maps: $$ L=L^k_l\\mathbf{e}_k\\otimes\\epsilon^l=L^k_l(B^i_k\\tilde{\\mathbf{e}}_i)\\otimes(F^l_j\\tilde{\\epsilon}^j)= (B^i_k L^k_l F^l_j) \\tilde{\\mathbf{e}}_i \\otimes \\tilde{\\epsilon}^j=\\tilde{L}^i_j \\tilde{\\mathbf{e}}_i \\otimes \\tilde{\\epsilon}^j $$ $$ \\implies \\tilde{L}^i_j = B^i_k L^k_l F^l_j. $$\nBilinear forms (including metric tensors) are tensor products of covector-covector pairs: $$ \\mathcal{B}=\\mathcal{B}_{ij}\\epsilon^i\\otimes\\epsilon^j. $$ We can verify the definition of bilinear forms: $$ s=\\mathcal{B}(\\mathbf{v},\\mathbf{w})=\\mathcal{B}_{ij}(\\epsilon^i\\otimes\\epsilon^j)(\\mathbf{v},\\mathbf{w})=\\mathcal{B}_{ij}\\epsilon^i(\\mathbf{v})\\otimes\\epsilon^j(\\mathbf{w}) $$ $$ =\\mathcal{B}_{ij}\\epsilon^i(v^k\\mathbf{e}_k)\\otimes\\epsilon^j(w^l\\mathbf{e}_l)=\\mathcal{B}_{ij}v^k w^l\\epsilon^i(\\mathbf{e}_k)\\otimes\\epsilon^j(\\mathbf{e}_l)=\\mathcal{B}_{ij}v^k w^l \\delta^i_k \\delta^j_l=\\mathcal{B}_{ij}v^i w^j, $$ and the transformation of bilinear forms: $$ \\mathcal{B}=\\mathcal{B}_{kl}\\epsilon^k\\otimes\\epsilon^l=\\mathcal{B}_{kl}(F^k_i\\tilde{\\epsilon}^i)\\otimes(F^l_j\\tilde{\\epsilon}^j)=F^k_i F^l_j\\mathcal{B}_{kl}\\tilde{\\epsilon}^i\\otimes\\tilde{\\epsilon}^j=\\tilde{\\mathcal{B}}_{ij}\\tilde{\\epsilon}^i\\otimes\\tilde{\\epsilon}^j $$ $$ \\implies \\tilde{\\mathcal{B}}_{ij} = F^k_i F^l_j\\mathcal{B}_{kl}. $$\nGeneral Tensor Products and Kronecker Products A general ($m$,$n$)-tensor can be defined by combining $m$ vectors $\\mathbf{e}_i,\\mathbf{e}_j,\\mathbf{e}_k,\\cdots$ and $n$ covectors $\\epsilon^r,\\epsilon^s,\\epsilon^t,\\cdots$ using tensor products: $$ T = T^{ijk\\cdots}_{rst\\cdots}(\\mathbf{e}_i\\otimes\\mathbf{e}_j\\otimes\\mathbf{e}_k\\otimes\\cdots)\\otimes (\\epsilon^r\\otimes\\epsilon^s\\otimes\\epsilon^t\\otimes\\cdots). $$ Applying changes of bases and dual bases, we obtain $$ \\begin{aligned} T \u0026amp;= T^{ijk\\cdots}_{rst\\cdots}(B^a_i\\tilde{\\mathbf{e}}_a\\otimes B^b_j\\tilde{\\mathbf{e}}_b\\otimes B^c_k\\tilde{\\mathbf{e}}_c\\otimes\\cdots)\\otimes (F^r_x\\tilde{\\epsilon}^x\\otimes F^s_y\\tilde{\\epsilon}^y\\otimes F^t_z \\tilde{\\epsilon}^z\\otimes\\cdots) \\\\ \u0026amp;= (B^a_i B^b_j B^c_k\\cdots) T^{ijk\\cdots}_{rst\\cdots} (F^r_x F^s_y F^t_z \\cdots) (\\tilde{\\mathbf{e}}_a\\otimes \\tilde{\\mathbf{e}}_b\\otimes \\tilde{\\mathbf{e}}_c\\otimes\\cdots)\\otimes (\\tilde{\\epsilon}^x\\otimes \\tilde{\\epsilon}^y\\otimes \\tilde{\\epsilon}^z\\otimes\\cdots). \\end{aligned} $$ But the definition of the tensor $T$ in the new bases and dual bases is $$ T = \\tilde{T}^{abc\\cdots}_{xyz\\cdots}(\\tilde{\\mathbf{e}}_a\\otimes \\tilde{\\mathbf{e}}_b\\otimes \\tilde{\\mathbf{e}}_c\\otimes\\cdots)\\otimes (\\tilde{\\epsilon}^x\\otimes \\tilde{\\epsilon}^y\\otimes \\tilde{\\epsilon}^z\\otimes\\cdots). $$ This gives us the general tensor transformation rule: $$ \\tilde{T}^{abc\\cdots}_{xyz\\cdots}=(B^a_i B^b_j B^c_k\\cdots ) T^{ijk\\cdots}_{rst\\cdots} (F^r_x F^s_y F^t_z\\cdots). $$\nWhen we apply an ($m$,$n$)-tensor tensor $T$ to an ($n$,0)-tensor (or a rank-$n$ vector) $D$, we will obtain an ($m$,0)-tensor (or a rank-$m$ vector) $T(D)$. This is a generalization of applying a (rank-1) covector (i.e., a (1,0)-tensor) to a $rank-1$ vector (i.e., a (0,1)-tensor) resulting in a scalar (i.e., a (0,0)-tensor).\nSpecifically, suppose that $$ D = D^{opq\\cdots}(\\mathbf{e}_o\\otimes\\mathbf{e}_p\\otimes\\mathbf{e}_q\\otimes\\cdots). $$ Then, we have $$ T(D)=T^{ijk\\cdots}_{rst\\cdots}(\\mathbf{e}_i\\otimes\\mathbf{e}_j\\otimes\\mathbf{e}_k\\otimes\\cdots)\\otimes (\\epsilon^r\\otimes\\epsilon^s\\otimes\\epsilon^t\\otimes\\cdots)(D^{opq\\cdots}\\mathbf{e}_o\\otimes\\mathbf{e}_p\\otimes\\mathbf{e}_q\\otimes\\cdots). $$ Note that there are multiple ways in which we can associate each covector with a vector in $(T_{rst\\cdots}\\epsilon^r\\otimes\\epsilon^s\\otimes\\epsilon^t\\otimes\\cdots)(D^{opq\\cdots}\\mathbf{e}_o\\otimes\\mathbf{e}_p\\otimes\\mathbf{e}_q\\otimes\\cdots)$. To avoid ambiguity, we should always clearly specify such association using Einstein\u0026rsquo;s notation. That is, whether it means $T(D)=T^{ijk\\cdots}_{rst\\cdots} D^{rst\\cdots}$ or $T(D)=T^{ijk\\cdots}_{rst\\cdots} D^{rts\\cdots}$ or anything else.\nNote that a tensor is a multilinear map, which is linear when all inputs except one are held constant:\n$T(x_1,\\cdots,nx_i,\\cdots,x_n)=nT(x_1,\\cdots,x_i,\\cdots,x_n),~\\forall i$, $T(x_1,\\cdots,x_i+y_i,\\cdots,x_n)=T(x_1,\\cdots,x_i,\\cdots,x_n)+T(x_1,\\cdots,y_i,\\cdots,x_n),~\\forall i$. Kronecker products are tensor products expressed in the array representation of tensors. For examples, matrices are Kronecker products of (column vector, row vector) pairs, and metric tensors are Kronecker products of (row vector, row vector) pairs.\nTensor Product Spaces Let $n\\in S$ be a scalar, $\\mathbf{v},\\mathbf{u}\\in V$ be vectors, and $\\alpha,\\beta\\in V^*$ be covectors. Tensor products satisfy the following rules\n$n(\\mathbf{v}\\otimes\\alpha)=(n\\mathbf{v})\\otimes\\alpha=\\mathbf{v}\\otimes(n\\alpha)$, $\\mathbf{v}\\otimes\\alpha+\\mathbf{v}\\otimes\\beta=\\mathbf{v}\\otimes(\\alpha+\\beta)$, $\\mathbf{v}\\otimes\\alpha+\\mathbf{u}\\otimes\\alpha=(\\mathbf{v}+\\mathbf{u})\\otimes\\alpha$. We can form tensor product spaces $V\\otimes V$, $V\\times V^*$, $V^*\\times V$, and $V^*\\times V^*$ using tensor products, and we can continue forming larger tensor products such as $V\\otimes V^* \\otimes V \\otimes \\cdots$ and so forth. A tensor space associated with a tensor $T$ contains elements obtained by any number of summations with $T$ in any order.\nFor example, consider a tensor space $V^* \\otimes V \\otimes V^* \\otimes V^*$ associated with the tensor $T^{~j}_{i~kl}\\epsilon^i\\mathbf{e}_j\\epsilon^k\\epsilon^l$. Below are some example elements in this tensor space:\n$T^{~j}_{i~kl} v^i \\alpha_j w^k u^l\\quad V \\times V^* \\times V \\times V \\to S$, $T^{~j}_{i~kl} U^{ikl}\\beta_j\\quad (V \\otimes V \\otimes V)\\times V^* \\to S$, $T^{~j}_{i~kl} \\alpha_j D^{kl}\\quad V^* \\times (V \\otimes V) \\to V^*$, $T^{~j}_{i~kl} L^i_j \\quad (V \\otimes V^*)\\to (V^* \\otimes V^*)$, \u0026hellip; Musical Isomorphism We would like to find a meaningful partner covector $\\nu=v_i\\epsilon^i\\in V^*$ for each vector $\\mathbf{v}=v^i\\mathbf{e}_i\\in V$. It turns out that one such partnership is called musical isomorphism, which is defined using the metric tensor $g\\in V^*\\otimes V^*$ (i.e., dot product).\nMusical isomorphism defines the partner of the vector $\\mathbf{v}\\in V$ as the linear function $\\nu:V\\to S$ such that $$ \\nu=g(\\cdot,\\mathbf{v})=g_{ik}\\epsilon^i\\otimes\\epsilon^k(v^j\\mathbf{e}_j)=g_{ik}v^j\\epsilon^i\\otimes\\epsilon^k(\\mathbf{e}_j)=g_{ik} v^j \\epsilon^i\\delta^k_j=(g_{ij}v^j)\\epsilon^i. $$ But we have $\\nu=v_i\\epsilon^i$ by definition. This defines the flat operation which lowers the index of the components $v^j$ of the vector $\\mathbf{v}$ by $$ v_i=g_{ij}v^j. $$ Therefore, the partner covector $\\nu$ for the vector $\\mathbf{v}$ is given by $$ \\nu=v_i\\epsilon^i=g_{ij}v^j\\epsilon^i. $$\nAnother way to think about the flat operation for lowering vector component indices $v^i$ is to consider the dot product between the corresponding basis vector $\\mathbf{e}_i$ and the vector $\\mathbf{v}$: $$ v_i=g(\\mathbf{e}_i,\\mathbf{v})=g(\\mathbf{e}_i,v^j\\mathbf{e}_j)=v^j g(\\mathbf{e}_i,\\mathbf{e}_j)=g_{ij}v^j. $$\nThis is a meaningful partnership because it holds in any basis by definition: $$ \\tilde{v}_i=\\tilde{g}_{ij}\\tilde{v}^j \\quad\\text{and}\\quad\\nu=\\tilde{v}_i\\tilde{\\epsilon}^i=\\tilde{g}_{ij}\\tilde{v}^j\\tilde{\\epsilon}^i. $$\nNote that $v^i\\not=v_i$ in general. The only exception is in the orthonormal basis where we have $$ v_i=g_{ij}v^j=\\delta_{ij}v^j=v^i. $$\nWe can define the inverse of the metric tensor as $h^{ki}\\in V\\otimes V$ such that composing $g$ and $h$ results in the identity: $$ h^{ij}g_{jk}=\\delta^i_k. $$ Then, the sharp operation which raises the index of the components $v_j$ of the covector $\\nu$ (i.e., the inverse of the flat operation) can be derived as follows: $$ h^{ij}v_j=h^{ij}g_{jk}v^k=\\delta^i_k v^k=v^i, $$ $$ \\mathbf{v}=v^i\\mathbf{e}_i=h^{ij}v_j\\mathbf{e}_i. $$ Likewise, this holds in any basis by definition: $$ \\tilde{v}^i=\\tilde{h}^{ij}\\tilde{v}_j, $$ $$ \\mathbf{v}=\\tilde{v}^i\\tilde{\\mathbf{e}}_i=\\tilde{h}^{ij}\\tilde{v}_j\\tilde{\\mathbf{e}}_i. $$\nThe flat and sharp operations for lowering and raising indices can be applied to tensors of any rank in general. For example, consider the tensor $T=T^i_{jk}\\mathbf{e}_i\\otimes\\epsilon^j\\otimes\\epsilon^k\\in V\\otimes V^* \\otimes V^*$. We can raise the index $j$ by $T^i_{jk}h^{jl}=T^{il}_k$, which results in a new tensor $T\u0026rsquo;=T^{il}_k \\mathbf{e}_i\\otimes\\mathbf{e}_l\\otimes\\epsilon^k\\in V\\otimes V \\otimes V^*$.\n","date":1704412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704412800,"objectID":"f38bc6571466acd45312aae395f7c5aa","permalink":"https://wenlin-chen.github.io/post/tensor-algebra/","publishdate":"2024-01-05T00:00:00Z","relpermalink":"/post/tensor-algebra/","section":"post","summary":"These are my learning notes for the lecture series Tensors for Beginners.\nMotivation and Applications of Tensors Tensors provide insights about how geometry works. Below are some examples that involve tensors.","tags":null,"title":"Tensor Algebra","type":"post"},{"authors":["Wen Wu","Wenlin Chen","Chao Zhang","Philip C. Woodland"],"categories":null,"content":"","date":1696032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696032000,"objectID":"a813e18c87043788c6c22b08f0d565dd","permalink":"https://wenlin-chen.github.io/publication/wu2023has/","publishdate":"2023-09-30T00:00:00Z","relpermalink":"/publication/wu2023has/","section":"publication","summary":"Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation such as data annotation and system assessment. Human perception and behaviour during human evaluation exhibit inherent variability due to diverse cognitive processes and subjective interpretations, which should be taken into account in modelling to better mimic the way people perceive and interact with the world. This paper introduces a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which incorporates human variability and allows for the efficient generation of human-like annotations for unlabelled test inputs. Under this framework, we propose two new model classes, conditional integer flows and conditional softmax flows, to account for ordinal and categorical annotations, respectively. The proposed method is evaluated on three real-world human evaluation tasks and shows superior capability and efficiency to predict the aggregated behaviours of human annotators, match the distribution of human annotations, and simulate the inter-annotator disagreements.","tags":[],"title":"It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation","type":"publication"},{"authors":["Wenlin Chen","Julien Horwood","Juyeon Heo","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1687824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687824000,"objectID":"6039f5f4bbc26f58c353b25a8afbb2f9","permalink":"https://wenlin-chen.github.io/publication/chen2023leveraging/","publishdate":"2023-06-27T00:00:00Z","relpermalink":"/publication/chen2023leveraging/","section":"publication","summary":"This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.","tags":[],"title":"Leveraging Task Structures for Improved Identifiability in Neural Network Representations","type":"publication"},{"authors":["Wenlin Chen","Hong Ge"],"categories":null,"content":"","date":1684972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684972800,"objectID":"724a54e99e1fb3b3b87a33cffd45322e","permalink":"https://wenlin-chen.github.io/publication/chen2023neural/","publishdate":"2023-05-25T00:00:00Z","relpermalink":"/publication/chen2023neural/","section":"publication","summary":"We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.","tags":[],"title":"Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning","type":"publication"},{"authors":["Wenlin Chen","Austin Tripp","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"e7e2dce54685980967029d324635a717","permalink":"https://wenlin-chen.github.io/publication/chen2023meta/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/publication/chen2023meta/","section":"publication","summary":"We propose Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT), a novel framework for learning deep kernel Gaussian processes (GPs) by interpolating between meta-learning and conventional deep kernel learning. Our approach employs a bilevel optimization objective where we meta-learn generally useful feature representations across tasks, in the sense that task-specific GP models estimated on top of such features achieve the lowest possible predictive loss on average. We solve the resulting nested optimization problem using the implicit function theorem (IFT). We show that our ADKF-IFT framework contains previously proposed Deep Kernel Learning (DKL) and Deep Kernel Transfer (DKT) as special cases. Although ADKF-IFT is a completely general method, we argue that it is especially well-suited for drug discovery problems and demonstrate that it significantly outperforms previous state-of-the-art methods on a variety of real-world few-shot molecular property prediction tasks and out-of-domain molecular property prediction and optimization tasks.","tags":[],"title":"Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction","type":"publication"},{"authors":["Wenlin Chen","Samuel Horváth","Peter Richtárik"],"categories":null,"content":"","date":1661126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661126400,"objectID":"eee04c746fd5fbd08cbed0a0667cb0ed","permalink":"https://wenlin-chen.github.io/publication/chen2022optimal/","publishdate":"2022-08-04T00:00:00Z","relpermalink":"/publication/chen2022optimal/","section":"publication","summary":"It is well understood that client-master communication can be a primary bottleneck in federated learning (FL). In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with \"important\" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which allows for secure aggregation and stateless clients, and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.","tags":[],"title":"Optimal Client Sampling for Federated Learning","type":"publication"},{"authors":["Austin Tripp","Wenlin Chen","José Miguel Hernández-Lobato"],"categories":null,"content":"","date":1651190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651190400,"objectID":"7c674ec1dda2935eabce28cc88b5af57","permalink":"https://wenlin-chen.github.io/publication/tripp2022evaluation/","publishdate":"2022-04-29T00:00:00Z","relpermalink":"/publication/tripp2022evaluation/","section":"publication","summary":"De novo drug design has recently received increasing attention from the machine learning community. It is important that the field is aware of the actual goals and challenges of drug design and the roles that de novo molecule design algorithms could play in accelerating the process, so that algorithms can be evaluated in a way that reflects how they would be applied in real drug design scenarios. In this paper, we propose a framework for critically assessing the merits of benchmarks, and argue that most of the existing de novo drug design benchmark functions are either highly unrealistic or depend upon a surrogate model whose performance is not well characterized. In order for the field to achieve its long-term goals, we recommend that poor benchmarks (especially logP and QED) be deprecated in favour of better benchmarks. We hope that our proposed framework can play a part in developing new de novo drug design benchmarks that are more realistic and ideally incorporate the intrinsic goals of drug design.","tags":[],"title":"An Evaluation Framework for the Objective Functions of De Novo Drug Design Benchmarks","type":"publication"},{"authors":["Wenlin Chen"],"categories":null,"content":"","date":1629331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629331200,"objectID":"12bedca3d469d735925baf247e9638c5","permalink":"https://wenlin-chen.github.io/publication/chen2021causal/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/publication/chen2021causal/","section":"publication","summary":"In this thesis, we study causal representation learning for latent space optimization, which allows for robust and efficient generation of novel synthetic data with maximal target value.  We assume that the observed data was generated by a few latent factors, some of which are causally related to the target and others of which are spuriously correlated with the target and confounded by an environment variable. Our proposed method consists of three steps, which exploits the structure of the causal graph that describes the assumed underlying data generating process. In the first step, we recover the true data representation (i.e., the latent factors from which the observed data originated). We obtain novel identifiability theory, showing that the true data representation can be recovered up to simple transformations by a generalized version of identifiable variational auto-encoders. In the second step, we identify the causal latent factors of the target, for which we propose a practical causal inference scheme that employs (conditional) independence tests and causal discovery algorithms. Our method does not require having access to the true environment variable, which overcomes a major limitation of existing causal representation learning approaches in the literature. In the final step, we query latent points that correspond to data points with high target values by intervening upon the causal latent factors using standard latent space optimization techniques. We empirically evaluate and thoroughly analyze our method on three different tasks, including a chemical design task. We show that our method can successfully recover the true data representation in the finite data regime and correctly identify the causal latent factors of the target, which results in state-of-the-art performance for black-box optimization.","tags":[],"title":"Causal Representation Learning for Latent Space Optimization","type":"publication"},{"authors":["Andrew Webb","Charles Reynolds","Wenlin Chen","Henry Reeve","Dan Iliescu","Mikel Luján","Gavin Brown"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"73540bab56f2b0d108a710976f7b0dcb","permalink":"https://wenlin-chen.github.io/publication/webb2020ensemble/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/publication/webb2020ensemble/","section":"publication","summary":"End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue—are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where overparameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.","tags":[],"title":"To Ensemble or Not Ensemble: When Does End-to-End Training Fail?","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://wenlin-chen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]